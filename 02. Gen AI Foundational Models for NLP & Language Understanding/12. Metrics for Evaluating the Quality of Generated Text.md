# Metrics for Evaluating the Quality of Generated Text

## Perplexity (sự bối rối)

Generative AI and large language models, or LLMs, are extensively used to generate text images and a lot more.
The measurement of their accomplishment lies in their ability to generate consistent and contextually relevant text.
But how can you measure and evaluate the performance of these models accurately? This is where perplexity comes into play.
It is a metric that has emerged as a precious tool for evaluating the efficiency of LLMs and GenAI models.

Consider the phrase I love to travel.
Given a text corpus, there are tools to assign precise probabilities to whole sequences of words, effectively gauging the likelihood of any specific sequence occurring within the data.
These probabilities mirror true values or y labels in a classification task.
As you parse the sentence, note that the conditional probability becomes more complex.
Each word's probability takes into account the cumulative context provided by all preceding words.
For instance, travel isn't considered alone but is part of the chain I love to, reflecting its dependence on what has already been said.
This principle is universally applicable to sequences of any length symbolized by the variable Omega_T to represent each word.
![[Pasted image 20250313233921.png|400]]

It is crucial to understand that this is a simplified model.
Actual probability computations for a complete corpus are significantly more elaborate.
Denote this inferred probability as P.
![[Pasted image 20250313234006.png|400]]

To evaluate the performance of your language model or LM, you must compare its output against the actual likelihood typically derived from validation data.
Your LM calculates the probability of observing a particular sequence as illustrated here.
It approximates the true probability distribution exemplified by a bigram model sourced from a neural network, where Theta represents the model parameters and the first term is one.
![[Pasted image 20250313234102.png|400]]

This estimated probability is known as the likelihood, which is symbolized as Q.
The term Q is essential for gauging the fidelity of your LM to the underlying data it aims to represent.
This is closely related to the loss function.
![[Pasted image 20250313234138.png|400]]

In perplexity, you can employ cross-entropy loss to measure the discrepancy between the predicted and actual distribution.
![[Pasted image 20250313234203.png|400]]

For demonstration, a simplified scenario is presented with a single variable.
As your predicted distribution converges with the true distribution, the cross-entropy loss diminishes.
Ideally, when the two distributions coincide perfectly, the cross-entropy loss reaches zero, indicating no difference between them.
Consequently, as the predicted distribution more closely approximates the true distribution, you'll observe a decrease in the cross-entropy loss, signifying an improvement in your model's accuracy.
![[Pasted image 20250313234242.png|400]]

In the context of language modeling, perplexity can be seen as a measure of how surprised or uncertain the model is when predicting the next word in a sequence.
Perplexity is calculated as an exponent of the loss obtained from the model.
For each sequence, perplexity is calculated from the average loss of all its tokens.
By applying the exponential function to the average cross-entropy loss, you can obtain perplexity.
Exponentiating the loss has the effect of undoing the logarithm and transforming the value back into a more interpretable space.
Let's look at an example to see how to calculate perplexity.
Suppose you have another model with less accurate predictions as shown.
By applying the exponential function to the new loss value, you get 142.6.
You can see that the exponential function makes the difference clearer.
Lower perplexity values indicate better performance.
Perplexity provides an overall measure of model performance, but doesn't capture the nuances of generated text quality.
Perplexity is usually used only to determine how well a model has learned the training set.
Other metrics like BLEU, ROUGE, and more are used on the test set to measure test performance.
![[Pasted image 20250313234355.png|400]]

## N-gram
There are also metrics that measure the similarity between the generated text and a set of references based on n-gram matching.
This is helpful in evaluating the quality of the generated text and when there is more than one valid generated text.
For example, in the translation task, you can compare the generated translation with different versions of the translation.
This illustration shows how matching n-grams are counted by comparing a hypothesis sequence with a reference sequence.
The is matched, so the unigram count increases to one.
Big is not matched, so the unigram count remains one.
Cat is matched, so the unigram count increments to two.
Sitting is not match, so the unigram count stays at two.
On is matched, so the unigram count goes to three.
The is matched, so the unigram count increases to four.
Also on the is the first matched bigram, so the bigram count increases to one.
Rug is not matched, so the unigram count remains four and the bigram count remains one.
![[Pasted image 20250313234530.png|400]]

## Precision and recall
In machine translation, precision and recall are used to evaluate the quality of the translated text compared to the reference or target translation.
Precision in machine translation measures the accuracy of the generated translation.
CountMatch is the number of matching n-grams between the generated translation and the reference translation, and CountGenerated is the total number of n-grams in the generated translation.
Recall that machine translation measures the completeness of the generated translation.
It emphasizes the coverage of the important content from the reference translation.
CountReference is the total number of n-grams in the reference translation.
F1 score is also a harmonic mean of precision and recall that is used to judge the performance of a model based on them.
![[Pasted image 20250313234629.png|400]]

## Implementations of evaluation metrics
In the field of natural language processing or NLP, there are several popular libraries providing implementations of various evaluation metrics.
Some of the commonly used libraries for NLP evaluation metrics are 
- NLTK library
	- BLEU: nltk.translate.bleu_score
	- METEOR: nltk.translate.meteor_score module
- PyTorch library
	- Perplexity and Cross-entropy loss: torch.nn.CrossEntropyLoss
- other libraries
	- BLEU: torchtext.data.metrics.bleu_score
	- ROUGE: torchmetrics.text.rouge.ROUGEScore

Let's look at a use case of the NLTK library that calculates the BLEU score for a generated translation.
First, define a function that creates references and hypothesis lists in the format that sentence BLEU accepts, calculates the BLEU score for them, and returns the score.
Then create an array of references.
Finally, call the function to calculate the BLEU score for the hypothesis sentence that can be the models generated translation.
```python
def calculate_bleu_score(generated_translation, reference_translations):
    # Convert the generated translations and reference translations into the expected format for sentence_bleu
    references = [reference.split() for reference in reference_translations]
    hypothesis = generated_translation.split()

    # Calculate the BLEU score
    bleu_score = sentence_bleu(references, hypothesis)

    return bleu_score
```

# Recap
- Perplexity
	- is a metric and a precious tool for evaluating the efficiency of LLMs and GenAI models.
	- Can employ cross-entropy loss to measure the discrepancy between the predicted and actual distribution.
	- calculated as an exponent of the loss obtained from the model.
	- provides an overall measure of model performance, but doesn't capture the nuances of generated text quality.
- In machine translation, precision measures the accuracy of the generated translation
- recall measures the completeness of the generated translation.
- F1 score is a harmonic mean of precision and recall that is used to judge the performance of a model based on them.
- Popular libraries that provide implementations of various evaluation metrics such as NLTK and PyTorch libraries.