# Word2Vec: Introduction and CBOW Models

## Word2Vec
- Short form for `word to vector`.
- Group of models that produce word embeddings or vectors
- Numerical representations capturing the essence of words.

For example, `king` is closer to `man` and `queen` to `woman` due to their similar meanings.
Interestingly, subtracting `man` from `king` yields a vector similar to `queen`, illustrating the effectiveness of these embeddings in capturing word relationships.
These vectors can be utilized in natural language processing, or NLP tasks to enhance performance by substituting the randomly generated embeddings.
![[Pasted image 20250311095212.png|400]]

To acquire these vectors, you can begin with randomly generated embeddings.
You can use a neural network model consisting of an input layer, an embedding layer, and an output layer.
Words are fed into an embedding layer, which interacts with a softmax output layer to predict context words.
Training involves tuning the `W` hidden layer and `W prime` output layer weights to refine word vector representations.
The number of neurons in both input and output layers corresponds to the vocabulary size, while the embedding layer's size, which is chosen by the user, defines the word vector dimensions.
![[Pasted image 20250311103512.png|400]]

After training the neural network, if the network predicts `queen` following `woman` with a higher probability than it does for `man`, the embedding for `queen` will be closer to `woman` than to `man`.
Similarly, if `king` is probabilistically more associated with `man` than with `woman`, the resulting vector for `king` is closer to `man` than `woman`.
![[Pasted image 20250311103652.png|400]]

Now, let's use the network to attempt to predict words from the given window using other words within the window.
Set the window width to 1.
At t=1, `exercises` is the target word with `she` and `every` as context words.
At t=2, `every` becomes the target surrounded by `exercises` and `morning` in context.
![[Pasted image 20250311220615.png|400]]

For the context and target words with a window width of 1, the context includes the word at t-1 and t+1.
The target word is at t.
At t=1, the context consists of `she` and `every`.
The target word is `exercises`.
As you move to t=2, the context includes `exercises`, `morning` and the target word is `every`.
![[Pasted image 20250311220823.png|400]]

## Continuous Bag of Words (CBOW) model.

Utilizes context words to predict a target word and generate its embedding.

When `she` and `every` are given as inputs, the model predicts `exercises`.
With `exercises` and `morning` as inputs, the model predicts `every`.
In the CBOW model for the sentence, `she exercises every morning`, the input dimension of 4 matches the number of unique words in the corpus.
The goal is to predict word likelihood, so the output dimension is also 4.
The target word is `exercises` with context words, `she` and `every` combined into a bag of words vector.
This vector passes through the hidden layer containing word embeddings.
In the output layer, expect the highest logic value for `exercises`, indicating the model's prediction for this context.
![[Pasted image 20250311221510.png|400]]

Now, let's shift the context window to consider new inputs.
The words, `exercises` and `morning` are encoded as one hot vectors and provided as inputs to the model.
During training, your goal should be to fine tune the model's weights so that it effectively predicts the target word `every`.
Which should correspond to the highest logic value in the output layer.

## Create a CBOW model

To create the CBOW model, first you will initialize it.
Then you will define the embedding layer using `nn.EmbeddingBag`, which calculates the average of context word embeddings.
You will also configure the fully connected layer using `self.fc`, with input size embed dim and output size vocab size.
In the `forward` method, you will pass input text and offsets through the embedding layer, which retrieves context word embeddings and calculates their average.
Next, you will apply the `ReLu` activation function.
Afterward, the output of the `ReLu` activation goes through the `fully connected` layer.
Finally, you will create an instance of the CBOW model.
```python
class CBOW(nn.Module):
    # Initialize the CBOW model
    def __init__(self, vocab_size, embed_dim, num_class):
        
        super(CBOW, self).__init__()
         # Define the embedding layer using nn.EmbeddingBag
        # It outputs the average of context words embeddings
        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=False)
        # Define the fully connected layer with input size embed_dim and output size vocab_size
        self.fc = nn.Linear(embed_dim, vocab_size)
        

        self.init_weights()
    # Initialize the weights of the model's parameters
    def init_weights(self):
        # Initialize the weights of the embedding layer
        initrange = 0.5
        self.embedding.weight.data.uniform_(-initrange, initrange)
        # Initialize the weights of the fully connected layer
        self.fc.weight.data.uniform_(-initrange, initrange)
        # Initialize the biases of the fully connected layer to zeros
        self.fc.bias.data.zero_()
        

    def forward(self, text, offsets):
        # Pass the input text and offsets through the embedding layer
        out = self.embedding(text, offsets)
        # Apply the ReLU activation function to the output
        out = torch.relu(out)
        # Pass the output of the ReLU activation through the fully connected layer
        return self.fc(out)
```
```python
vocab_size = len(vocab)
emsize = 24
model_cbow = CBOW(vocab_size, emsize, vocab_size).to(device)
```

Next, you will initialize the tokenizer using steps 1 and 2.
Using step 3, you will create a vocabulary from tokenized data.
In step 4, you will set the context size to 2.
Then you will slide over the text to form context target pairs.
```python
# Step 1: Get tokenizer
tokenizer = get_tokenizer('basic_english')  # This uses basic English tokenizer. You can choose another.

# Step 2: Tokenize sentences
def tokenize_data(sentences):
    for sentence in sentences:
        yield tokenizer(sentence)

tokenized_toy_data = tokenizer (toy_data)

# Step 3: Creating vocab
vocab = build_vocab_from_iterator(tokenize_data(tokenized_toy_data), specials=['<unk>'])
vocab.set_default_index(vocab["<unk>"])

# Step 4: Sliding on text
CONTEXT_SIZE = 2

cobow_data = []
for i in range(1, len(tokenized_toy_data) - CONTEXT_SIZE):
    context = (
        [tokenized_toy_data[i - j - 1] for j in range(CONTEXT_SIZE)]
        + [tokenized_toy_data[i + j + 1] for j in range(CONTEXT_SIZE)]
    )
    target = tokenized_toy_data [i]
    cobow_data.append((context, target))
```

Next, using step 5, you will set up a text processing pipeline, a data loader, and a batch function for a network.
It will have a bag of words of batch size 64 to prepare batches for training on next word prediction tasks.
```python
text_pipeline = lambda tokens:[vocab[token]  for token in tokens]

# Step 5: Creating target, context, and offset batches of data
def collate_batch(batch):
    target_list, context_list, offsets = [], [], [0]
    for _context, _target in batch:
        target_list.append(vocab[_target])  
        processed_context = torch.tensor(text_pipeline(_context), dtype=torch.int64)
        context_list.append(processed_context)
        offsets.append(processed_context.size(0))
    target_list = torch.tensor(target_list, dtype=torch.int64)
    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)
    context_list = torch.cat(context_list)
    return target_list.to(device), context_list.to(device), offsets.to(device)

BATCH_SIZE = 64

dataloader_cbow = DataLoader(
    cobow_data, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch
)
print(dataloader_cbow) 
```
## Recap
- `Word2vec` is the short form for word to vector. It is a group of models that produce word embeddings, or vectors, which are numerical representations capturing the essence of words.
- A neural network model consists of an input layer, an embedding layer, and an output layer.
- Words are fed into an embedding layer, which interacts with an output layer to predict context words.
- The number of neurons in both input and output layers corresponds to the vocabulary size.
- While the embedding layer's size, which is chosen by the user, defines the word vector dimensions.
- The continuous bag of words, or CBOW model utilizes context words to predict a target word and generates its embedding