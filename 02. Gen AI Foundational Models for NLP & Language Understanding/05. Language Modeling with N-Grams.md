# Language Modeling with N-Grams

## Bi-grams

Consider the phrases 
- I like ...
- I hate ...
How would you complete these sentences? 
For instance, if you were given I like, followed by a blank, would you choose `surgery` or `vacation`?
Most people would likely complete it with `vacation` upon seeing the word `like`.
Similarly, with the phrase I hate followed by a blank, the expected completion is `surgery`, reflecting common associations with these words.
These examples demonstrate how your word choices are often influenced by the context provided by preceding words.

This is where the bi-gram model comes into play.
It's a conditional probability model.
What's the probability of your third word, given the second one, word two represents your context.
The second word, which is crucial in guiding the prediction.
![[Pasted image 20250306204708.png|400]]

In a `bi-gram` model, your context size is one, meaning you'll consider only the immediate previous word to predict the next one.
Given the second word is `like`, that is, word two equals `like`, what could the third word be? Using the table, count the occurrences of each potential follow up to `like` in the third column.
By examining the table, observe that `surgery` never follows `like`.
Hence, the probability of word three being `surgery` is zero.
However, vacation appears after like, and the probability of word three being vacation is one.
![[Pasted image 20250306205939.png|400]]
## Trigrams
Consider the phrases `surgeons like surgery` and `I hate surgery`.
Adding surgeons changes the context of the sentence, making it at least plausible that surgeons like surgery.
The `tri-gram` model is also a conditional probability function and can improve on the `bi-gram` model's limitations by increasing the context size to two.
Now, in addition to word two, you also use word one to predict word three.
Let's examine the probability of word three equaling surgery.
Given that word one is `I` and word two is `like`, you will find that the probability is zero.
However, if you change word one to `surgeons`, the probability of word three being `surgery` is now one.
![[Pasted image 20250306210309.png|400]]

Consider the phrases `surgeons like surgery`, `I hate surgery`, and `I like vacations`.
How would you complete the phrase `surgeons like`? 
You can employ the tri-gram model to predict the next word by leveraging the probabilities generated from the table.
For context, set the first word to `surgeons` and the second to `like` to predict the third word, denoted as omega.
Identify which word maximizes the likelihood of being the third word.
This optimization is achieved using the `argmax` function, which selects the word with the highest probability from the different possible third words, given that the first word is `surgeons` and the second is `like`.
![[Pasted image 20250306210614.png|400]]

As illustrated, all possible outcomes for the third word have a probability of zero, except for `surgery`.
Therefore, the predicted third word is `surgery`.
![[Pasted image 20250306212238.png|400]]

Let's consider predicting the next word, word 3, given that the first word is I and the second word is like.
If instead you're aiming to predict a word at some arbitrary point t, using the preceding points `t-1` and `t-2`.
And if you're assuming these relationships remain constant over time, that is, implying stationarity, then you can apply the tri-gram or bi-gram models at any point in time.
For the tri-gram case, you can use words at positions `t-2` and `t-1` to predict the word at position `t`.
Following the same predictive process, you can generalize the concept of a tri-gram to an n-gram model, which allows for an arbitrary context size.

## N-gram model

In this expanded framework, `t` denotes the chosen context size.
However, calculating the probabilities for larger context sizes can become increasingly complex.
Fortunately, neural networks offer a solution by approximating these probabilities.
Specifically, apply the `softmax` function to estimate the probabilities, and then train a neural network to predict the next word based on the previous words.
The context word is used as a feature, that is, the context vector to define the previous words.
![[Pasted image 20250306212616.png|400]]

Let's revisit the previous example where each word in your vocabulary of six words, that is, each word is represented by a six dimensional, one hot encoded vector.
![[Pasted image 20250306212832.png|400]]

Consider encoding the phrase `I like` with a context size of two.
If you employed a bag of words model, the vector representation of `I like` would be identical to that of like I.
![[Pasted image 20250306213004.png|400]]

To establish the concatenate vector, concentrate the individual vectors corresponding to each word, as illustrated here.
Instead of performing the operation on raw one hot vectors, concatenate the one hot encoded embedding vectors.
In the realm of neural networks, the context vector is generally defined as the product of your context size and the size of your vocabulary.
Typically, this vector is not computed directly, instead, you should construct it by concatenating the embedding vectors.
![[Pasted image 20250306213216.png|400]]

## N-gram model: neural network architecture

Let's examine the neural network architecture for an n-gram model.
Imagine you're working with a small problem, a six-word vocabulary and a two word context.
Recall the neural classifier designed to categorize one of four articles which has four output neurons.
However, the network must predict one of six possible outputs, one for each word in the vocabulary.
So there are six neurons in the output layer.
Given the context size of two, adjust the input accordingly.
The input dimension is the product of vocabulary size and context size.
With six words and a context size of 2, your input dimension is 12.
![[Pasted image 20250306213646.png|400]]

Utilize the final layer's output directly without needing to convert it to probabilities.
This feedforward neural network ignores the dependence on t, as it does not have a built-in mechanism to capture the order or position of words in a sentence, like recent neural networks that work better at generating text.

## Recap

- Bi-gram model is a conditional probability model with context size one. You consider only the immediate previous word to predict the next one.
- Tri-gram model is also a conditional probability function and can improve on the bi-gram model's limitations by increasing the context size to two.
- The concept of a tri-gram can be generalized to an n-gram model, which allows for an arbitrary context size.
- Context vector:
	- Is product of your context size and the size of your vocabulary.
	- Is not computed directly, but is constructed by the embedding vectors close.