# Encoder-Decoder RNN Models: Translation

## Encoder-decoder architecture.
As you know, RNNs can be used to create sequence to sequence models that receive sequence x as input and generate sequence y as output.
Note that x and y do not necessarily need to be of the same length.
For this purpose, encoder decoder architectures are introduced.
![[Pasted image 20250313231848.png|400]]

A popular example of a sequence to sequence model is a translation model.
Let's check an actual example to see how encoder decoder RNNs generate translation.
The encoder decoder sequence to sequence model works together to transform an input sequence into an output sequence.
Encoder is a series of RNNs that process the input sequence, individually passing their hidden states to their next RNN.
The last hidden state context is passed to the decoder module.
The decoder module similarly is a series of RNNs that autoregressively generates the translation as one token at a time.
Each generated token goes back into the next RNN along with the hidden state to generate the next token of the output sequence until the end token is generated.
![[Pasted image 20250313232001.png|400]]

As RNNs become more complex, you can abstract away the finer details.
Here, an RNN is represented with an RNN cell.
The key distinction here is that the output from the previous state is recycled as input for the next state.
For the encoder, the interest lies only in the hidden state and to not use the output.
This is because only the decoder will generate the output text.
The encoder is only responsible for encoding the input sequence.
![[Pasted image 20250313232045.png|400]]

# RNN Encoder
The encoder consists of multiple encoder units, each responsible for processing input tokens.
At the heart of each unit is an embedding layer which transforms the input token into an embedded vector.
These vectors then traverse through the RNN cell to produce the hidden state, which is subsequently handed over to the RNN cell of the next encoder unit.
![[Pasted image 20250313232214.png|400]]

## Build an encoder in PyTorch
To build an encoder in PyTorch, define a class inheriting from torch.nn.module similar to a standard neural network.
The embedding vectors serve as input to a long short term memory or LSTM layer characterized by n layers, that is, the number of recurrent layers and hid dim, that is the size of the hidden and cell states.
Incorporate dropout as an optimization technique to enhance performance, in the LSTM layer input embeddings are transformed into hidden states and cell states, with hid dim specifying the number of neurons.
During the forward method execution, the embedding layer processes the input and the LSTM layer outputs the hidden and cell states.
Only these states are retained.
The output vector is discarded since it's not required by the encoder.
Remember, unlike gated recurrent units or GRUs, which have only hidden states, LSTMs maintain an additional cell state for each layer.
```python
class Encoder(nn.Module):
    def __init__(self, vocab_len, emb_dim, hid_dim, n_layers, dropout_prob):
        super().__init__()

        self.hid_dim = hid_dim
        self.n_layers = n_layers

        self.embedding = nn.Embedding(vocab_len, emb_dim)

        self.lstm = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout_prob)
        self.dropout = nn.Dropout(dropout_prob)

    def forward(self, input_batch):
        #input_batch = [src len, batch size]
        embed = self.dropout(self.embedding(input_batch))
        embed = embed.to(device)
        #outputs = [src len, batch size, hid dim * n directions]
        #hidden = [n layers * n directions, batch size, hid dim]
        #cell = [n layers * n directions, batch size, hid dim]
        outputs, (hidden, cell) = self.lstm(embed)

        return hidden, cell
```

## RNN Decoder
The decoder is also composed of many decoder units within which the predicted token is generated.
Each unit has an embedding layer, where embedded vectors are created.
These vectors then pass through the RNN cell to output the updated hidden state, which will be passed to the RNN cell of the subsequent decoder unit.
Next, a linear layer maps the RNN output to the output dimension to generate the next token.
As you can see, the decoder works a bit differently from the encoder in the way that it receives its previously generated token and generates the next token autoregressively.
![[Pasted image 20250313232645.png|400]]

## Build an decoder in PyTorch

To build a decoder in PyTorch, create a decoder class, which is a subclass of nn module serving as the base class for neural network modules.
In the constructor init method, the decoder's parameters and layers are initialized.
The parameters for the decoder are defined as follows.
Output dim is the number of possible output values or the target vocabulary length.
M dim is the dimensionality of the embedding layer.
Hid dim is the dimensionality of the hidden state in the LSTM.
N layers is the number of layers in the LSTM.
Dropout is the dropout probability.
The decoder includes an embedding layer that maps the output values to dense vectors of size m dim.
An lstm layer that takes the embedded input and produces hidden states of size hid dim.
Next, a linear layer maps the LSTM output to the output dimension output dim.
Finally, a soft max activation function is applied to the output, generating a probability distribution over the output values.
```python
class Decoder(nn.Module):
    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout):
        super().__init__()

        self.output_dim = output_dim
        self.hid_dim = hid_dim
        self.n_layers = n_layers

        self.embedding = nn.Embedding(output_dim, emb_dim)
        self.lstm = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout)
        self.fc_out = nn.Linear(hid_dim, output_dim)
        self.softmax = nn.LogSoftmax(dim=1)
        self.dropout = nn.Dropout(dropout)

    def forward(self, input, hidden, cell):
        #input = [batch size]

        #hidden = [n layers * n directions, batch size, hid dim]
        #cell = [n layers * n directions, batch size, hid dim]

        #n directions in the decoder will both always be 1, therefore:
        #hidden = [n layers, batch size, hid dim]
        #context = [n layers, batch size, hid dim]

        input = input.unsqueeze(0)
        #input = [1, batch size]

        embedded = self.dropout(self.embedding(input))
        #embedded = [1, batch size, emb dim]

        output, (hidden, cell) = self.lstm(embedded, (hidden, cell))
        #output = [seq len, batch size, hid dim * n directions]
        #hidden = [n layers * n directions, batch size, hid dim]
        #cell = [n layers * n directions, batch size, hid dim]

        #seq len and n directions will always be 1 in the decoder, therefore:
        #output = [1, batch size, hid dim]
        #hidden = [n layers, batch size, hid dim]
        #cell = [n layers, batch size, hid dim]
        prediction_logit = self.fc_out(output.squeeze(0))
        prediction = self.softmax(prediction_logit)
        #prediction = [batch size, output dim]

        return prediction, hidden, cell
```

## Construct sequence-to-sequence model
To construct the sequence to sequence model, define the sequence to sequence class inheriting from nn module with the encoder decoder device and target vocabulary trg vocab as inputs.
The forward method accepting source and target sequences and teacher forcing ratio manages the model's forward pass.
Teacher forcing is a training technique used in sequence to sequence models that involves using the true or correct output sequence from the training data as input to the decoder.
Instead of using the previously generated output from the model itself, it boosts model training.
Initialize batch size trg len and trg vocab size create output tensor to hold the decoder's predictions at each step.
Extract hidden and cell states from the encoder of using the encoder src and set them as initial states for the decoder.
Start the decoder with the first target sequence token.
For each target sequence time step, pass the input and prior states to the decoder, saving the output tensor and outputs.
Decide whether you need to apply teacher forcing at each step using the true target token trg T or the predicted one output argmax 1, depending on the teacher forcing ratio.
Then return the output's tensor which includes predictions for each time step.
```python
class Seq2Seq(nn.Module):
    def __init__(self, encoder, decoder, device,trg_vocab):
        super().__init__()

        self.encoder = encoder
        self.decoder = decoder
        self.device = device
        self.trg_vocab = trg_vocab

        assert encoder.hid_dim == decoder.hid_dim, \
            "Hidden dimensions of encoder and decoder must be equal!"
        assert encoder.n_layers == decoder.n_layers, \
            "Encoder and decoder must have equal number of layers!"

    def forward(self, src, trg, teacher_forcing_ratio = 0.5):
        #src = [src len, batch size]
        #trg = [trg len, batch size]
        #teacher_forcing_ratio is probability to use teacher forcing
        #e.g. if teacher_forcing_ratio is 0.75 you use ground-truth inputs 75% of the time

        batch_size = trg.shape[1]
        trg_len = trg.shape[0]
        trg_vocab_size = self.decoder.output_dim

        #tensor to store decoder outputs
        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)

        #last hidden state of the encoder is used as the initial hidden state of the decoder
        hidden, cell = self.encoder(src)
        hidden = hidden.to(device)
        cell = cell.to(device)

        #first input to the decoder is the <bos> tokens
        input = trg[0,:]

        for t in range(1, trg_len):
            #insert input token embedding, previous hidden and previous cell states
            #receive output tensor (predictions) and new hidden and cell states
            output, hidden, cell = self.decoder(input, hidden, cell)

            #place predictions in a tensor holding predictions for each token
            outputs[t] = output

            #decide if you are going to use teacher forcing or not
            teacher_force = random.random() < teacher_forcing_ratio

            #get the highest predicted token from your predictions
            top1 = output.argmax(1)

            #if teacher forcing, use actual next token as next input
            #if not, use predicted token
            #input = trg[t] if teacher_force else top1
            input = trg[t] if teacher_force else top1


        return outputs
```

## Recap
- RNNs can be used to create sequence to sequence models that receive one sequence as input, and generate another sequence as output.
- The encoder decoder architectures are introduced so that the sequences do not necessarily require to be of the same length.
- The encoder decoder sequence to sequence model works together to transform an input sequence into an output sequence.
- Encoder
	- is a series of RNNs that process the input sequence
	- individually passing their hidden states to their next RNN.
	- The last hidden state context is passed to the decoder module.
- The decoder module similarly, is a series of RNNs that autoregressively generate creates the translation as one token at a time.
- The embedding layer transforms the input token into an embedded vector, which traverses through the RNN cell to produce the hidden state.
