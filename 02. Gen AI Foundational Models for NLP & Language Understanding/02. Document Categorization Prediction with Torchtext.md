# Document Categorization Prediction with Torchtext

A document classifier seamlessly categorizes articles by analyzing the text content.
As demonstrated, you simply feed in the raw text and the system efficiently outputs the document's classification, whether it's `science` and `technology`, `sports`, `business`, or any other class.

## Quick recap of neural networks
A neural network is a mathematical function consisting of a sequence of matrix multiplications with a variety of other functions.
It begins with an input layer.
For instance, consider a bag of words vector.
In this layer, you'll perform matrix multiplication, occasionally adding a bias.
This constitutes the hidden layer if the input employs a bag of words.
This layer is known as the embedding layer and its output is referred to as the logits.
Subsequently, apply an activation function to each logit, a process known as activation where each element is called a neuron.
Sometimes an activation function or bias addition is not applied to embedding layers.
The operation is then repeated.
Another matrix multiplication is performed and once again each resulting element is termed a neuron.
Through this iterative process, the input data is transformed step by step, enabling the network to learn classification.
The parameters that the network adjusts during training are known as learnable parameters.
![[Pasted image 20250305142139.png|300]]

In the article, ESPNs varied football coverage input an embedding vector into the network.
For each category, the neural network outputs a vector of logits where each logit is a score reflecting the article's likelihood of fitting a particular news category.
As shown here, the associated table links vector elements to categories.
![[Pasted image 20250305142856.png|300]]

To determine the class of an article, you're going to input the logits from the output layer into the `argmax` function.
The `argmax` function identifies the index of the highest logit value corresponding to the most likely class.
Here, the `argmax` function selects index `1`, which has the highest logit value of seven
This indicates that the network classifies the article as belonging to the category associated with index `1`.
This corresponds to the article class of `sports`.
![[Pasted image 20250305142802.png|300]]

This graph illustrates the architecture of a neural network.
Each circle represents a neuron starting from the left.
Each neuron in the first box corresponds to an element in the input vector or input layer, and the connecting lines represent the weight matrix.
The neurons at each subsequent level signify the components of the hidden and output layers.
After processing through an activation function, obtain the hidden layer values, denoted as z.
The final layer's connections embody the weights leading to the output, with each neuron reflecting one of the four output classes.
After that, use argmax to find the class with the highest score.
![[Pasted image 20250305205901.png|400]]

## Neural network in PyTorch

You will use the `AG news` dataset using `torchtext`.
The output consists of text representing news articles along with their corresponding labels indicating their categories.
```python
train_iter= iter(AG_NEWS(split="train"))
y,text= next(train_iter)
print(y,text)
```
![[Pasted image 20250306112252.png|400]]

```python
ag_news_label = {1: "World", 2: "Sports", 3: "Business", 4: "Sci/Tec"}
ag_news_label[y]
ag_news_label
ag_news_label.values()
```

Let's build the standard text processing pipeline
```python
tokenizer = get_tokenizer("basic_english")

def yield_tokens(data_iter):
	for _,text in data_iter:
		yield tokenizer(text)

vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=["<unk>"])
vocab.set_default_index(vocab["<unk>"])

def text_pipeline(x):
	return vocab(tokenizer(x))
```

You should tweak your functions so that label numbering begins at zero
```python
def label_pipeline(x):
	return int(x) - 1
```

Now you'll set up a batch function for embedding bags and add code to append the labels for each sample to a batch
```python
def collate_batch(batch):

	label_list, text_list, offsets = [], [], [0]
	
	for_label, _text in batch:
	
		label_list.append(label_pipeline(_label))
		
		processed_text = torch.tensor(text_pipeline(_text),dtype=torch.int64)
		text_list.append(processed_text)
		offsets.append(processed_text.size(0))

label_list = torch.tensor(label_list, dtype=torch.int64)

offsets = torch.tensor(offsets[ :- 1]).cumsum(dim=0)
text_list = torch.cat(text_list)

return label_list.to(device), text_list.to(device), offsets.to(device)
```

Next, you'll create a data loader with a batch size of three
```python
BATCH_SIZE = 3

train_dataloader = Dataloader(AG_NEWS(split="train"), batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)
```

Let's explore the sample
Here you have three labels for each sample
Next are the token indices which form the basis for your bag of words model
Finally, observe the relative positions of these indices, which are essential for constructing the bag of words model
```python
label, text, offsets=next(iter(train_dataloader ))
label, text, offsets

# (tensor([0, 3, 2]),

# tensor([ 16, 9362, 16, 1265, 8607, 39539, 433, 380, 7, 2089,
# 641, 6, 5436, 5, 2286, 7, 3699, 3351, 33, 62,
# 37, 10514, 1, 148, 295, 3842, 941, 13712, 295, 179,
# 104, 37, 1534, 2436, 9036, 72, 2071, 1387, 7, 884,
# 662, 6, 941, 4, 388, 22, 3557, 822, 817, 1,
# 1385, 1, 986, 1, 9377, 1150, 3346, 153, 13, 27,
# 14, 27, 15, 577, 769, 1625, 1385, 1, 986, 1,
# 82366, 1, 64, 1, 13, 22573, 1, 137, 14, 10,
# 57, 708, 5, 3346, 153, 3, 57929, 5, 8355, 296,
# 3, 747, 24, 4238, 3756, 59855, 577, 769, 124, 1]),

# tensor([ 0, 23, 50]))
```

The model's architecture features two primary layers defined in the constructor
The first is the embedding bag layer, followed by the fully connected layer
The initialization of weights is also done during the forward pass
You'll input text and offset it into the embedding bag without applying the activation, which then feeds into the fully connected layer for the final output
```python
class TextClassificationModel(nn.Module):
	def init (self, vocab_size, embed_dim, num_class):
		super(TextClassificationModel, self) ._ init_()
		self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=False)
		self.fc = nn.Linear(embed_dim, num_class)
		self.init_weights()
	
	def init_weights(self):
		initrange = 0.5
		self.embedding.weight.data.uniform_(-initrange, initrange)
		self.fc.weight.data.uniform_(-initrange, initrange)
		self.fc.bias.data.zero_()
	
	def forward(self, text, offsets):
		embedded = self.embedding(text, offsets)
	return self.fc(embedded)
```

Model
```python
emsize=64

vocab_size=len(vocab)
num_class=4

model = TextClassificationModel(vocab_size, emsize, num_class)

predicted_label=model(text, offsets)
```

First up, you're looking at the logic values in PyTorch, each row represents a different sample
Columns correspond to the logic values for each class
Unlike the earlier row vector example, it's organized by columns
Here, you'll then apply the `argmax` function across each row
This identifies the maximum value in each row
The position of this maximum value indicates the predicted class for samples 0, 1, and 2
![[Pasted image 20250305212258.png|400]]

The prediction function works on real text that starts by taking in tokenized text.
It processes the text through the pipeline, and the model predicts the category.
The label with the highest output value is then returned as the predicted category.
When you apply the function, the article gets classified under `sports`
```python
def predict(text, text_pipeline):
	with torch.no_grad():
		text =torch.tensor(text_pipeline(text))
		output = model(text, torch.tensor([0]))
		return ag_news_label[output.argmax(1).item() + 1]

predict("I like sports",text_pipeline)

# 'Sports'
```
## Recap
- Document classifier seamlessly categorizes articles by analyzing the text content.
- Neural network is a mathematical function consisting of a sequence of matrix multiplications with a variety of other functions.
- Argmax function identifies the index of the highest logit value, corresponding to the most likely class.
- Hyperparameters are externally set configurations of a neural network.
- Prediction function:
	- Works on real text that starts by taking in tokenized text.
	- Processes the text through the pipeline, and the model predicts the category.