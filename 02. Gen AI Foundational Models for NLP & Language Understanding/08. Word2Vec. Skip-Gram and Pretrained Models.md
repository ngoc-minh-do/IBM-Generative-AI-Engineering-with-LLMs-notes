## Word2Vec: Skip-Gram and Pretrained Models

## Skip-gram model

A skip-gram model is a reverse of the Continuous Bag of Words (CBOW) model.
It predicts surrounding context words from a specific target word.
For instance, it aims to forecast words `t-1` and `t+1` for a given target word, `t`.
At t = 1, with `exercises` as the target, the model predicts `she` and `every`.
At t = 2, when `every` is the target, it aims to predict `exercises` and `day`.
The terminology may be confusing as the target is not the dependent variable.
![[Pasted image 20250312215259.png|400]]

In this depiction, `exercises` is the selected target word encoded as a one hot vector within the vocabulary space, marked by a 1 for `exercises` and 0 for all other terms.
The output layer is tasked with predicting the surrounding context words, `she` and `every`.
Ideally, after training, the models output for each context position should have the highest logic values for the actual context words.
![[Pasted image 20250312215400.png|400]]

The skip-gram model simplifies the task by predicting 1 context word at a time from a target word.
For example, for t =1, with `exercises` as the target, it predicts the preceding context word `she`.
Independently, it predicts the following context word `every`.
This makes the complex context prediction easier by breaking it into smaller manageable tasks.
Now let's examine the neural network.
At the input, you have one hot encoded vector for `exercises` and prediction is `she`.
![[Pasted image 20250312215709.png|400]]

The second pair is `exercises` and `every`.
![[Pasted image 20250312215746.png|400]]

## Create a skip-gram model in PyTorch
To create a skip-gram model, first you will initialize it.
Then you will define the embeddings layer using `nn.Embedding`, which creates word embeddings for the given vocabulary size and embedding dimension.
`fc` layer is a fully connected layer with an input dimension or embed dim and an output dimension of vocab_size.
In the forward method, the input text is passed through the embeddings layer to obtain word embeddings.
Next, you will apply activation and linear layers.
Finally, you will create an instance of the skip-gram model.
```python
class SkipGram_Model(nn.Module):

    def __init__(self, vocab_size, embed_dim):
        super(SkipGram_Model, self).__init__()
        # Define the embeddings layer
        self.embeddings = nn.Embedding(
            num_embeddings=vocab_size,
            embedding_dim=embed_dim
        )
        
        # Define the fully connected layer
        self.fc = nn.Linear(in_features=embed_dim, out_features=vocab_size)

    def forward(self, text):
        # Perform the forward pass
        # Pass the input text through the embeddings layer
        out = self.embeddings(text)
        
        # Pass the output of the embeddings layer through the fully connected layer
        # Apply the ReLU activation function
        out = torch.relu(out)
        out = self.fc(out)
        
        return out

emsize = 24
model_sg = SkipGram_Model(vocab_size, emsize).to(device)       
```

## Training the model
The sequence generation function is identical to CBOW, but with a switch in the order of the target and context.
This breakdown allows you to work with the full context in smaller parts.
The code segment breaks the context into smaller, manageable segments.
```python
# Define the window size for the context around the target word.
CONTEXT_SIZE = 2

# Initialize an empty list to store the (target, context) pairs.
skip_data = []

# Iterate over each word in the tokenized toy_data, while excluding the first 
# and last few words determined by the CONTEXT_SIZE.
for i in range(CONTEXT_SIZE, len(tokenized_toy_data) - CONTEXT_SIZE):

    # For a word at position i, the context comprises of words from the preceding CONTEXT_SIZE
    # as well as from the succeeding CONTEXT_SIZE. The context words are collected in a list.
    context = (
        [tokenized_toy_data[i - j - 1] for j in range(CONTEXT_SIZE)]  # Preceding words
        + [tokenized_toy_data[i + j + 1] for j in range(CONTEXT_SIZE)]  # Succeeding words
    )

    # The word at the current position i is taken as the target.
    target = tokenized_toy_data[i]

    # Append the (target, context) pair to the skip_data list.
    skip_data.append((target, context))

# skip_data = [
#	('i', ['wish', 'i', 'was', 'little'])
#	('was', ['i', 'wish', 'little', 'hit])
# ]
```

Let's now look at the first sample in the code snippet.
Each sample consists of two elements, the target word and its context.
The nested loop iterates through the context, that is, the second element of the sample, and appends each part to the target.
This results in the original full context being segmented into smaller discrete parts.
```python
skip_data_ = [[(sample[0],word) for word in  sample[1]] for sample in skip_data]
```
![[Pasted image 20250312220730.png|400]]

Next, you will flatten the data.
The collate function is similar to the CBOW model.
You will then create a data loader object.
```python
skip_data_flat = [item for items in skip_data_ for item in items]

def collate_fn(batch):
    target_list, context_list = [], []
    for _context, _target in batch:
        
        target_list.append(vocab[_target]) 
        context_list.append(vocab[_context])
        
    target_list = torch.tensor(target_list, dtype=torch.int64)
    context_list = torch.tensor(context_list, dtype=torch.int64)
    return target_list.to(device), context_list.to(device)
    
dataloader = DataLoader(skip_data_flat, batch_size=BATCH_SIZE, collate_fn=collate_fn)

target, context = next(iter(dataloader))
```

Let's examine a single batch from the data loader you have created.
The tensors are returned, including the target and context.
You can also observe the indices.
Additionally, the tokens are displayed here, which makes it quite intuitive.
![[Pasted image 20250313090607.png|400]]

To train the parameters, you will define the learning rate and loss function CrossEntropyLoss criterion.
The optimizer optimizes the parameters of the model which are obtained by model CBOW parameters.
For skip-gram model, you only need to change the input object of the optimizer.
Then you will define a learning rate scheduler.
```python
LR = 5  # learning rate

criterion = torch.nn.CrossEntropyLoss()
optimizer = torch.optim.SGD(model_sg.parameters(), lr=LR)
scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)
```

Now let's define the training function to train the model for a specified number of epochs.
You must include a condition to check whether the input is for `skip-gram` or `CBOW`.
The output of this function includes the trained model and a list of average losses for each epoch.
```python
def train_model(model, dataloader, criterion, optimizer, num_epochs=1000):
    # List to store running loss for each epoch
    epoch_losses = []

    for epoch in tqdm(range(num_epochs)):
        # Storing running loss values for the current epoch
        running_loss = 0.0

        # Using tqdm for a progress bar
        for idx, samples in enumerate(dataloader):

            optimizer.zero_grad()
            
            # Check for EmbeddingBag layer in the model
            if any(isinstance(module, nn.EmbeddingBag) for _, module in model.named_modules()):
                target, context, offsets = samples
                predicted = model(context, offsets)
            
            # Check for Embedding layer in the model
            elif any(isinstance(module, nn.Embedding) for _, module in model.named_modules()):
                target, context = samples
                predicted = model(context)
                
            loss = criterion(predicted, target)
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)
            optimizer.step()
            running_loss += loss.item()

        # Append average loss for the epoch
        epoch_losses.append(running_loss / len(dataloader))
    return model, epoch_losses
```

Now to retrieve word embeddings, you will call the train function with the data loader you created for training the CBOW model.
Once the model is trained, you can retrieve the weights which are the actual word embeddings.
You can get the embedding vector for a specific word by its index.
```python
model_cbow, epoch_losses=train_model(model_cbow, dataloader_cbow, criterion, optimizer, num_epochs=400)

word_embeddings = model_cbow.embedding.weight.detach().cpu().numpy() 

word = 'baller'
word_index = vocab.get_stoi()[word] # getting the index of the word in the vocab
print(word_embeddings[word_index])
```

## Text classification using pre trained embeddings

- Stanford GloVe leverage large-scale data for word embeddings

Now you will learn about text classification using pretrained word embeddings.
Word embeddings utilize Stanford's pretrained GloVe or global vectors that leverage large scale data for word embeddings.
It can be integrated into PyTorch via `torchtext.vocab` for improved NLP tasks such as classification.
You will initialize GloVe with GloVe within parentheses, name equals within single quotes 6B, to load pre trained vectors and create a custom vocab object to match different tokens.
```python
from torchtext.vocab import GloVe,vocab

# creating an instance of the 6B version of Glove() model
glove_vectors_6B = GloVe(name ='6B') # you can specify the model with the following format: GloVe(name='840B', dim=300)

# Build vocab from glove_vectors
vocab = vocab(glove_vectors_6B.stoi, 0,specials=('<unk>', '<pad>'))
vocab.set_default_index(vocab["<unk>"])
```

You've developed a text classification model and integrated GloVe word embeddings into a PyTorch layer.
You use dot from pretrained with GloVe vectors, 6B.vectors.
You will create a word amending bag operation.
You can also try setting `freeze` to false if you have a larger dataset.
```python
class TextClassificationModel(nn.Module):
    def __init__(self, vocab_size, embed_dim, num_class):
        super(TextClassificationModel, self).__init__()
        self.embedding = torch.nn.Embedding.from_pretrained(glove_vectors_6B.vectors,freeze=True)
        self.fc = nn.Linear(embed_dim, num_class)
        self.init_weights()

    def init_weights(self):
        initrange = 0.5
        self.embedding.weight.data.uniform_(-initrange, initrange)
        self.fc.weight.data.uniform_(-initrange, initrange)
        self.fc.bias.data.zero_()

    def forward(self, text,offsets):
        embedded = self.embedding(text)
        # you get the average of word embeddings in the text
        means = []
        for i in range(1,len(offsets)):
			#this is like eme
          text_tmp = embedded[(offsets[i-1]:offsets[i]]
          means.append(text_tmp.mean(0))

        return self.fc(torch.stack(means))
```

## Recap
- Skip-gram model predicts surrounding context words from a specific target word.
- Skip-gram operates in contrast to the CBOW model.
- Skip-gram model simplifies the task by predicting one context word at a time from a target word.
- The sequence generation function in the creation of the skip-gram model is identical to CBOW, but with a switch in the order of the target and context.
- This breakdown allows you to work with the full context in smaller parts.
- While training the parameters, you define the:
	- learning rate
	- loss function
	- optimizer
	- learning rate scheduler.
- Once the model is trained, you can retrieve the weights, which are the actual word embeddings.
- Word embeddings utilize Stanford pretrained GloVe or global vectors that leverage large scale data for word embeddings.