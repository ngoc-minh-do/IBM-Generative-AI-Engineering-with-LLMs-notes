# Language Modeling with N-Grams with PyTorch

In PyTorch, you can create an `embedding` layer with an arbitrary vocabulary size and set the embedding dimension and a context size of two.
The next layer's input dimension must be the product of this context vector embedding dimension and the context size.
```python
embedding_dim=3
context_size=2
vocab_size=4

embedding = nn.Embedding(vocab_size, embedding_dim)
linear = nn.Linear(context_size * embedding_dim, 2)
```

Start with two indices representing the input context size of 2.
These two samples are used as input for the embedding layer.
The output is two embedding vectors of dimension 3.
Use the method reshape to reshape the embeddings to form the context vector, which is the context size or number of samples multiplied by the embedding dimension.
This concatenates all the samples together.
This is used as an input to the next layer.
```python
inputs=torch.tensor([2,3])
my_embeddings=embeddings(inputs)

my_embeddings_c = torch.reshape(my_embeddings, (-1, context_size * embedding_dim))

linear1(my_embeddings_c)
```

## N-gram language model

In PyTorch, the n-gram language model is essentially a classification model using the context vector and an extra hidden layer to enhance performance.
```python
class NGramLanguageModeler(nn.Module):

    def __init__(self, vocab_size, embedding_dim, context_size):
        super(NGramLanguageModeler, self).__init__()
        self.context_size=context_size
        self.embedding_dim=embedding_dim
        self.embeddings = nn.Embedding(vocab_size, embedding_dim)
        self.linear1 = nn.Linear(context_size * embedding_dim, 128)
        self.linear2 = nn.Linear(128, vocab_size)

    def forward(self, inputs):
        embeds = self.embeddings(inputs)
        embeds=torch.reshape( embeds, (-1,self.context_size * self.embedding_dim))
        out = F.relu(self.linear1(embeds))
        out = self.linear2(out)

        return out
```

The n-gram model predicts words surrounding a target by incrementally shifting what's known as a sliding window.
In a bigram model, the prediction for words at position t is based on the positions t-1 and t-2, beginning at t = 3 to preclude negative indices.
Each table row represents a distinct value that aligns with the words position.
Column 2 displays the context and column 3 shows the predicted word, both dependent on row t.
Consider the phrase, I like vacations, with each word's index beneath it.
For t = 3, the context is I like shown in blue and the predicted target word is vacations in red.
Moving to t = 4, presented in row 2, the context updates to like vacations with and next prediction.
This method is applied throughout the entire sequence.
![[Pasted image 20250307095019.png|300]]

## Context-target pairs
In PyTorch for n-gram modeling, implement windowing to create batches of context and target words.
It initiates the process where targets and contexts are generated using a batch function.
A for loop iterates from the start of the context size, which in this example begins at two marking vacation as your initial target within the window.
`i` is the target index. The context is gathered by subtracting `j` from, then slide the window forward, incrementing `i` to capture successive targets and contexts for the entire sequence.
```python
CONTEXT_SIZE=3
BATCH_SIZE=10
EMBEDDING_DIM = 10

def collate_batch(batch):
    batch_size=len(batch)
    context, target=[],[]
    
    for i in range(CONTEXT_SIZE,batch_size):
        target.append(vocab([batch[i]]))
        context.append(vocab([batch[i-j-1] for j in range(CONTEXT_SIZE)]))

    return torch.tensor(context).to(device), torch.tensor(target).to(device).reshape(-1)
```

Here, you are going to create a toy data set.
```python
song= """We are no strangers to love
You know the rules and so do I
A full commitments what Im thinking of
...
"""
```

## Text pipeline
Now, you'll create a pipeline that converts the text to indexes.
Instead of using a dataset object, you will use a list object.
```python
tokenizer = get_tokenizer("basic_english")
tokens=tokenizer(song)

vocab = build_vocab_from_iterator(tokenized_song, specials=["<unk>"])
vocab.set_default_index(vocab["<unk>"])

text_pipeline = lambda x: vocab(tokenizer(x))
```

## Training the model
In training the model, prioritize the loss over accuracy as your key performance indicator.
```python
def train(dataloader, model, number_of_epochs=100, show=10):
    MY_LOSS = []

    for epoch in tqdm(range(number_of_epochs)):
        total_loss = 0

        for context, target in dataloader:
            model.zero_grad()          # Zero the gradients to avoid accumulation
            predicted = model(context)  # Forward pass through the model to get predictions
            loss = criterion(predicted, target.reshape(-1))  # Calculate the loss
            total_loss += loss.item()   # Accumulate the loss

            loss.backward()    # Backpropagation to compute gradients
            optimizer.step()   # Update model parameters using the optimizer

        MY_LOSS.append(total_loss/len(dataloader))  # Append the total loss for the epoch to MY_LOSS list

    return MY_LOSS  # Return the list of  mean loss values for each epoch
```


Train the model similarly to your classification model by padding tokens to ensure consistent shape.
Here, you will pad with previous values for alignment.
```python
Padding=BATCH_SIZE-len(tokens)%BATCH_SIZE
tokens_pad=tokens+tokens[0:Padding]

dataloader = DataLoader(
     tokens_pad, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_batch
)

criterion = torch.nn.CrossEntropyLoss()

model = NGramLanguageModeler(len(vocab), EMBEDDING_DIM, CONTEXT_SIZE).to(device)

optimizer = optim.SGD(model_2.parameters(), lr=0.01)
```

The `index_to_token` attribute obtained via `vocab.get_itos()` is a list where each element corresponds to a word and the index within the list corresponds to that word's token index.
This list serves as a mapping to translate the numerical output of a neural network, which could represent class or token indices back into a human readable format, essentially acting as a decoder.
Thus, when a neural network predicts an index, this list can be used to retrieve the associated word.
```python
index_to_token = vocab.get_itos()
index_to_token

# ['<unk>',
# 'gonna',
# 'you',
# 'never',
# ....]
```

## Make prediction
Apply a text processing pipeline to the string `Never gonna`.
The result is the list `[3, 1]`, which represents the token indices.
Convert the list of token indices into a PyTorch tensor.
Make a prediction with the model and select the index with the largest value.
Finally, convert the index to a token using the index_to_token mapping.
```python
context = text_pipeline("Never gonna")
# [3, 1]

x_c = torch.tensor(context).reshape(-1, 1)
# tensor([[3], [1]])

out=model(x_c)
predicted_index = torch.argmax(out, 1)
# tensor([42])

index_to_token[predicted_index]
# hearts
```

## Generating a song
You can use this function to generate a sequence of words using a model.
```python
def write_song(model,number_of_words=100):
    my_song=""
    for i in range(number_of_words):
        with torch.no_grad():
            context=torch.tensor(vocab([tokens[i-j-1] for j in range(CONTEXT_SIZE)])).to(device)
            word_inx=torch.argmax(model(context))
            my_song+=" "+index_to_token[word_inx.detach().item()]

    return my_song
```

## Recap
- N-gram model allows for an arbitrary context size.
- N-gram language model is essentially a classification model using the context vector and an extra hidden layer to enhance performance.
- It predicts words surrounding a target by incrementally shifting it as a sliding window.
- In training, the model, prioritize the loss over accuracy as your key performance indicator or KPI.