# Introduction to Sequence-to-Sequence Models and Recurrent Neural Network

## Sequence-to-Sequence Models
Are used in:
- machine translation, such as converting English phrases into French.
- Chatbots transform your queries into conversational responses
- Text summation, summarization algorithms condense extensive texts into concise summaries.
- Code generation, you describe your task and the AI generates the appropriate code.

Sequence-to-sequence models process multiple inputs, here illustrated by five embedding vectors labeled x1 to x5 corresponding to time units.
The outputs marked y1 to y5 typically represent tokens but can vary widely with many applications.
![[Pasted image 20250313095443.png|400]]

Notably, input and output sequences need not be of equal length.
For instance, sequence-to-label tasks take multiple inputs to produce a single label, useful in document classification.
![[Pasted image 20250313095532.png|400]]

Conversely, label-to-sequence tasks generate a full sequence from a single input, as seen in generative models for image creation.
![[Pasted image 20250313095606.png|400]]

Context is vital for accurately interpreting language.
Take the sentences, the `man bites the dog` and` the dog bites the man`.
While a `bag of words` model cannot differentiate between them, treating them as similar due to identical word frequencies, sequence representation through one hot encoded words, or embeddings captures their distinct meanings.
The difference is highlighted in red, illustrating how such representations enable a more precise understanding of context.
Neural networks typically operate under the assumption that each sample is independent and identically distributed, or IID.
![[Pasted image 20250313095750.png|400]]

To illustrate, imagine drawing a ball labeled with variable y_t from an URN multiple times.
If each draw is independent and the URN's contents are unchanged, the t is redundant, and the probability distribution remains constant, fulfilling the IID assumption.
![[Pasted image 20250313100208.png|400]]

However, consider the scenario where the balls are not replaced after each draw.
In this case, the probability for y at t = 1 is 1/10.
But the probability at t = 2 depends on the previous draw at t = 1.
This necessitates adjusting the probability distribution after each draw, leading to the concept of conditional distributions, the model needs memory.
Therefore, you require a model that can account for this dependency using models that have memory.
![[Pasted image 20250313100243.png|400]]

In sequence model training, you'll refine your data, starting with a collection of sentences punctuated by BOS and EOS tags to denote beginnings and ends.
This clear demarcation helps the model recognize start and stop points within a sequence.
Next, sort the sentences by length to batch together those of similar size, which streamlines the learning process.
PyTorch, like other frameworks, requires consistent batch sizes.
Therefore, you should append pad symbols to the shorter sentences to equalize their lengths with the rest.
![[Pasted image 20250313121747.png|500]]

## Recurrent neural networks (RNN)

-  is a type of artificial neural network that uses sequential or time series data.
- As its name suggests, RNN is designed to remember past information and use it to influence future decisions.

Let's explore how a simple RNN operates.
First, look at the input layer labeled as x_t.
This is where your RNN receives data at each specific timestep.
It's akin to receiving a new puzzle piece at every moment.
![[Pasted image 20250313122039.png|400]]

Next, you have the heart of your RNN, the hidden state denoted as h_t.
Think of it as the network's memory, where you will apply an activation function, usually at tanh.
It captures and retains information from all the previous inputs.
This is crucial because it allows the network to remember and consider past data while processing the current input.
![[Pasted image 20250313122111.png|400]]

Now observe how the hidden state and the current input combine.
In some RNNs, they're concatenated, forming a larger set of data for processing in what is also called the concatenated layer.
This is critical for understanding the network's ability to connect past and present information.
Let's move to the output represented by z_t.
This is what the RNN calculates at each timestep based on the current input and what it has remembered in its hidden state, this is where the context is stored.
This process is similar to a regular neural network.
For the classification problem, you can simply obtain the maximum value and use RNNs in hat y in language modeling to predict the word Omega hat.
This method is called greedy, decoding where the model picks the highest score token and returns it as its prediction.
Please note that top-k sampling strategies seem to produce more fluent text than the traditional greedy method.
![[Pasted image 20250313223446.png|400]]

Let's unroll the RNN over time.
Start with the initial hidden state, that is a vector of zeros.
Then with each input x_t, the RNN updates its hidden state h_t and produces the output hat y_t.
For the next iteration, introduce a new input x_t.
The network utilizes the hidden state from the first timestep, learning from the past to inform the future, and then the process repeats.
To output a sequence hat y_t, you sometimes need to use only the hidden state, or you can even use the last hidden state h_t as an input to another RNN.
![[Pasted image 20250313224724.png|400]]
## RNN improvements
RNNs only remember short-term information and are challenging to train.
Two popular RNN enhancements are:
- Gated recurrent units, or (GRUs)
- Long short-term memory (LSTMs)
In a GRU, there are two gates: the reset r and the update z.
The update gate determines the proportion of the previous hidden state to carry forward, while the reset gate decides how much of the previous hidden state to disregard.
Working together, they update the hidden state and control the flow of information over time.
Finally, another activation function is applied before the model makes a prediction.
The LSTM cell comprises the input gate, the forget gate, and the output gate.
LSTMs extend a network's memory, complementing short-term with long-term recall.
They selectively retain and transport crucial data through time.
The diagram shows h as short-term memory, discerning what's important.
It selectively filters specific information from c that is pertinent to the current timestep.
Conversely, c preserves the full scope of the memory to be conveyed to subsequent timesteps.
![[Pasted image 20250313223922.png]]

# Recap
- sequence-to-sequence models within generative AI are used in machine translation
- Sequence-to-label tasks take multiple inputs to produce a single label, useful in document classification.
- Label-to-sequence tasks generate a full sequence from a single input, as seen in generative models for image creation.
- RNN 
	- is a type of artificial neural network that uses sequential or time series data.
	- is designed to remember past information and use it to influence future decisions.
	- RNNs only remember short-term information and are challenging to train.
	- Two popular RNN enhancements are gated recurrent units, or GRUs, and long short-term memory, or LSTMs.