# Encoder-Decoder RNN Models: Training and Inference

## Learning objective
- Use a translation dataset in PyTorch to train and infer an encoder decoder RNN based model.
- You'll be working with the `Multi30K` dataset which includes training validation and test sets of English to German text.

Before delving into implementing the sequence to sequence or seek to seek model, let's review how to load data in batches for training models using PyTorch's data loader.
A `Multi30K_de_en_dataloader.py` file has been created that fetches the multi 30 K dataset.
The collation, that is tokenization, numericalization, and addition of beginning of sequence or BOS, end of sequence or EOS, and padding have also been done.
You will also see the created iterable batches of SRC and TRG tensors.

As a first step, you must download and run the `Multi30K_de_en_dataloader.py` file, then call the get translation data loaders function with an arbitrary batch size to create the data loaders for training and validation.
You can also check a data sample.
```python
!wget 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-AI0205EN-SkillsNetwork/Multi30K_de_en_dataloader.py'

%run Multi30K_de_en_dataloader.py

train_dataloader, valid_dataloader = get_translation_dataloaders(batch_size = 4))

src, trg = next(iter(train_dataloader))
src,trg
```
![[Pasted image 20250313230203.png| 300]]

In general, sequence to sequence models are more difficult to train than RNNs and several factors come into play.
However, typically the aim is to minimize the cross entropy loss by summoning the output of the predicted outcomes with the actual labels.
Let's use blue to denote a correct prediction.
![[Pasted image 20250313230252.png|400]]
## Training
The procedure for training sequence to sequence models has almost similar steps, but let's highlight similar differences from neural networks.
First, initialize the model in training mode to activate essential layers like dropout, ensuring optimal performance during training.
Iterate through training data batches, assign input SRC and target TRG sequences to the correct device, and generate predictions by output.
For sequence models like RNNs where the input shape often differs from other model types, it's crucial to reshape the outputs tensor too here.
Here, `trg[1:]` represents the rows excluding the initial BOS or beginning of sequence token, ensuring you don't include it in loss computation.
Batch size is the columns, each representing a separate sequence in the batch and output dimension corresponds to the columns representing the predicted output for each token in the sequence.
This reshaping aligns the rows and columns correctly for loss calculation, matching the output predictions to the target dimensions.
Finally, you can calculate the average loss per batch after processing all batches
```python
def train(model, iterator, optimizer, criterion, clip):
    model.train()
    epoch_loss = 0

    # Wrap iterator with tqdm for progress logging
    train_iterator = tqdm(iterator, desc="Training", leave=False)

    for i, (src,trg) in enumerate(iterator):

        src = src.to(device)
        trg = trg.to(device)
        optimizer.zero_grad()

        output = model(src, trg)

        output_dim = output.shape[-1]
        output = output[1:].view(-1, output_dim)

        trg = trg[1:].contiguous().view(-1)

        loss = criterion(output, trg)
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)
        optimizer.step()

        # Update tqdm progress bar with the current loss
        train_iterator.set_postfix(loss=loss.item())
        epoch_loss += loss.item()

    return epoch_loss / len(list(iterator))
```

## Evaluate
You will also create and evaluate function that is identical to the training function, but you must use different data and set the model to valve to speed it up.
```python
def evaluate(model, iterator, criterion):
    model.eval()
    epoch_loss = 0
    # Wrap iterator with tqdm for progress logging
    valid_iterator = tqdm(iterator, desc="Training", leave=False)

    with torch.no_grad():

        for i, (src,trg) in enumerate(iterator):

            src = src.to(device)
            trg = trg.to(device)

            output = model(src, trg, 0) #turn off teacher forcing

            output_dim = output.shape[-1]
            output = output[1:].view(-1, output_dim)
            trg = trg[1:].contiguous().view(-1)

            loss = criterion(output, trg)
            # Update tqdm progress bar with the current loss
            valid_iterator.set_postfix(loss=loss.item())
            epoch_loss += loss.item()

    return epoch_loss / len(list(iterator))
```

## Prediction
Let's see how to make a prediction.
Performing translations in sequence models requires a more complex function for making predictions.
The function takes a model, a source sentence, a target language vocabulary, and a maximum translation length.
Firstly, convert the source sentence to the correct format, then feed it into the models encoder to obtain hidden and sell states.
Start the target tensor with the BOS token to initialize the translation and reshape it.
To generate the translation iterate up to Max Len.
Utilize the last target tensor and the previous states in the decoder to obtain new outputs and states.
Then choose the next token with the highest probability, add it to the translation and store it.
End the process if the EOS token appears else feed the output to the input.
Finally, convert token indices to words, remove special tokens, and concatenate the tokens to form the translated sentence.
```python
import torch.nn.functional as F

def generate_translation(model, src_sentence, src_vocab, trg_vocab, max_len=50):
    model.eval()  # Set the model to evaluation mode

    with torch.no_grad():
        src_tensor = text_transform[SRC_LANGUAGE](src_sentence).view(-1, 1).to(device)

        # Pass the source tensor through the encoder
        hidden, cell = model.encoder(src_tensor)

        # Create a tensor to store the generated translation
        # get_stoi() maps tokens to indices
        trg_indexes = [trg_vocab.get_stoi()['<bos>']]  # Start with <bos> token

        # Convert the initial token to a PyTorch tensor
        trg_tensor = torch.LongTensor(trg_indexes).unsqueeze(1)  # Add batch dimension

        # Move the tensor to the same device as the model
        trg_tensor = trg_tensor.to(model.device)

		# Generate the translation
        for _ in range(max_len):

            # Pass the target tensor and the previous hidden and cell states through the decoder
            output, hidden, cell = model.decoder(trg_tensor[-1], hidden, cell)

            # Get the predicted next token
            pred_token = output.argmax(1)[-1].item()

            # Append the predicted token to the translation
            trg_indexes.append(pred_token)

            # If the predicted token is the <eos> token, stop generating
            if pred_token == trg_vocab.get_stoi()['<eos>']:
                break

            # Convert the predicted token to a PyTorch tensor
            trg_tensor = torch.LongTensor(trg_indexes).unsqueeze(1)  # Add batch dimension

            # Move the tensor to the same device as the model
            trg_tensor = trg_tensor.to(model.device)

        # Convert the generated tokens to text
        # get_itos() maps indices to tokens
        trg_tokens = [trg_vocab.get_itos()[i] for i in trg_indexes]

        # Remove the <sos> and <eos> from the translation
        if trg_tokens[0] == '<bos>':
            trg_tokens = trg_tokens[1:]
        if trg_tokens[-1] == '<eos>':
            trg_tokens = trg_tokens[:-1]

        # Return the translation list as a string

        translation = " ".join(trg_tokens)

        return translation
```

## Recap
- In general, sequence to sequence models are more difficult to train than RNNs due to several factors.
- In model training, the aim is to minimize the cross entropy loss by summing the output of the predicted outcomes with the actual labels.
- The procedure for training sequence to sequence models includes:
	- initializing the model and training mode to activate essential layers like dropout, ensuring optimal performance during training
	- Iterating through training data batches, assigning input SRC and target TRG sequences to the correct device
	- Generating predictions by output
	- Reshaping the outputs tensor, which aligns the rows and columns correctly for loss calculation.
	- calculating the average loss per batch after processing all batches.
- Performing translations in sequence models requires complex functions for making predictions.