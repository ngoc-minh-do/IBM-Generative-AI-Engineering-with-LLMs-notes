# Converting Words to Features

## One-hot encoding

One-hot encoding is a method used to convert categorical data into feature vectors that a neural network can understand. 

Consider a table with the first column representing the token index and the second column representing the token, the third column represents the one-hot encoded vector. 
The dimension of the vector corresponds to the number of words in the vocabulary and the token index represents the element of each vector. 
![[Pasted image 20250304223406.png|300]]

The token `I` is represented with all elements set to zero except for the one corresponding to `I`, this will be the feature x for the word I. 

The token `like` is represented in a similar way, you can observe a pattern, this will be the feature x for the word like. 

The token cats is represented in a similar way and so on. 
![[Pasted image 20250304223840.png|300]]

You can represent each token in a document as a vector. ![[Pasted image 20250304223923.png|100]]
## Bag-of-words

The `bag-of-words` representation portrays a document as the aggregate or average of one-hot encoded vectors. 
For `I like cats`, you combine the one-hot vectors, you add the one-hot vectors for I like and cats. 

The x vector is the `bag-of-words` vector for cats. 
![[Pasted image 20250304224350.png|300]]

## Embedding and embedding bags. 

Consider a neural network used to classify the previous sentences about cats, dogs or hippos. 
You input the `bag-of-words` vector, and select the output value with the highest association for cats. 
You'll understand how the embedding and embedding bag layers can effectively replace this initial linear layer. 
![[Pasted image 20250304224802.png|500]]

Let's look at the hidden layer when we input the one-hot vector for the word cat, which has zeros everywhere except the seventh position, the activation in the hidden layer corresponds to the parameters of the seventh neuron.
![[Pasted image 20250304225118.png|500]]

Instead of using a one-hot encoded vector, you can substitute it with the token index. 
In this instance seven, the output mirrors the one-hot vector. 

The layer that accepts this index is called the embedding layer, and its output is the embedding vector. 
![[Pasted image 20250304225404.png|500]]

The embedding weights are combined to form an embedding matrix. 
The number of columns is the embedding dimension, each row represents a word. 
![[Pasted image 20250304225732.png|300]]

Embedding vectors generally have a lower dimensionality compared to one-hot encoded vectors. 
Reducing the dimensionality simplifies the computational requirements for the model. 

When you feed a bag of words vector to a neural network's hidden layer, the output is just the sum of the embeddings.
![[Pasted image 20250304225924.png|300]]

It also helps to view embedding as a row in the embedding matrix, labeling them with the word they represent. 
This process entails summing up all the vectors from `bag-of-words` and multiplying the result with the embedding matrix.
![[Pasted image 20250304230136.png|200]] ->![[Pasted image 20250304230208.png|450]] 

Instead of doing this, you can use an embedding bag layer. 
The input is simply the indexes for each token and the output is the sum of word embeddings. 
![[Pasted image 20250304230421.png|500]]

## Embeddings and embedding bag in PyTorch. 

First, you need the tokens. 
Initialize the tokenizer iterator from the dataset and vocabulary. 
```python
dataset = [
"I like cats",
"I hate dogs",
"I'm impartial to hippos"
]

# Define tokenizer and yield_tokens
tokenizer = get_tokenizer("spacy", language='en_core_web_sm)

def yield_tokens(data_iter):
    for data_sample in data_iter:
        yield tokenizer(data_sample)

# Build vocabulary
vocab = build_vocab_from_iterator(yield_tokens(train_iter))
```

The `input_ids` function tokenizes and generates the indexes for each data sample. 
These indexes are stored in the list index where each element in the list is the indexes for the different documents. 
```python
input_ids=lambda x:[torch.tensor(vocab(tokenizer(data_sample))) for data_sample in dataset]

index=input_ids(dataset)

print(index)
# [tensor([0, 7, 2]), tensor([0, 4, 3]), tensor([0, 1, 6, 8, 5])]
```

You can initiate the embedding layer and specify the dimension size of the embeddings. 
Next, determine the count of unique tokens present in the vocabulary. 
Now, create the embedding layer embeds using the `nn.Embedding` constructor. 
```python
embedding_dim = 3
n_embedding = len(vocab)
# n_embedding:9
embeds = nn.Embedding(n_embedding, embedding_dim)

# Embedding(9, 3)
```

You apply the embedding object. 
The input is the index for the phrase `I like cats` to retrieve the embedding. 
What you'll see is a PyTorch tensor representation. 
Each row in this tensor aligns with the respective embedding for the words `I`, `like` and `cats`. 
```python
i_like_cats=embeds(index[0])
i_like_cats

# tensor([[-0.4826, 0.4928, 1.4965],      # I
# [0.4324, -0.8601, -0.2977],             # like
# [0.2466, 1.1865, -0.0031]])             # cats
```

Initializing the embedding bag layer is almost the same as the embedding layer. 
```python
embedding_dim = 3
n_embedding = len(vocab)
# n_embedding:9
embedding_bag = nn.EmbeddingBag(n_embedding, embedding_dim)

# Embedding(9, 3)
```

Let's output the embedding bag for the first document using its index, you can access the embedding for `I like cats`. 
The outcome is a PyTorch tensor representing the sum of average of all embeddings. 
```python
i_like_cats=embedding_bag(index[0], offsets=torch.tensor([0]))
i_like_cats

# tensor([[0.0550, -0.4719, -0.1624]],              # I like cats
```

For one sample, the offset parameter is always zero. 
Let's explore the offset parameter. 
In NLP datasets are often represented as one dimensional tensors. 
However, when working with embedding bags and other applications, it's crucial to identify the position of each document. 
Let's dive into this with our sample dataset. 
You combine the tensors of individual documents using the `cat` function with the index setting. 
The offset parameter captures the starting position of every document. 
```python
# index:[tensor([0, 7, 2]), tensor([0, 4, 3]), tensor([0, 1, 6, 8, 5])]

index_flat=torch.cat(index)
# tensor([0, 7, 2, 0, 4, 3, 0, 1, 6, 8, 5])
```

**Offset parameter**
You can count the tokens in each sample. 
This step helps in pinpointing the initial position. 
Using the `cumulative sum` method, you can accumulate the lengths, thereby determining the starting position of each sequence. 
```python
offset=[len(sample) for sample in index]
offset.insert(0, 0)
# [0, 3, 3, 5]

offset=torch.cumsum(offset, 0)[0:-1]
# tensor([0, 3, 6])
```

**Embedding bag with offset parameter**
You use the embedding bag function along with the offset parameter as with the index tensor. 
The result is the embedding bag. 
For each individual document you have the average of its words, embeddings. 
```python
my_embeddings=embedding_bag(index_flat, offsets=offset)

# tensor([[-0.5663, -0.3031, 0.2618],               # I like cats
# [-1.1062, 0.8861, 1.4102],                        # I hate dogs
# [-0.4400, 0.1588, 0.1175]],                       # I'm impartial to hippos
```













