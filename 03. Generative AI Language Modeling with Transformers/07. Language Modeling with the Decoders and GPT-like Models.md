# Language Modeling with the Decoders and GPT-like Models

## Decoder
Transformers initially developed for language translation tasks comprise two principal components, an encoder and a decoder.
These architectures have evolved with the decoder playing a pivotal role in text generation, serving as the foundation for sophisticated models such as GPT, LLaMA, and Granite.
![[Pasted image 20250319002854.png|400]]

## Decoders: Cheat sheet
This cheat sheet will help you summarize how decoder models work.
Generative pre-training, or GPT, is self-supervised and involves training a decoder to predict the subsequent token or word in a sequence given a set of words.
Autoregressive models facilitate sequence generation by anticipating each new token, such as words based on the sequences' preceding tokens, adhering to the methodology outlined above.
![[Pasted image 20250319003119.png|400]]

Fine-tuning is a supervised process that optimizes the initially trained GPT model for specific tasks like QA classification.
Reinforcement learning from human feedback (RLHF), represents a fine tuning approach that enhances model performance on specific tasks, proving particularly effective in chatbot development.
![[Pasted image 20250319003839.png|400]]

A notable distinction in text generation is the decoder's operation without the encoder's input, diverging from its role in translation, where it relies on cross attention with the encoder.
![[Pasted image 20250319003932.png|450]]->![[Pasted image 20250319004018.png|230]]

This unique operation in text generation involves predicting the subsequent word based on the preceding sequence of tokens.
![[Pasted image 20250319004208.png|400]]

## GPT
GPT is based on the decoder model.
A decoder functions autoregressively when it forecasts future words based on previously encountered words.
This is illustrated by the process where the model predicts the word `IBM`, starting with the beginning of sentence token `<BOS>`.
This predicted word `IBM` is then appended to the initial `<BOS>` token.
Consequently, the model now processes the input sequence `<BOS> IBM`, leading it to predict the subsequent word `taught`.
Such a method is intricately designed to mimic human-like text prediction, sequentially predicting each new word in a chain given the context of the preceding words.
![[Pasted image 20250319085336.png|300]]

The principal distinction between encoders and decoders in transformer architectures lies in the use of masked self-attention for decoders.
Decoders leverage an attention mechanism, which at its core involves matrix multiplication.
During training, the entire sequence is fed into the model.
Masking is a critical technique that ensures the model only attends to previous tokens in the sequence when making predictions, facilitating autoregressive generation.
It is important to note that masking is employed during inference or prediction as well, acting as an intermediary step to preserve the autoregressive property.
You can subsequently use an encoder as a decoder for text generation if you add a mask.
You will focus on masking when you learn to train the decoders.
![[Pasted image 20250319085423.png|400]]

## Decoder for language model
Now lets transition to the autoregressive process of generating translation using the model.
Lets explore how decoder models such as GPT generate text by predicting the next token in a sequence.
Take a question answering scenario where the model responds with `good, thanks`, to the prompt `how are you?`. Here's an overview of the generation process.
The process begins with the prompt `how are you?`. In some models, the `<BOS>` token is added.
This prompt is tokenized into individual tokens and converted to word embedding vectors shown in gray.
![[Pasted image 20250319091527.png|300]]

In a standard neural network, typically the text embeddings or the context vector are fed into the network one at a time, as shown here.
![[Pasted image 20250319091712.png|200]]->![[Pasted image 20250319091735.png|200]]

Then you would apply the argmax function to the resulting values to determine the token index of the subsequent word.
While it's possible to increase the context vector size to enhance the model's capacity, this approach is not feasible if the input size is variable.
Moreover, increasing the context vector size indiscriminately can lead to overfitting.
![[Pasted image 20250319091844.png|200]]->![[Pasted image 20250319091858.png|200]]->![[Pasted image 20250319091927.png|200]]
![[Pasted image 20250319091956.png|200]]

In decoders, word embeddings and positional encodings are applied to these word embeddings, which are illustrated in pink.
Next, the decoder produces a sequence of transformed embeddings, or tokens, termed contextual embeddings, depicted in blue.
Contextual embeddings aptly describe how the transformer processes the input word embeddings by accounting for the context in which each word occurs within the sequence.
Unlike static word embeddings, which remain constant for a word irrespective of its context, these contextual embeddings change based on the surrounding words and the flow of information within the transformer.
These contextual embeddings can be likened to logits from a hidden layer, and are subsequently passed through a final linear layer to predict a sequence of logits shown in red.
While the process is laid out sequentially here for ease of understanding, in reality it occurs simultaneously and the outputs are decoded much like a neural network operates.
Finally, an Argmax operation is applied to the last token to select the index with the highest score from the logits.
In this instance, `Good` is chosen as the next token.
![[Pasted image 20250319093326.png|500]]

The selected token `Good` is appended to the previous input sequence `how are you?` The updated input sequence becomes` how are you? Good`.
The word embeddings for good are calculated.
![[Pasted image 20250319094736.png|400]]

The updated input sequence is fed into the model again, the new vectors go through the layers similarly, and the model generates the next token based on the updated context.
In this case, the model generates `thanks` as the next token.
The model keeps generating tokens based on the updated input sequence until a specific condition is met.
For example, the generation process may continue until an end of sequence token such as `<EOS>` is reached or the maximum number of output tokens are generated.
![[Pasted image 20250319094807.png|500]]

# Recap
- Decoder
	- Plays a pivotal role in text generation
	- Serving as the foundation for sophisticated models such as GPT, LLaMA, and Granite.
	- A decoder functions autoregressively when it forecasts future words based on previously encountered words.
- The principal distinction between encoders and decoders in transformer architectures lies in the use of masked self-attention for decoders.
- Decoders leverage an attention mechanism, which at its core involves matrix multiplication.
- Masking ensures the model only attends to previous tokens in the sequence when making predictions, facilitating autoregressive generation.
- For generating translation using the decoder model
	- word embeddings and positional encodings are applied to the word embeddings.
	- the decoder produces a sequence of transformed embeddings, or tokens termed contextual embeddings.
	- These contextual embeddings can be likened to logits from a hidden layer, and are subsequently passed through a final linear layer to predict a sequence of logits.
	- Finally, an argmax operation is applied to the last token to select the index with the highest score from the logits.