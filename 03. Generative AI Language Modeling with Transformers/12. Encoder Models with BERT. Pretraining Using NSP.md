# Encoder Models with BERT: Pretraining Using NSP

## BERT pretraining
BERT is pre-trained via self-supervised learning on vast text corpora, using mass language modeling (MLM) and next sentence prediction (NSP).

## Next sentence prediction (NSP)
To perform the NSP task, the BERT model is trained to determine whether a given sequence logically follows a preceding one.
Take for example, the sentence, `My dog is cute`.
You would anticipate the following sentence to logically be, `He likes playing`.
During BERT's training, a random sentence such as `He likes studying medicine` may be presented as a potential successor.
The model's job is to predict which sentence is the appropriate continuation.
You must alter the embeddings for this task.

## BERT embeddings
In BERT, you can perform tokenization using a standard tokenizer for simplicity and ease of coding from scratch.
Please note that actual BERT uses word piece tokenization.
Begin by adding the `CLS` token at the start of your sequence.
This special token has a specific function.
Following this, you will insert the `SEP` token to denote the end of a sentence.
Each token will be associated with its respective embedding.
Additionally, you can introduce segment embeddings, which serve to distinguish whether a token belongs to the first or the second sentence in paired sentence tasks.
Finally, incorporate positional encodings to give the model an idea of the order of tokens in the sequence.
![[Pasted image 20250320191414.png|400]]

Let's look at the segment embeddings.
You have tokenized the sentences `My dog is cute` and `He likes playing.`
For the segment embeddings, you need to assign a value of one to all tokens up to and including the first SEP token, indicating they belong to the first sentence.
After the SEP token, assign a value of two, signifying that the subsequent tokens are part of the second sentence.
Additionally, use a binary variable y to indicate sentence succession.
A 0 denotes that the sentence is not the logical follow up or `NotNext`.
While 1 signifies that it is indeed the subsequent or next sentence.
Let's set y=1.
![[Pasted image 20250320191734.png|400]]

Here, insert a random sentence.
`My dog is cute, and He likes studying medicine`
For the segment embeddings, follow the same procedure.
But the binary variable, y=0, denotes that the sentence is not the logical follow up or `NotNext`.
![[Pasted image 20250320192047.png|400]]

Let's consider an example of a data set.
`My name is Dave`, followed by, `I live in Toronto`, is labeled as a consecutive sentence with the value set to one.
This indicates that the sentences are sequentially related.
Conversely, `My name is Joyce. Toronto is the Capital of Ontario` is labeled with a value set to false, indicating that the sentences are not related.
![[Pasted image 20250320223308.png|400]]

In addition to the `CLS` and `SEP` tokens, incorporate zero padding to ensure all sentences are of uniform length for processing purposes.
![[Pasted image 20250320223343.png|400]]

The labels have been padded for the masked language modeling as well, shown here as BERT labels.
![[Pasted image 20250320223434.png|400]]

## NSP: Next sentence prediction
In the NSP task, the input consists of word embeddings that are processed by the encoder to generate contextual embeddings.
These embeddings are used to determine whether for a given pair of sentences, the second sentence logically follows the first, just like a standard classification task.
![[Pasted image 20250320223621.png|500]]

The `CLS` token highlighted in yellow corresponds to the NSP classification token, which encapsulates information from the entire sequence.
This token is then fed into a neural network designed specifically for the NSP task, which assesses the relationship between pairs of sentences.
Let's approach this as a two-class classification problem.
Use the output value from the logits to determine the class.
![[Pasted image 20250320223750.png|400]]

To train the encoder component of the BERT model, you can utilize the outputs from a neural network that has been trained for NSP and MLM tasks.
The training process involves minimizing the combined loss, which is the sum of the losses from the two tasks.
Accurate word prediction for masked words and the ability to discern whether the predicted word fits within the context.
![[Pasted image 20250320223901.png|500]]

In PyTorch, the logits are used as inputs to the loss function to drive this optimization.
![[Pasted image 20250320224055.png|500]]
![[Pasted image 20250320224133.png|200]]

## Sentiment extraction with BERT
After pre-training, you can fine- tune BERT for specific downstream tasks.
Fine-tuning involves training BERT on a task-specific dataset, such as sentiment analysis, where the model predicts whether the sentiment of the input sequence is positive, neutral, or negative as shown.
![[Pasted image 20250320224234.png|400]]

During fine-tuning, the BERT learns task-specific patterns and adapts its representations to perform well on the target task.
Token representation of the CLS token will be transformed via a neural network to generate the model prediction.
The contextual embeddings can also be used in many other tasks like vector databases.
![[Pasted image 20250320224308.png|400]]

# Recap
- To perform the next sentence prediction or NSP task, the model is trained to determine whether a given sequence logically follows a preceding one.
- The BERT model's job is to predict which sentence is the appropriate continuation.
- The `CLS` token is added at the start of the sequence, and the `SEP` token is added to denote the end of a sentence.
- Segment embedding serve to distinguish whether a token belongs to the first or the second sentence in paired sentence tasks.
- Positional encodings give the model an idea of the order of tokens in the sequence.
- In the NSP task
	- the input consists of word embeddings processed by the encoder to generate contextual embeddings.
	- These embeddings are used to determine whether for a given pair of sentences, the second sentence logically follows the first, just like a standard classification task.
- Training the encoder component of the BERT model involves minimizing the combined loss, which is the sum of the losses from the two tasks.
- Accurate word prediction for masked words and the ability to discern whether the predicted word fits within the context.
- Fine-tuning involves training BERT on a task-specific dataset, such as sentiment analysis
- The BERT model predicts whether the sentiment of the input sequence is positive, negative, or neutral.