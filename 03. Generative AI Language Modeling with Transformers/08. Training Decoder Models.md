# Training Decoder Models

## Decoder models for prediction and inference tasks
Let's look at some new notations and refresh some of the concepts for sequence models.
In language modeling, you'll use Omega hat_t to represent the token or word index at time t.
When appropriate, you'll also use it to denote the token value.
This representation is derived from the maximum output value of the final layer in the neural network that is associated with the decoder.
During prediction, you use this approach to generate a word embedding vector denoted as x hat_t, which signifies your prediction.
You use a hat notation to indicate that it is an estimate, although the word embedding itself is not a true estimate, but rather a hypothetical vector based on the decoder's output.
![[Pasted image 20250320142853.png|200]]

Let's delve into the prediction tasks associated with autoregressive transformer decoder models.
Beginning with the first word embedding x at the initial time t equals zero, the decoder generates a contextual embedding, typically represented in gray.
This contextual embedding is then fed through the neural network, which helps shape the embedding of the subsequent encoding.
![[Pasted image 20250320143127.png|300]]

The predicted word embedding x_1 hat is combined with the original word embedding and reintroduced to the model.
As a result, you obtain two contextual embeddings both depicted in gray.
![[Pasted image 20250320143227.png|300]]

The last of these contextual embeddings is leveraged to predict the next token.
This newly predicted token and the previous embeddings are fed back into the decoder.
The sequence of events produces three contextual embeddings, and once again the final embedding is used to predict the subsequent token.
![[Pasted image 20250320143723.png|400]]

This cyclical process repeats itself throughout the model's operation.
It should be noted that positional encoding is added at each step.
![[Pasted image 20250320143618.png|400]]

## Training decoder models.
This training set example presents input tokens in the first column and their targets, shifted one time step forward in the second.
Special tokens are appended depending on the model.
Start tokens may or may not be used, and tokens are typically included and zero padding is applied for uniform sequence length.
![[Pasted image 20250320143929.png|300]]

You use the variable Omega to denote tokens with superscripts indicating the sample number.
Each sequence is a distinct sample.
![[Pasted image 20250320144057.png|300]]

These input tokens are converted into word embeddings denoted as x and fed into the model.
![[Pasted image 20250320144123.png|300]]

This video focuses on analyzing a single sequence, analogous to examining one sample in a neural network.
![[Pasted image 20250320144226.png|300]]

### Decoder prediction for training
During the training phase, the decoder's operation diverges from that of the inference phase.
The decoder employs actual word embeddings rather than their approximations throughout the training.
It processes the whole input sequence from index 0-3 and employs a causal mask to ensure that output predictions depend exclusively on prior sequence elements.
The output denoted as h and z are estimated values for the sequence at indices 1-4 built upon the inputs from indices 0-3.
![[Pasted image 20250320144929.png|400]]

Unlike inference tasks where only the final token is used, in training, the model uses all the predicted tokens at every sequence position.
To achieve this, all output contextual embeddings illustrated in gray and the output logits marked in red are harnessed to compute the loss.
![[Pasted image 20250320145319.png|400]]

The predictions represented by Omega hat primed are most likely tokens.
![[Pasted image 20250320145357.png|400]]

The loss is then calculated using the true tokens Omega and the logit values.
Here, the loss is the comparison between the actual token and the predicted token as shorthand.
After calculating the loss, you can train the model on one sequence or batch of sequences at a time.
Henceforth, for simplicity, let's drop the prime to streamline the notation.
![[Pasted image 20250320145438.png|400]]

### Regular training
During training, the model can use its own predictions from the previous step as the input for the next step.
For example, the initial input x_0 produces the output Omega_1, which in turn generates the token x hat_1.
This predicted token is then used to generate the following token, Omega hat_2, and the process continues in this manner.
The model's output is used to determine the loss.
![[Pasted image 20250320145636.png|400]]

### Teacher forcing training
Teacher forcing is a technique employed in training sequence models such as transformers.
In this method, instead of feeding the model's own predictions back as inputs for subsequent time steps, the actual previous token from the sequence is used.
In this example, x_0 serves as the initial input to produce the models output Omega hat_1.
For the next time step, the actual token, x_1, is used to generate the subsequent output, irrespective of the model's previous predictions.
The process continues for the whole sequence.
This approach ensures that even if the model makes an incorrect prediction at any step, it will still receive the correct token as the next input, allowing it to remain aligned with the actual sequence.
Consequently, this helps the model learn accurate mappings from inputs to outputs to determine the loss.
![[Pasted image 20250320145720.png|400]]

## Causal attention masking
Causal attention masking is a technique that you can use in decoder training to determine which tokens and a sequence are attended to by each token during generation.
In the self-attention process, a causal attention mask with negative infinity values in its upper triangle is applied to the attention matrix.
![[Pasted image 20250320151844.png|300]]

This mask ensures that each token can only attend to preceding tokens or itself, prohibiting future tokens from influencing the attention score.
For example, subsequent tokens should not influence the token `me` when predicting `IBM`.
![[Pasted image 20250320153221.png|400]]

Post-masking, the scores undergo a softmax operation, which nullifies the impact of future tokens.
![[Pasted image 20250320153324.png|500]]

As a result, the computation of subsequent embeddings is concentrated exclusively on the previous and current tokens, disregarding the future ones.
![[Pasted image 20250320153355.png|500]]

The animation showcases the mask attention mechanism during decoder training.
It computes attention scores using the complete input sequence denoted as x_0-x_3.
The scores for x_0 are exclusively used for predicting Omega_1.
![[Pasted image 20250320153507.png|500]]

To predict the second token, attention is limited to x_0 and x_1, masking out the subsequent tokens.
This selective attention extends to all future predictions.
![[Pasted image 20250320154903.png|500]]

Though the process may seem sequential, it occurs in parallel within the attention mechanism of the decoder.
Notably, this masking technique is crucial for accurately predicting intermediate tokens during inference.
![[Pasted image 20250320155028.png|500]]

## Conditional probabilities and context
The process is crucial for calculating probabilities unlike a neural network language model, where the condition is static.
The attention module incorporates subsequent probabilities into its estimates.
This is illustrated by how the probability of `GPT` depends on `me` and its preceding tokens and how `me` relies on its own predecessors.
![[Pasted image 20250320155152.png|400]]

A key limitation is the context size, which is the number of tokens the model can consider as input.
![[Pasted image 20250320155321.png|400]]

After the likelihood is determined during training, the logarithm is computed and combined with other sequences to train the model.
![[Pasted image 20250320155342.png|400]]

## Recap
- During the training phase, the decoder's operation diverges from that of the inference phase.
- The decoder employs actual word embeddings rather than their approximations throughout the training.
- In training, the model predicts the next token at every sequence position.
- During training, the model can use its own predictions from the previous step as the input for the next step.
- In teacher forcing technique, instead of feeding the model's own predictions back as inputs for subsequent time steps, the actual previous token from the sequence is used.
- Causal attention mask ensures that each token can only attend to preceding tokens or itself, prohibiting future tokens from influencing the attention score.