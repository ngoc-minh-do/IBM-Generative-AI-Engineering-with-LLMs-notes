# From Attention to Transformers

## Scaled dot-product attention with multiple heads.
The scaled dot-product attention mechanism within transformer models fundamentally involves a series of matrix multiplications incorporating queries Q, keys K, and a scaling factor to prevent the dot-product from becoming too large.
Depending on the task type, a masking operation may be employed, and finally, the values V are multiplied to produce the output.
![[Pasted image 20250316234316.png]]

## Self-attention
In the self-attention architecture, the queries Q, keys K, and values V all derive from the same input column vector X.
They are each multiplied by their respective learnable parameter matrices.
![[Pasted image 20250316234416.png|350]]

## Cross-attention
For tasks such as language translation, cross-attention mechanisms are utilized wherein the values and keys come from a different input source, like the language that needs to be translated, represented as Z.
![[Pasted image 20250316234530.png|350]]

## Multi-head attention
Multi-head attention operates by executing several scale dot-product attention processes in parallel.
The output is called heads.
This strategy allows each head to attend to distinct segments of the input sequence.
![[Pasted image 20250316234811.png|150]]

For example, the input consists of embeddings displayed at the bottom as seven four-dimensional vectors in blue.
The output is an enhanced set of embeddings depicted as seven four-dimensional vectors at the top in gray.
You multiply the inputs by the learnable parameter and scale dot-product attention followed by a linear layer.
![[Pasted image 20250316234941.png|500]]

Then you multiply the result by a final learnable parameter and the output is termed a head.
In an example with multiple heads, each four-dimensional vector is split into two two-dimensional vectors, yielding 14 vectors.
These split vectors are processed by their individual attention mechanisms.
![[Pasted image 20250316235044.png|450]]

Subsequently, the 14 output vectors are concatenated, recombining the corresponding pairs to produce seven four-dimensional vectors, then the final linear layer is applied.
![[Pasted image 20250316235135.png|450]]

You can scale the process for an arbitrary number of heads by concatenating the outputs of each head and then multiplying them by a final linear layer.
The number of heads serves as a hyperparameter, with the sole constraint being that the input dimension must be divisible by the number of heads.
![[Pasted image 20250316235238.png|450]]

## Multi-head dependency
Consider the phrase: `heavy snow makes driving more difficult`.
You can see that `makes` has multiple dependencies, including `snow`, `more` and `difficult`.
In this instance, the color blocks represent different heads in the multi-head attention.
Each head focuses on a different dependency.
One of the attention heads focuses on the distant dependency of the verb `makes`, completing the phrase `makes more difficult`.
Other heads focus on `snow` and `driving`.
![[Pasted image 20250316235435.png|500]]

## Multi-head attention implementation in PyTorch
You can utilize the `nn.MultiheadAttention` module of PyTorch to initialize the `multihead_attn` object with an embedding dimension of 4 and 2 attention heads.
You must ensure that the number of embedded dimensions is divisible by the number of heads by performing a modulus check.
The multihead attention object is then instantiated with the specified parameters You can explicitly set batch first to false, adhering to the standard sequence processing convention.
```python
# Embedding dimension
embed_dim =4
# Number of attention heads
num_heads = 2
print("should be zero:",embed_dim %num_heads)
# Initialize MultiheadAttention
multihead_attn = nn.MultiheadAttention(embed_dim=embed_dim, num_heads=num_heads,batch_first=False)
```

Randomly generated query, key, and value tensors with a sequence length of 10 and a batch size of 5 are created.
You can then invoke the forward method of the multihead attention mechanism.
The output size matches one of the input sizes, signifying consistent dimensionality.
It's important to note that multi-head attention is merely one component of the broader transformer architecture.
```python
seq_length = 10 # Sequence length
batch_size = 5 # Batch size
query = torch.rand((seq_length, batch_size, embed_dim))
key = torch.rand((seq_length, batch_size, embed_dim))
value = torch.rand((seq_length, batch_size, embed_dim))
# Perform multi-head attention
attn_output, _= multihead_attn(query, key, value)
print("Attention Output Shape:", attn_output.shape)
# Attention Output Shape: torch.Size([10, 5, 4])
```

## Scaled dot-product attention to transformer architecture: Encoders and decoders
The efficiency of attention mechanisms can be enhanced by incorporating additional layers, as in the case of transformers.
The two primary types of transformers are encoders and decoders.

### Encoder
In the encoder phase of a transformer, the multihead attention mechanism initiates the process by handling word embeddings that have been combined with positional encodings.
Only the input, represented as X, is passed to the multihead attention.
![[Pasted image 20250317000617.png|200]]

Unlike some models, encoder models do not apply masking directly to the attention mechanism.
![[Pasted image 20250317000514.png|400]]

The next step is the normalization process, referred to as add and Norm.
It is essential as it merges the attention output with its original input embeddings and normalizes the result.
This process enhances the depth of the model while mitigating potential issues with gradients.
Subsequently, a feed-forward network applies a fully connected layer to each position, followed by an additional add and norm phase.
Depending on the requirements of the specific use case, further layers can be integrated into the architecture.
![[Pasted image 20250317000722.png|200]]

### Decoder 
In the transformers decoder, the multi-head attention layer is distinctively masked to prohibit foresight into future data.
This design ensures that each position can only attend to earlier positions in the sequence.
![[Pasted image 20250317000849.png|300]]

Additionally, normalization layers are employed consistently throughout the decoder.
For complex tasks like language translation, a cross-attention layer is incorporated.
![[Pasted image 20250317000943.png|300]]

Following this, a position-wise feed-forward network applies a fully connected layer independently to each position succeeding by an add and norm step, further refining the output.
While decoders primarily drive language generation, they also include a final linear layer analogous to a neural network, which maps embeddings to word predictions.
In PyTorch implementations, a masked encoder can be adapted to perform similarly to a decoder, which may seem confusing.
![[Pasted image 20250317001018.png|300]]

Stacking multiple transformer layers and series allows the modeling of complex relationships and representations within the data across different levels.
In this instance, you stack two encoder layers.
This layer depth is an important hyperparameter that can be adjusted to enhance performance.
![[Pasted image 20250317001058.png|200]]

## Construct a series of encoder layer instances in PyTorch.
First, specify the number of attention heads and the dimensions of the embeddings.
Then determine the desired number of layers.
The next step is to create a transformer encoder layer object by specifying the number of heads in the dimensionality of the embeddings, then provide the number of layers to the transformer encoder constructor to assemble a sequence of encoder layers.
Positional encodings are added before this.
```python
# Embedding dimension
embed_dim = 4
# Number of attention h
num_heads = 2
# Checking if the embedding dimension is divisible by the number of heads, print("should be zero", embed_dim % num_h
# Number of encoder layers
num_layers = 6
# Initialize the encoder layer with specified embedding dimension and number of heads.
encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads)
# Build the transformer encoder by stacking the encoder layer 6 times.
transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
```

You can randomly initialize the input tensor X with dimensions corresponding to sequence length, batch size, and embedding dimension.
When X is passed through the transformer encoder, it is processed sequentially across all the encoder layers.
Each layer applies multihead self-attention, followed by feed-forward neural networks incorporating residual connections and layer normalization at each step.
You can see that the output embeddings retain the same size as the input.
```python
# Define sequence length as 10 and batch size as 5 for the input data.
seq_length = 10 # Sequence length
batch_size = 5 # Batch size
# Generate random input tensor to simulate input embeddings for the transformer encoder.
x = torch.rand((seq_length, batch_size, embed_dim))
# Apply the transformer encoder to the input
encoded = transformer_encoder(x)
# Output the shape of the encoded tensor to verify the transformation.
print("Encoded Tensor Shape:", encoded.shape)
# torch.Size([10, 5, 4])
```
## Recap
- The scale dot-product attention mechanism involves a series of matrix multiplications that incorporate queries, keys, and a scaling factor.
- A masking operation may be employed, and the values are multiplied to produce the output.
- Multihead attention operates by executing several scale dot-product attention processes in parallel.
- The efficiency of attention mechanisms can be enhanced by incorporating additional layers, as in the case of transformers.
- Stacking multiple transformer layers in series allows the modeling of complex relationships and representations within the data across different levels.