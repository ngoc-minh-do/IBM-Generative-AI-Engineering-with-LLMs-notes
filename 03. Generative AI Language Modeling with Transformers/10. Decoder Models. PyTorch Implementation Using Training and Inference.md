# Decoder Models: PyTorch Implementation Using Training and Inference

## Introduction to decoder model
- The decoder model in PyTorch refers to a neural network architecture that uses encoded information and generates output sequences.
- Decoder models help in sequence-to-sequence text generation tasks, such as machine translation and image captioning.
- It's important to note that the decoder model is similar to the generative pretrained transformer or GPT.

## Creating a small instance of the model
Let's begin by understanding how to create a small instance of the model.
Create a small instance of the model, first specify the model hyperparameters, including encoder layers of two and the number of attention heads of two.
Then define the instance for the custom GPT model.
```python
ntokens = len(vocab)  # size of vocabulary
emsize = 200  # embedding dimension
nlayers = 2  # number of ``nn.TransformerEncoderLayer`` in ``nn.TransformerEncoder``
nhead = 2  # number of heads in ``nn.MultiheadAttention``
dropout = 0.2  # dropout probability

model = CustomGPTModel(embed_size=emsize, num_heads=nhead, num_layers=nlayers, vocab_size=ntokens,dropout=dropout).to(DEVICE)
```

## Loss
During loss calculation, the encoder model generates a source and a target.
However, examining a batch from the target, you can observe that the dimensions are sequence length by batch size.
![[Pasted image 20250320174611.png|400]]

During prediction, the decoder model generates logits, Class 1 and Class 2.
It's important to note that the dimensions of the logits are sequence length, batch size, and number of classes.
Additionally, each table represents one sample of a batch.
![[Pasted image 20250320174711.png|400]]

In preparation for loss calculation, you can see the reshaping of logits where each row corresponds to the prediction for a token, spanning across both the sequence and the batch dimensions.
Next to calculate the loss, you can reshape the target tensor so that its elements correspond correctly to the logits.
This process ensures that every row from the logits aligns with the appropriate target outcomes for accurate loss estimation.
The illustration represents the Omega hat, which is useful for its simplicity, and all the related logits are in a real world scenario.
![[Pasted image 20250320174915.png|400]]

## Training
However, the training process is similar to other models such as convolutional neural networks or CNNs, recurrent neural networks or RNNs, transformers and generative models.
It uses the modified loss shape and other functions such as validation and checkpoint saving that help in the optimization.
```python
optimizer = Adam(model.parameters(), lr=1e-2, weight_decay=0.01, betas=(0.9, 0.999))
scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 10000, gamma=0.9)

def train(model: nn.Module,train_data) -> None:
    model.train()  # turn on train mode
    total_loss = 0.
    log_interval = 10000
    start_time = time.time()

    num_batches = len(list(train_data)) // block_size
    for batch,srctgt in enumerate(train_data):
        src= srctgt[0]
        tgt= srctgt[1]
        logits = model(src,src_mask=None)
        logits_flat = logits.reshape(-1, logits.shape[-1])
        loss = loss_fn(logits_flat, tgt.reshape(-1))

        optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)
        optimizer.step()
        total_loss += loss.item()

        if (batch % log_interval == 0 and batch > 0) or batch==42060:
            lr = scheduler.get_last_lr()[0]
            ms_per_batch = (time.time() - start_time) * 1000 / log_interval
            #cur_loss = total_loss / log_interval
            cur_loss = total_loss / batch
            ppl = math.exp(cur_loss)
            print(f'| epoch {epoch:3d} | {batch//block_size:5d}/{num_batches:5d} batches | '
                  f'lr {lr:02.4f} | ms/batch {ms_per_batch:5.2f} | '
                  f'loss {cur_loss:5.2f} | ppl {ppl:8.2f}')
            start_time = time.time()

    return total_loss
```

## Training: Evaluate function
The evaluate function measures the performance of the model by computing its average loss in the validation dataset.
However, the trained model is useful to generate inferences.
```python
def evaluate(model: nn.Module, eval_data) -> float:
    model.eval()  # turn on evaluation mode
    total_loss = 0.
    with torch.no_grad():
        for src,tgt in eval_data:
            tgt = tgt.to(DEVICE)
            #seq_len = src.size(0)
            logits = model(src,src_mask=None,key_padding_mask=None)
            total_loss +=  loss_fn(logits.reshape(-1, logits.shape[-1]), tgt.reshape(-1)).item()
    return total_loss / (len(list(eval_data)) - 1)
```

## Autoregressive text generation
- generate auto regressive text, also known as inference.
- Autoregressive text generation is a method of generating text where the model predicts the next token in a sequence based on the previous tokens.
- This process is iterative, where the decoder model generates one token at a time and uses the generated token as input for the next prediction.

### Prompt encoding
Preparing an encoding prompt helps to create a process for text generation.
This process serves as a starting point for the model to generate subsequent tokens.
Once this prompt is tokenized, the decoder model can process and generate the next tokens based on the input.
Here, the pad tokens are used to process multiple batches.
However, they're not necessary for a single prediction.
```python
def encode_prompt(prompt, block_size=BLOCK_SIZE):
    # Handle None prompt
    while prompt is None:
        prompt = input("Sorry, prompt cannot be empty. Please enter a valid prompt: ")

    tokens = tokenizer(prompt)
    number_of_tokens = len(tokens)

    # Handle long prompts
    if number_of_tokens > block_size:
        tokens = tokens[-block_size:]  # Keep last block_size characters

    prompt_indices = vocab(tokens)
    prompt_encoded = torch.tensor(prompt_indices, dtype=torch.int64).reshape(-1, 1)
    return prompt_encoded
```

For example, a tokenized decoded prompt is mapped with vocab indices and padding.
![[Pasted image 20250320180351.png|400]]

### Text generation
The generate function creates auto regressive text in the decoder model.
To do so, first, create the encoded prompt tensor from the input sentence, complete the sentence width
=> `complete this sentence with`
Then feed the prompt into the model.

You can see that the mask is generated in the model, and the model will output the logits during the process.
Next, select the final logit vector.
Now, identify and generate the token with the highest logit score, in this case, `the`.
Further, append the, to the prompt for subsequent auto regressive text generation
=> `complete this sentence with the`

You can see that the decoder model receives the updated prompt, and generates a token with the highest logit value.
In this case, the token is `next`.
Now append a token `next` to the prompt to generate the next round of auto regressive text
=> `complete this sentence with the next`

You can see that the decoder model has again received the updated prompt, and generated a token with the highest logit value.
In this case, it is a `word`.
Now append the token `word` to the prompt to generate the next round of autoregressive text
=> `complete this sentence with the next word`

Finally, this loop continues until the token reaches the other end of the sentence, or passes max new tokens to the decoder model.
```python

#auto-regressive Language Model text generation
def generate(model, prompt=None, max_new_tokens=500, block_size=BLOCK_SIZE, vocab=vocab, tokenizer=tokenizer):
    # Move model to the specified device (e.g., GPU or CPU)
    model.to(DEVICE)

    # Encode the input prompt using the provided encode_prompt function
    prompt_encoded = encode_prompt(prompt).to(DEVICE)
    tokens = []

    # Generate new tokens up to max_new_tokens
    for _ in range(max_new_tokens):
        # Decode the encoded prompt using the model's decoder
        logits = model(prompt_encoded,src_mask=None,key_padding_mask=None)

        # Transpose the logits to bring the sequence length to the first dimension
        logits = logits.transpose(0, 1)

        # Select the logits of the last token in the sequence
        logit_prediction = logits[:, -1]

        # Choose the most probable next token from the logits(greedy decoding)
        next_token_encoded = torch.argmax(logit_prediction, dim=-1).reshape(-1, 1)

        # If the next token is the end-of-sequence (EOS) token, stop generation
        if next_token_encoded.item() == EOS_IDX:
            break

        # Append the next token to the prompt_encoded and keep only the last 'block_size' tokens
        prompt_encoded = torch.cat((prompt_encoded, next_token_encoded), dim=0)[-block_size:]

        # Convert the next token index to a token string using the vocabulary
        # Move the tensor back to CPU for vocab lookup if needed
        token_id = next_token_encoded.to('cpu').item()
        tokens.append(vocab.get_itos()[token_id])

    # Join the generated tokens into a single string and return
    return ' '.join(tokens)
```
![[Pasted image 20250320180812.png]]

## Recap
- The encoder model generates a source and a target while calculating loss.
- The training process is similar to other models and uses the modified loss shape and other functions that help in optimization.
- Autoregressive text generation is a method of generating text where the model predicts the next token in a sequence, based on the previous tokens.
- function `generate` is used to create an autoregressive text in the decoder model.
- Generate an auto regressive text by:
	- create an encoded prompt, and feed it into the model.
	- Then generate a mask
	- Select logit output
	- identify and generate the token with the highest logit score
- append it for generating subsequent autoregressive text.