# Self-attention mechanism

- self-attention mechanism is the heart of the language transformer.
- Each word in sequence attends every other word in parallel to generate embeddings.
## Simple language modeling in the self-attention mechanism.

The simple language modeling in the self-attention mechanism predicts the next word in the sentence, an important aspect of understanding natural language.
For example, in the table the sequence of words is presented on the left side and the prediction of words for the simple  language modeling is on the right.
Now, when you insert the word `not like`, the simple language modeling will predict the word `hate`.
![[Pasted image 20250316185729.png|300]]

Similarly, the input word `do like` would predict the word `like`.
It means that when the context of the word changes, the meaning of the preceding words also changes.
![[Pasted image 20250316185753.png|300]]

## Word embedding to sequence matrix

The words transform into matrices of the embedding sequence where each embedded word represents a column vector within the sequence matrix.
For example, X represents the matrix and the subscript `not like` represents the word sequence with each column corresponding to a word embedding.
It means the matrix X `not like` contains the embedding for `not` and `like`.
Therefore, each sequence is converted into a matrix representation and represented as a sample sequence in the dataset.
![[Pasted image 20250316185848.png|300]]

Consider n sample sequences denoted as Xn.
It means that each sequence can have a different length.
It should be noted that libraries like PyTorch may arrange these vectors differently.
![[Pasted image 20250316185939.png|300]]

## Query projections with bias
Self-attention mechanisms and simple language modeling involve generating three key components, query Q, key K and value V.
This process begins with the input word embeddings combined with learnable parameters.
You can derive a query matrix by multiplying the input sequence matrix by learnable parameters known as query projection weights and biases.
Here, a row vector of one s with the same length as the number of tokens is also used.
![[Pasted image 20250316190017.png|300]]

## Key projections with bias
Similarly, you can derive the key matrix by multiplying the input sequence matrix with a different set of learnable parameters, key projection weights and biases.
![[Pasted image 20250316190051.png|300]]

## Value projections with bias
You could apply the same process to obtain the value matrix where the input is again multiplied by its corresponding set of learnable parameters, known as value projection weights and biases.
However, a bias term is not always included, and sequential embeddings are usually added before these operations.
They are omitted in this simplified explanation for clarity.
![[Pasted image 20250316190115.png|300]]

## Attention mechanism for contextual embeddings
The self-attention mechanism employs the query key and value constructs to refine the input word embeddings.
In this context, the square root of D represents the dimension of the embeddings.
![[Pasted image 20250316190213.png|300]]
![[Pasted image 20250316190301.png|300]]

The resulting matrix, denoted as H', features columns that embody enhanced embeddings and are termed contextual embeddings.
![[Pasted image 20250316190317.png|300]]

Further, the number of these embeddings aligns with the length of the input sequence, ensuring a one-to-one correspondence between the original and enhanced representations.
You can apply an additional layer to the context embeddings where you will get output as H, a refined version of H' using additional learnable parameters.
Therefore, H is a more nuanced representation of the initial embeddings.
![[Pasted image 20250316190438.png|300]]

## Input neural network
Self-attention mechanisms are instrumental in generating sequence outputs.
For example, reducing the context embedding mechanism to an averaging operation helps to simplify the process.
In this simplified framework, the contextual embedding serves as inputs to a neural network where the initial layers output is denoted as Z superscript 1.
![[Pasted image 20250316190721.png|200]]

## Final layer: Predicting the next word with Neural Network
Lets consider a previous example of a self-attention mechanism used for translation, where the output of the self-attention mechanism is the embedded word you wish to translate.
![[Pasted image 20250316190751.png|300]]

In this process, a dot product is computed between the embedding and a list of embeddings through matrix multiplication.
The argmax function assists in obtaining the index of the translated words token.
When using self-attention, the dot product is replaced by a neural network where Z1 represents the average embedding, serving as an input parameter for the neural network.
This network generates a series of logits denoted as Z2.
Moreover, the input dimension of the neural network aligns with the embedding space and its output dimension matches the vocabulary size.
![[Pasted image 20250316191102.png|300]]
However, during training, the neural network employs the softmax function to compute the probability distribution across all potential words.
This process helps to train the network and the learnable parameters of the attention mechanism.
![[Pasted image 20250316191127.png|250]]

In Pytorch, logits are used directly during training and the argmax function is used to determine the sequences subsequent token or word index.
![[Pasted image 20250316191219.png|300]]
![[Pasted image 20250316191319.png|300]]
![[Pasted image 20250316191341.png|200]]

## Determine the query, key, and value matrices

Now lets predict the token for `not like` using the self-attention mechanism where the expected outcome is `hate`.
The self-attention mechanism generates the query key and value matrices through multiplications with the learnable parameters and the bias.
In this example, the input x is multiplied by the projections to obtain Q, K and V.
This process consists of a series of matrix multiplications.
These operations can be parallelized and processed via a graphic processing unit or GPU, allowing more data to be processed efficiently.
This is one reason why the self-attention mechanism outperforms other sequential models like recurrent neural networks or RNNs.
### Values

![[Pasted image 20250316222114.png|300]]
![[Pasted image 20250316222150.png|400]]
![[Pasted image 20250316222205.png|400]]
![[Pasted image 20250316222245.png|300]]
### Query
![[Pasted image 20250316222405.png|300]]
![[Pasted image 20250316222429.png|400]]
![[Pasted image 20250316222446.png|300]]
![[Pasted image 20250316222504.png|200]]
### Key
![[Pasted image 20250316222536.png|300]]
![[Pasted image 20250316222549.png|400]]
![[Pasted image 20250316222605.png|300]]
![[Pasted image 20250316222618.png|200]]

## Compute attention scores
![[Pasted image 20250316222716.png|100]]
![[Pasted image 20250316222738.png|300]]
![[Pasted image 20250316222813.png|200]]

###  Normalize attention scores
Additionally, you can obtain the attention scores by multiplying the query and key matrices, then apply the softmax function of these scores to normalize them.
![[Pasted image 20250316222859.png|300]]

### Derive attention values
Next, multiply the normalized scores with the value matrix V to derive the H', which enhances the contextual embeddings by being multiplied with output projection parameters.
![[Pasted image 20250316223839.png|200]]
![[Pasted image 20250316223853.png|300]]
![[Pasted image 20250316223909.png|200]]

## Apply output projection
![[Pasted image 20250316224256.png|200]]
![[Pasted image 20250316224322.png|400]]
![[Pasted image 20250316224346.png|300]]
![[Pasted image 20250316224401.png|200]]

## Average contextual word embeddings
![[Pasted image 20250316224504.png|200]]
![[Pasted image 20250316224526.png|300]]
![[Pasted image 20250316224604.png|300]]

## Map to vocabulary space
Further, take an average of these embeddings and pass through the final layer that maps to the vocabulary, acquiring the activations corresponding to each word.
![[Pasted image 20250316224657.png|200]]
![[Pasted image 20250316224903.png|300]]
![[Pasted image 20250316224926.png|200]]

## Select maximum output logit
Finally, apply the argmax function to these activations and the index obtained reveals the logistic value associated with `hate`.
![[Pasted image 20250316225011.png|200]]
![[Pasted image 20250316225148.png|200]]
![[Pasted image 20250316225250.png|300]]

## Understanding attention mechanism
Further, diving into the attention mechanism, you can observe that each sequence is arranged with the input tokens positioned above, followed by the normalized attention, and the corresponding values beneath.
These values act as modified word embeddings.
For example, looking at the first two samples on the left, you can find the first column consistently represents the value for the token `not`, a pattern that holds true for each token.
Meanwhile, the normalized attention underscores the unique relational dynamics between the pairs, highlighting the intricate interplay within the sequence.
![[Pasted image 20250316225410.png]]

In this example, the attention scores derived from the dot product developments in keys and queries plays a critical role in the self-attention mechanism, shedding light on the models focal points.
The attention scores are illustrated for three phrases, not like, not hate, and an arbitrary sequence elucidating the relationships between tokens within the sequence.
![[Pasted image 20250316225521.png|400]]
## Recap
- simple language modeling in the self-attention mechanism predicts the next word in the sentence, an important aspect of understanding natural language.
- The words transform into matrices in the embedding sequence, where each embedded word represents a column vector within the sequence matrix.
- Self-attention mechanisms in language modeling generate query key and value using the input word embeddings and learnable parameters.
- The self-attention mechanisms use the query, key and value in context embedding to modify the input row vectors.
- The self-attention mechanisms help generate output using various methods.