Generative AI models use attention mechanisms to focus on the most relevant parts of the input data and their relationships.

Let's start exploring attention mechanisms by drawing parallels with how Python dictionaries help in translation.
Python dictionaries help with tasks such as translation by pairing keys with their corresponding values.
When you query the dictionary, you input the key.
In return, the dictionary provides the value that corresponds to that key.
In this instance, the keys represent French words, and the values are the English translations.
To translate a French word to English, query the dictionary with the French term.
The dictionary returns the English translation.
![[Pasted image 20250316172111.png|400]]

You can think of the dictionary as having two columns.
The left column contains the keys, which are words in French, whereas the right column contains the corresponding English translations.
Each row represents a distinct word pairing.
![[Pasted image 20250316172259.png|300]]

## Attention mechanism
### Key
The attention mechanism uses a comparable structure, but instead of strings, you'll employ one-hot encoded vectors.
Each French word in the dictionary is represented as a one-hot encoded vector called keys.
![[Pasted image 20250316172459.png|200]]

These individual keys are assembled into a matrix K, each row corresponding to the one-hot encoded word.
![[Pasted image 20250316172549.png|400]]

### Query
Like a Python dictionary, the query vectors are identical to the key vectors, represented with Q instead of K.
![[Pasted image 20250316172738.png|200]]

The English words are also represented by one-hot encoded vectors called values.
![[Pasted image 20250316172829.png|300]]

### Value
These individual values are then assembled into a matrix called V, each row containing the one-hot encoded word.
![[Pasted image 20250316172932.png|200]]![[Pasted image 20250316172950.png|350]]

### Keys and Values
The key matrix K and the value matrix V must be arranged in such a way that the query vector for the word intended for translation aligns with the same row across both matrices.
You can see that the row for `chat` aligns with `cat`, `est` with `is`, and `sous` with `under`.
The English article `the` is represented twice within the value matrix V.
![[Pasted image 20250316173123.png]]

### Attention for translation
Next, you'll transpose the key matrix K converting the rows to columns.
This is a shortcut for performing a series of dot products.
![[Pasted image 20250316173359.png|400]]

Let's examine the attention mechanism formula for translation.
It's the product of the query vector for the word, the transpose of K and the V matrix.
![[Pasted image 20250316173529.png|400]]

The result will be a vector.
Let's refer to it as `h`, representing the value vector corresponding to the translated word.
First, you insert the one-hot encoded French query word you intend to translate to.
Then you multiply the query vector by the transposed key matrix.
The key matrix is transposed, so each column corresponds to the queries for the labeled words.
Similarly, the value rows are labeled with the corresponding word that they represent.
![[Pasted image 20250316173716.png|500]]

The attention formula involves computing the dot product across the key vectors that form the columns of matrix K.
Due to their orthogonality, all values in the resultant row vector are zero, except for the position where the query vector perfectly matches a key vector.
![[Pasted image 20250316173754.png|500]]

Multiplying this non-zero element by the value matrix isolates the corresponding translated word vector.
You can now have the word vector for the translated word in this case, `under`.
![[Pasted image 20250316173829.png|400]]

To retrieve the translated word from the translated vector, you can apply bellow equation, where h is the output of the attention mechanism, and V is the value matrix.
When multiplying h with V transpose, you perform a dot product with every value, as each element is orthogonal except for the word with the same value.
You get a vector of zero except for the one that is the same.
This column has the same index as the word index of the translated word in the value matrix, `under` in this case.
![[Pasted image 20250316175449.png|400]]

## Attention mechanism with word embeddings
With a few adjustments, you can apply the attention mechanism to word embeddings.
This process helps capture contextual relationships between words and thus enables the translation of words the model has not encountered before.
You substitute the keys and values with word embeddings, aligning the rows with their respective translation.
![[Pasted image 20250316175920.png|500]]

You will then apply the attention formula to the word embedding for `ci-dessous`, a word absent from the Key matrix.
![[Pasted image 20250316175956.png|400]]
![[Pasted image 20250316180246.png|300]]

This word translates to `below`.
Let's see the product of the Query matrix with the Key matrix.
You perform a dot product with each term in the Key matrix.
![[Pasted image 20250316180137.png|500]]

When you examine the output, you'll see that the maximum value corresponds to the column for sous, which means `below`.
This correlation makes sense as the words share similar embedding values.
Remember, you require a one-hot encoded vector to select the precise word from the value matrix.
![[Pasted image 20250316180158.png|400]]

You can refine the attention formula by incorporating the softmax function on the resultant output of the dot product between the query vector and the key.
![[Pasted image 20250316180449.png|400]]
![[Pasted image 20250316180545.png|400]]

Let's see what happens in the softmax operation.
You can refer to this resultant vector as Alpha.
In simple terms, you perform the matrix product on the query and the key and then apply the softmax function on the resultant vector Z.
![[Pasted image 20250316180639.png|400]]

The softmax function accentuates the largest value by scaling it close to one while diminishing the smaller values closer to zero.
The function helps amplify the largest value relative to the rest.
When you apply the softmax function to the resultant vector Z, it transforms the column with the largest Alpha into a form that resembles a binary vector.
![[Pasted image 20250316180813.png|400]]
![[Pasted image 20250316181110.png|400]]

You can now use the attention mechanism to translate the word `ci-dessous`.
Consider the output labeled h.
![[Pasted image 20250316181247.png|400]]

Given that this embedding closely resembles that of sous, you identify the column with the highest value.
![[Pasted image 20250316181344.png|400]]
![[Pasted image 20250316181517.png|400]]
Implementing the softmax function simplifies this into a one-hot vector.
![[Pasted image 20250316181536.png|400]]

Multiply this one-hot vector with the value matrix to retrieve the word embedding for the translated word `under`.
![[Pasted image 20250316181559.png|400]]

This describes a search method that identifies the most similar vector using an approach akin to one-hot encoding.
Because the resulting vector h corresponds to the intended word, performing the dot product with h will yield the highest value at the index of the target word.

## Attention for sequences.

You can consolidate all the query vectors into a single matrix, denoted as Q to process every vector concurrently within a single matrix product operation.
![[Pasted image 20250316182103.png|200]]
![[Pasted image 20250316182203.png|300]]

The input values are treated as a sequence of embeddings depicted in pink.
The process yields a set of refined embeddings tailored for specific tasks, illustrated in blue.
![[Pasted image 20250316182223.png|300]]

It's important to note that the actual translation method in transformers slightly differs.
In addition, positional encodings can be added.
However, for simple data problems, these positional encodings may not be necessary.
## Recap
- Attention mechanisms employ the `query`, `key` and `value` matrices.
- The `query` vector should align with the same row across `key` and `value` matrices.
- You can apply attention mechanism to word embeddings. This process helps capture contextual relationships between words.
- You can refine the attention formula by incorporating the softmax function on the output of the dot product between the `query` vector and the `key`.
- For employing attention for sequences, you can consolidate all the query `vectors` into a single matrix.