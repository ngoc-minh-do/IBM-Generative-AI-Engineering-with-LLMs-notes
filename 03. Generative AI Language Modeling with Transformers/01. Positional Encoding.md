# Positional Encoding

Have you ever tried unscrambling letters to form words?
The order of position of the letters in a word is important to convey the intended meaning.
Similarly, in transformer-based models, it's crucial to capture the position of each token to maintain the semantic meaning, as tokens are processed independently and simultaneously.

Let's take an example.
Consider the phrases `King and Queen are awesome` and `Queen and King are awesome`.
These sentences are slightly different.
Let's look at the embeddings for these.
You'll see that the vector representation of embeddings is identical.
You can introduce positional encoding to address this issue.
It incorporates information about the position of each embedding within the sequence.
Typically, positional encoding is added to the input embeddings, enabling the model to differentiate between the positions of various elements in the input sequence.
You can see that after adding positional encoding, the vector representations of king and queen in the second sentence shown in green is different from their vector representations in the first sentence shown in blue.
![[Pasted image 20250316152740.png|400]]

Positional encoding consists of a series of sine and cosine waves and involves two parameters.
![[Pasted image 20250316155019.png|400]]

The `pos` parameter represents the position of the sine wave over time, similar to the time variable t or the x coordinate in a standard plot.
![[Pasted image 20250316152959.png|400]]

The parameter `i`, which is the dimension index, effectively generates a unique sine or cosine wave for each embedding, controlling the number of oscillations for each wave.
Each one of these waves is added to a different dimension in the word embedding.
![[Pasted image 20250316153152.png|300]]->![[Pasted image 20250316153224.png|300]]

Let's explore this further with an example.
Let's begin with the sequence of embeddings for `transformers are awesome`, where each row represents an embedding for a specific word and each column corresponds to an element within that embedding.
`POS` is the specific position of each word embedding within the sequence.
In this example, the sequence length is 3.
Each word embedding has a dimensionality of 4 and will be differentiated based on whether its indices are odd or even.
![[Pasted image 20250316153718.png|400]]

Let's add positional encoding to the embedding for transformers.
For each dimension in positional embeddings, you introduce the corresponding sine and cosine waves.
Thus, for i=0, a sine wave is added, and this pattern is repeated for i=1, ensuring that each dimension is uniquely represented by its own wave.
![[Pasted image 20250316155109.png]]

Similarly, the positional encoding values are calculated for the word `are`.
![[Pasted image 20250316155354.png]]

Finally, the positional encoding values are calculated for the word `awesome`.
![[Pasted image 20250316155339.png]]

You can view a table depicting the values for positional encoding corresponding to the input sentence.
![[Pasted image 20250316155427.png]]

The four graphs illustrate positional encoding, one for each dimension of the word embeddings.
For a sequence length of 3, you see only three values for each sine function.
![[Pasted image 20250316155520.png|400]]

Lets generate positional encoding with an embedding dimension of 8 to depict a realistic setting.
![[Pasted image 20250316155659.png|400]]

Occasionally, for practicality, you can align the maximum sequence size to match the vocabulary size by rotating the positional encodings.
Rows become indicative of varying encoding functions, while columns represent time or position in a sequence.
![[Pasted image 20250316155736.png|400]]

Positional encodings provide unique and periodic values for sequence positions.
The cosine waves will never intersect the line at the same points, ensuring models distinguish and process variable length sequences effectively.
Their range between minus 1 and 1 prevents overshadowing the embeddings.
These encodings also support differentiability and relative positioning, making it easier to train the model.
![[Pasted image 20250316155800.png|400]]

You can also represent each word embedding as a column vector x.
Positional encoding can be conceptualized as a series of vectors, where each vector p captures a specific location within a sequence.
![[Pasted image 20250316155916.png|400]]

When this positional vector is added to its corresponding embedding vector, the combination preserves the positional information, ensuring the elements sequence order is maintained within the resulting vector.
![[Pasted image 20250316155953.png|400]]

In models such as GPT, positional encodings are not static but rather learnable parameters w.
These learnable parameters, represented by tensors, are added to the embedding vector and optimized during training.
![[Pasted image 20250316160032.png|400]]

Segment embeddings used in certain models, such as BERT, are related to positional encodings, providing additional positional information.
You can integrate segment embeddings into the existing embeddings alongside the positional encodings.
![[Pasted image 20250316160053.png|300]]->![[Pasted image 20250316160200.png|350]]

## Positional encoding in PyTorch

You can construct a module to integrate positional encoding into the embeddings.
The output shape sequence length is set to exceed the longest sequence in the training dataset, with the embedding dimension defining its depth.
Next, you incorporate the token's positional encoding into the embeddings and apply dropout as a regularization technique to mitigate overfitting.
You can apply positional encoding to get the encoded tokens.
```python
VOCAB_SIZE = len(vocab)
EMBEDDING_DIM = 10

embedding = nn.Embedding(VOCAB_SIZE, EMBEDDING_DIM)
my_embeddings = embedding(token_indexs)

class PositionalEncoding(nn. Module):
	def __init__(self, emb_size, dropout, maxlen = 5000):
		super(PositionalEncoding, self).__init__()
		# Create a positional encoding matrix as per the Transformer paper's formula
		den = torch.exp(-torch.arange(0, emb_size, 2) * math.log(10000) / emb_size)
		pos = torch.arange(0, maxlen).reshape(maxlen, 1)
		pos_embedding = torch.zeros((maxlen, emb_size))
		pos_embedding[:, 0::2] = torch.sin(pos * den)
		pos_embedding[:, 1::2] = torch.cos(pos * den)
		pos_embedding = pos_embedding.unsqueeze(-2)
		
		self.dropout = nn.Dropout(dropout)
		self.register_buffer('pos_embedding', pos_embedding)
	
	def forward(self, token_embedding: torch.Tensor):
		# Apply the positional encodings to the input token embeddings
		return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0),:])

encoded_tokens = positional_encoding(my_embeddings)
```

You define a module with learnable parameters for positional encoding.
An nn.parameter tensor is instantiated with the desired shape.
This positional encoding is then integrated with the embeddings.
You can also apply dropout as a regularization strategy to reduce the risk of overfitting.
Finally, the module applies these positional encodings to obtain the final encoded tokens.
```python
class PositionalEncoding(nn.Module):
	def __init__(self, emb_size, dropout, maxlen=5000):
		super(PositionalEncoding, self).__init_()
		self.dropout = nn.Dropout(p=dropout)
		
		# Create a learnable positional encoding parameter
		# Shape: (maxlen, emb size)
		self.pos_embedding = nn.Parameter(torch.zeros(maxlen, emb_size))
		# Initialize
		nn.init.kaiming_uniform_(self.pos_embedding, a=math.sqrt(5))

def forward(self, token_embedding: torch.Tensor):
	# Apply the positional encodings to the input token embeddings
	seq_len = token_embedding.size(0)
	# Get the relevant portion of the positional encoding
	pe = self.pos_embedding[:seq_len, :]
	# Add positional encoding to each batch
	token_embedding = token_embedding + pe.unsqueeze(1)
	return self.dropout(token_embedding)

encoded_tokens = positional_encoding(my_embeddings)
```

## Recap

- Positional encoding incorporates information about the position of each embedding within the sequence.
- Positional encoding consists of a series of sine and cosine waves and involves two parameters.
- The `pos` parameter represents the position of the sine wave over time.
- The parameter `i`, the dimension index, effectively generates a unique sine or cosine wave for each embedding, controlling the number of oscillations for each wave.
- Positional encodings can also be learnable parameters. These learnable parameters, represented by tensors, are added to the embedding vector and optimized during training.
- Segment embeddings used in certain models, such as BERT, are related to positional encodings, providing additional positional information.
