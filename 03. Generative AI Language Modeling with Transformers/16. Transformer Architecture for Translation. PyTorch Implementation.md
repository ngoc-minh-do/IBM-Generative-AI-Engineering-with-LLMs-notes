# Transformer Architecture for Translation: PyTorch Implementation

## Goal
 - create an encoder decoder model for translation tasks using PyTorch
 - then train the model to generate German to English translations

## Data loader
To begin with, let's load the dataset using a batch size of 100, a hyperparameter that can be tuned during the training process.
```python
%run Multi30K_de_en_dataloader.py
train_dataloader, _ = get_translation_dataloaders(batch_size = 1)
```

The output from the dataset loading process consists of pairs of sentences in German and English, representing the source and target languages for the translation task.
These are denoted as `src` for the source, German sentences, and `tgt` for the target, English sentences.
```python
german, english = next(data_itr)
src, tgt = next(data_itr)
```

## Masking
Just like training a decoder model, it's important to ensure that the model isn't influenced by future words for the target, so you should generate a causal mask.
The image here shows the functions composition.
```python
def generate_square_subsequent_mask(sz,device=DEVICE):
    mask = (torch.triu(torch.ones((sz, sz), device=device)) == 1).transpose(0, 1)
    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))
    return mask
```
![[Pasted image 20250321233819.png|400]]

The `create_mask` function is designed to construct masks for the source and target sequences within a transformer model.
First, invoke the square mask from before.
For the source sequence, the mask is typically an array of Boolean values initialized to false.
There's no need to mask future tokens since the entire source sequence should be visible at once during the encoding process.
Finally, this function also produces padding masks for both the target and source sequences.
These masks flag the positions of padding tokens, commonly marked with zeros in the input tensors as true.
```python
def create_mask(src, tgt,device=DEVICE):
    src_seq_len = src.shape[0]
    tgt_seq_len = tgt.shape[0]

    tgt_mask = generate_square_subsequent_mask(tgt_seq_len)
    src_mask = torch.zeros((src_seq_len, src_seq_len),device=DEVICE).type(torch.bool)

    src_padding_mask = (src == PAD_IDX).transpose(0, 1)
    tgt_padding_mask = (tgt == PAD_IDX).transpose(0, 1)
    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask
```

## Positional encoding
Here, you have a positional encoding module that injects some information about the relative or absolute position of the tokens in the sequence.
```python
# Add positional information to the input tokens
class PositionalEncoding(nn.Module):
    def __init__(self,
                 emb_size: int,
                 dropout: float,
                 maxlen: int = 5000):
        super(PositionalEncoding, self).__init__()
        den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)
        pos = torch.arange(0, maxlen).reshape(maxlen, 1)
        pos_embedding = torch.zeros((maxlen, emb_size))
        pos_embedding[:, 0::2] = torch.sin(pos * den)
        pos_embedding[:, 1::2] = torch.cos(pos * den)
        pos_embedding = pos_embedding.unsqueeze(-2)

        self.dropout = nn.Dropout(dropout)
        self.register_buffer('pos_embedding', pos_embedding)

    def forward(self, token_embedding: Tensor):
        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])
```
![[Pasted image 20250321234257.png|300]]
## Token embedding
This is a token-embedding module for generating text embeddings in the form of a vector.
```python
class TokenEmbedding(nn.Module):
    def __init__(self, vocab_size: int, emb_size):
        super(TokenEmbedding, self).__init__()
        self.embedding = nn.Embedding(vocab_size, emb_size)
        self.emb_size = emb_size

    def forward(self, tokens: Tensor):
        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)
```
![[Pasted image 20250321234317.png|300]]

## Encoder-decoder model
Let's now put all the components together to create an encoder-decoder model for translation.
First, create the embedding layers for the source and target.
Next, create the positional encoding layer, then create the transformer.
Finally, create a generator layer similar to the output layer on a neural network.
```python
class Seq2SeqTransformer(nn.Module):
    def __init__(self,
                 num_encoder_layers: int,
                 num_decoder_layers: int,
                 emb_size: int,
                 nhead: int,
                 src_vocab_size: int,
                 tgt_vocab_size: int,
                 dim_feedforward: int = 512,
                 dropout: float = 0.1):
        super(Seq2SeqTransformer, self).__init__()

        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)
        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)
        self.positional_encoding = PositionalEncoding(
            emb_size, dropout=dropout)
        self.transformer = Transformer(d_model=emb_size,
                                       nhead=nhead,
                                       num_encoder_layers=num_encoder_layers,
                                       num_decoder_layers=num_decoder_layers,
                                       dim_feedforward=dim_feedforward,
                                       dropout=dropout)
        self.generator = nn.Linear(emb_size, tgt_vocab_size)
```

There are several methods in this class.
In the `forward` method, you'll first generate the token embeddings and positional encodings for both the source and target sequences.
These embeddings are then provided as input to the transformer model, which comprises both the encoder and decoder components.
Simultaneously, you'll apply the appropriate masks instructing the transformer to selectively ignore certain parts of the input sequence during processing.
Finally, the output from the transformer passes through a linear layer, which is not shown here.
This layer is responsible for generating the output vectors, which are the predictions returned by the model.
```python
    def forward(self,
                src: Tensor,
                trg: Tensor,
                src_mask: Tensor,
                tgt_mask: Tensor,
                src_padding_mask: Tensor,
                tgt_padding_mask: Tensor,
                memory_key_padding_mask: Tensor):
        src_emb = self.positional_encoding(self.src_tok_emb(src))
        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))
        outs = self.transformer(src_emb, tgt_emb, src_mask, tgt_mask, None,
                                src_padding_mask, tgt_padding_mask, memory_key_padding_mask)
        outs =outs.to(DEVICE)
        return self.generator(outs)
```


Now, let's define the encoder method.
It's important to note that this method is created for learning purposes only.
In practice, the transformer layer itself handles both encoding and decoding processes.
Within the encoder method, the source sequence undergoes token embedding and positional encoding.
Subsequently, the sequence is processed by the actual encoder, resulting in an encoded vector, often referred to as the memory.
```python
    def encode(self, src: Tensor, src_mask: Tensor):
        return self.transformer.encoder(self.positional_encoding(
                            self.src_tok_emb(src)), src_mask)
```

Next, you'll define the decoder method, which takes the target sequence and the memory as inputs.
The target sequence receives token embedding and positional encoding, similar to the source sequence.
This prepared sequence is then passed to the decoder along with the memory, culminating in the generation of the result vector.
```python
    def decode(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor):
        return self.transformer.decoder(self.positional_encoding(
                          self.tgt_tok_emb(tgt)), memory,
                          tgt_mask)
```
![[Pasted image 20250321234429.png|400]]

## Training
The training process for transformers has some similarities to other methods.
But let's focus on the key distinctions.
Iterate over the source, `src`, and target, `tgt`, sequences provided in each batch.
The target input is essentially the sequence with the last token removed.
Then generate the necessary masks.
A forward pass through the model generates the predictions.
During this phase, you'll also input the target input.
This procedure is similar to what happens during inference or prediction, where the transformer relies on its own previous outputs to make future predictions.
The target output is essentially the target input shifted forward in time.
Internally, the transformer employs the mask to recursively make predictions, building upon its previous outputs.
The loss is computed using the target output and the output logits.
```python
def train_epoch(model, optimizer, train_dataloader):
    model.train()
    losses = 0

    # Wrap train_dataloader with tqdm for progress logging
    train_iterator = tqdm(train_dataloader, desc="Training", leave=False)

    for src, tgt in train_iterator:
        src = src.to(DEVICE)
        tgt = tgt.to(DEVICE)

        tgt_input = tgt[:-1, :]

        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)
        src_mask = src_mask.to(DEVICE)
        tgt_mask = tgt_mask.to(DEVICE)
        src_padding_mask = src_padding_mask.to(DEVICE)
        tgt_padding_mask = tgt_padding_mask.to(DEVICE)

        logits = model(src, tgt_input, src_mask, tgt_mask, src_padding_mask, tgt_padding_mask, src_padding_mask)
        logits = logits.to(DEVICE)

        optimizer.zero_grad()

        tgt_out = tgt[1:, :]
        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))
        loss.backward()

        optimizer.step()
        losses += loss.item()

        # Update tqdm progress bar with the current loss
        train_iterator.set_postfix(loss=loss.item())

    return losses / len(list(train_dataloader))
```

## Evaluation
Similarly, you can evaluate the loss on the training data using the validation data.
```python
def evaluate(model):
    model.eval()
    losses = 0

	for src, tgt in val_dataloader:
        src = src.to(DEVICE)
        tgt = tgt.to(DEVICE)

        tgt_input = tgt[:-1, :]

        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)
        logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)

        tgt_out = tgt[1:, :]
        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))
        losses += loss.item()

    return losses / len(list(val_dataloader))
```

## Inference
The decoding process or inference begins by preparing the inputs.
You'll start by constructing the memory tensor, which is derived by passing the source input and its corresponding mask through the `model.encode` function.
Here, the German source text is encoded as illustrated.
Next, initialize a tensor named `ys`, setting its dimensions to 1, 1 and filling it with the start symbol.
This symbol serves as the initial token for the decoding process.
Now, let's concentrate on the actual decoded text.
In the decoder, you'll input the mask, ensuring attention is given only to the tokens already predicted.
Alongside, pass the memory tensor which contains the encoded context from the source input.
The output of the decoder behaves similarly to that of a neural network.
Identify the maximum value corresponding to the predicted words index in the probability distribution generated by the model.
This index is then used to retrieve the predicted word from the vocabulary.
Now, add the output token index and show the output token here.
This is used as input to the decoder.
For the next iteration, repeat the process of getting the output token index.
This is used as input to the decoder.
For the next iteration, you'll repeat the process.
You'll stop if you see the EOS tag or when you reach maximum iterations.
```python
def greedy_decode(model, src, src_mask, max_len, start_symbol):
    src = src.to(DEVICE)
    src_mask = src_mask.to(DEVICE)

    memory = model.encode(src, src_mask)
    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)
    for i in range(max_len-1):
        memory = memory.to(DEVICE)
        tgt_mask = (generate_square_subsequent_mask(ys.size(0))
                    .type(torch.bool)).to(DEVICE)
        out = model.decode(ys, memory, tgt_mask)
        out = out.transpose(0, 1)
        prob = model.generator(out[:, -1])
        _, next_word = torch.max(prob, dim=1)
        next_word = next_word.item()

        ys = torch.cat([ys,
                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0)
        if next_word == EOS_IDX:
            break
    return ys
```

## Model parameters
As you can see here, the hyperparameters and other essential settings for the training process have been meticulously configured.
```python
# Define special symbols and indices
UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3
# Make sure the tokens are in order of their indices to properly insert them in vocab
special_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']
DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

loss_fn = CrossEntropyLoss(ignore_index=PAD_IDX)

SRC_VOCAB_SIZE = len(vocab_transform[SRC_LANGUAGE])
TGT_VOCAB_SIZE = len(vocab_transform[TGT_LANGUAGE])
EMB_SIZE = 512
NHEAD = 8
FFN_HID_DIM = 512
BATCH_SIZE = 128
NUM_ENCODER_LAYERS = 3
NUM_DECODER_LAYERS = 3
```

## Model training
Now, let's train the model.
First, initialize the data loaders, train data loader, and validation data loader.
Next, the transformer model is instantiated with parameters.
For optimization, you can choose the Adam optimizer while defining learning rates and momentums.
Then train the model using the train and evaluate the function.
```python
train_dataloader, val_dataloader = get_translation_dataloaders(batch_size = BATCH_SIZE)
transformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE,
                                 NHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, FFN_HID_DIM)
transformer = transformer.to(DEVICE)
optimizer = torch.optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)
TrainLoss=[]
ValLoss=[]

NUM_EPOCHS = 10

for epoch in range(1, NUM_EPOCHS+1):
    start_time = timer()
    train_loss = train_epoch(transformer, optimizer, train_dataloader)
    TrainLoss.append(train_loss)
    end_time = timer()
    val_loss = evaluate(transformer)
    ValLoss.append(val_loss)
    print((f"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, "f"Epoch time = {(end_time - start_time):.3f}s"))
torch.save(transformer.state_dict(), 'transformer_de_to_en_model.pt')
```

## Application: German to English translation
Here, you have a function called translate.
It takes your model and a source sentence and gives back the translated sentence.
```python
# translate input sentence into target language
def translate(model: torch.nn.Module, src_sentence: str):
    model.eval()
    src = text_transform[SRC_LANGUAGE](src_sentence).view(-1, 1)
    num_tokens = src.shape[0]
    src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)
    tgt_tokens = greedy_decode(
        model,  src, src_mask, max_len=num_tokens + 5, start_symbol=BOS_IDX).flatten()
    return " ".join(vocab_transform[TGT_LANGUAGE].lookup_tokens(list(tgt_tokens.cpu().numpy()))).replace("<bos>", "").replace("<eos>", "")
```

In the loop, you can draw sentences from your dataset and print the original German sentence alongside its ground truth English translation and the model's translation.
Since the primitive model was used, you can see that the model translation is very similar to the actual English translation.
```python
for n in range(5):
    german, english= next(data_itr)

    print("German Sentence:",index_to_german(german).replace("<bos>", "").replace("<eos>", ""))
    print("English Translation:",index_to_eng(english).replace("<bos>", "").replace("<eos>", ""))
    print("Model Translation:",translate(transformer,index_to_german(german)))
    print("_________\n")
```
![[Pasted image 20250322000125.png|400]]

## Recap
- the `create_mask` function is designed to construct masks for the source and target sequences within a transformer model.
- The linear layer is responsible for generating the output vectors, which are the predictions returned by the model.
- In practice, the transformer layer handles both encoding and decoding processes.
- Within the encoder method
	- the source sequence undergoes token embedding and positional encoding.
	- Subsequently, the sequence is processed by the actual encoder, resulting in an encoded vector, often referred to as the memory.
- The decoder method 
	- takes the target sequence and the memory as inputs.
	- The target sequence receives token embedding and positional encoding similar to the source sequence.
	- This prepared sequence is then passed to the decoder along with the memory, culminating in the generation of the result vector.
- The translate function takes your model and a source sentence and gives back the translated sentence.