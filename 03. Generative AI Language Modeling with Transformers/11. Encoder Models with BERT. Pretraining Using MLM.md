# Encoder Models with BERT: Pretraining Using MLM

## BERT (Bidirectional encoder representations from transformers) 
- developed by Google, revolutionized NLP with its deep understanding of word context and semantics.
- It's pre-trained via self supervised learning on vast text corpora using masked language modeling MLM and next sentence prediction.
- BERT's architecture allows for fine-tuning specific tasks like text summarization, question answering, and sentiment analysis, adapting its comprehensive knowledge to specialized applications.
![[Pasted image 20250320183229.png|400]]

BERT utilizes an encoder only architecture that is only the encoder part of the transformer model.
This design allows BERT to process entire sequences of text simultaneously, theoretically enhancing its understanding of the context and nuances within the text.
![[Pasted image 20250320183256.png|400]]

Unlike autoregressive models, BERT is not typically used for text generation tasks.
Instead, it excels at a broad range of language comprehension tasks.
Consider the task of predicting a known word denoted by the token `MASK` within the input `CLS, IBM, MASK, me, BERT, SEP`.
An autoregressive model would only have access to the `CLS` and `IBM` tokens to predict the masked word.
However, Bert can utilize the full context from both sides of the masked token, which allows for a more informed prediction, in this case, taught.
One other difference is that the BERT models contain the segment embeddings used in training.
![[Pasted image 20250320183415.png|400]]
## MLM (Masked language modeling).
It involves randomly masking some of the input tokens and training BERT to predict the original mask tokens.
This task helps Bert learn contextual representations and understand the relationships between words.
The prediction method is almost identical to that used in decoder models like GPT.
![[Pasted image 20250320184518.png|400]]

The encoder outputs a set of contextual embeddings depicted in gray, which are then passed through another layer and converted into a set of logits shown in red.
The masked word is identified by selecting the word corresponding to the index with the highest logit value.
The main distinction lies in the fact that encoder models have access to the entire sequence.
![[Pasted image 20250320185628.png|400]]

## Bi-directional transformers
A hallmark of encoder models like BERT is their bi-directional training method, which enables the model to understand the context from both sides of any given word in a sentence.
For example, take the task of predicting the missing word in the `farmers cultivate the ? to grow crops`.
In contrast to models such as GPT, which only consider the preceding text, `farmers cultivate the` due to their causal attention mechanism, BERTS architecture allows it to analyze text from both directions.
And decoder models like GPT causal attention is visually represented by an X for the masked attention and OS for the active attention units.
![[Pasted image 20250320185946.png|400]]

BERT, however, employs a bi-directional mechanism without such masking, fully utilizing the context around each word as depicted by the presence of only Os.
This bi-directional approach gives Bert a more sophisticated grasp of language nuances and complex word interdependencies.
![[Pasted image 20250320190006.png|400]]

## Masking strategy
According to the original BERT paper, you should randomly hide 15% of the input words and try to predict them.
You can use the hidden vectors of these masked words to make predictions using a vocabulary.
Instead of reconstructing the entire input, you should only focus on predicting the masked words.
Although this allows you to obtain a bi-directional pre-trained model.
A downside is that you are creating a mismatch between pre-training and fine-tuning since the mask token does not appear during fine-tuning.
To address this, don't always replace masked words with the mask token.
When generating training data, 15% of the word positions are randomly chosen.
If a word is chosen, you can replace it with the mask token 80% of the time, a random token 10% of the time and leave it unchanged 10% of the time.
This data is used to predict the original word using cross entropy loss.
![[Pasted image 20250320190246.png|400]]

Let's consider an example to illustrate.
85% of the words remain unchanged and from the remaining 15% 80% are replaced with mask 10% with a random token like the and 10% are used for prediction without any changes like cultivate.
![[Pasted image 20250320190322.png|400]]

## Recap
- BERT's architecture allow for fine-tuning specific tasks like
	- text summarization
	- question answering
	- sentiment analysis.
- BERT utilizes an encoder only architecture which allows it to process entire sequences of text simultaneously
- Theoretically enhancing its understanding of the context and nuances within the text.
- Masked language modeling (MLM) involves randomly masking some of the input tokens and training BERT to predict the original masked tokens.
- For prediction
	- Encoder outputs a set of contextual embeddings
	- Contextual embeddings are then passed through another layer and converted into a set of logits.
	- The masked word is identified by selecting the word corresponding to the index with the highest logit value.
- The main distinction between decoder and encoder models lies in the fact that encoder models have access to the entire sequence.
- A hallmark of encoder models like BERT is their bi-directional training method
	- enables the model to understand the context from both sides of any given word in a sentence.