# Pretraining BERT Models with PyTorch

## Creating dataset object from CSV file
First, create a custom dataset class called `BERTCSVDataset` to load data from the CSV file into a PyTorch dataset.
The `getitem` method retrieves an individual item from the dataset based on the index IDX.
It accesses the corresponding row from the data frame and converts the JSON formatted strings back to tensors, including `bert_input`, `bert_label`, `segment_label`, `is_next`.
```python
class BERTCSVDataset(Dataset):
    def __init__(self, filename):
        self.data = pd.read_csv(filename)
        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        row = self.data.iloc[idx]
        try:
            bert_input = torch.tensor(json.loads(row['BERT Input']), dtype=torch.long)
            bert_label = torch.tensor(json.loads(row['BERT Label']), dtype=torch.long)
            segment_label = torch.tensor([int(x) for x in row['Segment Label'].split(',')], dtype=torch.long)
            is_next = torch.tensor(row['Is Next'], dtype=torch.long)
            original_text = row['Original Text']  # If you want to use it
        except json.JSONDecodeError as e:
            print(f"Error decoding JSON for row {idx}: {e}")
            print("BERT Input:", row['BERT Input'])
            print("BERT Label:", row['BERT Label'])
            # Handle the error, e.g., by skipping this row or using default values
            return None  # or some default values

            # Tokenizing the original text with BERT
        encoded_input = self.tokenizer.encode_plus(
            original_text,
            add_special_tokens=True,
            max_length=512,
            padding='max_length',
            truncation=True,
            return_tensors="pt"
        )

        input_ids = encoded_input['input_ids'].squeeze()
        attention_mask = encoded_input['attention_mask'].squeeze()

        return(bert_input, bert_label, segment_label, is_next, input_ids, attention_mask, original_text)
```

## Collating data in batches
The collate function, `collate_batch` is used for batching and padding sequences in a PyTorch data loader.
```python
PAD_IDX = 0
def collate_batch(batch):
    bert_inputs_batch, bert_labels_batch, segment_labels_batch, is_nexts_batch,input_ids_batch,attention_mask_batch,original_text_battch = [], [], [], [],[],[],[]
    for bert_input, bert_label, segment_label, is_next,input_ids,attention_mask,original_text in batch:
        # Convert each sequence to a tensor and append to the respective list
        bert_inputs_batch.append(torch.tensor(bert_input, dtype=torch.long))
        bert_labels_batch.append(torch.tensor(bert_label, dtype=torch.long))
        segment_labels_batch.append(torch.tensor(segment_label, dtype=torch.long))
        is_nexts_batch.append(is_next)
        input_ids_batch.append(input_ids)
        attention_mask_batch.append(attention_mask)
        original_text_battch.append(original_text)

    # Pad the sequences in the batch
    bert_inputs_final = pad_sequence(bert_inputs_batch, padding_value=PAD_IDX, batch_first=False)
    bert_labels_final = pad_sequence(bert_labels_batch, padding_value=PAD_IDX, batch_first=False)
    segment_labels_final = pad_sequence(segment_labels_batch, padding_value=PAD_IDX, batch_first=False)
    is_nexts_batch = torch.tensor(is_nexts_batch, dtype=torch.long)

    return bert_inputs_final, bert_labels_final, segment_labels_final, is_nexts_batch
```

## Creating dataloaders
Now you can create data loaders for the train and test datasets using PyTorches data loader class.
You start by defining the batch size.
Next, you specify the file paths for the train and test datasets.
You can then create instances of the BERTCSVDataset class for the train and test datasets using the specified file paths.
Finally, create the corresponding data loaders.
```python
BATCH_SIZE = 2

train_dataset_path = './bert_dataset/bert_train_data.csv'
test_dataset_path = './bert_dataset/bert_test_data.csv'

train_dataset = BERTCSVDataset(train_dataset_path)
test_dataset = BERTCSVDataset(test_dataset_path)

train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)
test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_batch)
```

## BERT embedding
The BERT embedding class is a module used for embedding operations and positional encodings in a BERT model.
The main difference from other models is that you add segment embeddings.
```python
class BERTEmbedding (nn.Module):
    def __init__(self, vocab_size, emb_size ,dropout=0.1,train=True):
        super().__init__()

        self.token_embedding = TokenEmbedding( vocab_size,emb_size )
        self.positional_encoding = PositionalEncoding(emb_size,dropout)
        self.segment_embedding = nn.Embedding(3, emb_size)
        self.dropout = torch.nn.Dropout(p=dropout)

    def forward(self, bert_inputs, segment_labels=False):
        my_embeddings=self.token_embedding(bert_inputs)
        if self.train:
          x = self.dropout(my_embeddings + self.positional_encoding(my_embeddings) + self.segment_embedding(segment_labels))
        else:
          x = my_embeddings + self.positional_encoding(my_embeddings)

        return x
```

## BERT model
Let's now define the complete BERT model.
Define a BERT class, a subclass of `torch.nn.Module`.
The `init` method initializes the BERT model by specifying the vocabulary size, model dimension, `d_model`, number of layers, `n_layers`, number of attention heads, `heads`, and dropout rate, `dropout`.
The BERT model includes an embedding layer that combines token embeddings and segment embeddings.
This is achieved with the BERT embedding class defined earlier.
You can use transformer encoder layers to encode the input embeddings.
The number of transformer layers in the specified configurations are determined by the `n_layers`, `heads`, `dropout`, and `d_model` parameters.
The model has a linear layer for next sentence prediction (NSP), which predicts the relationship between two consecutive sentences.
It takes the output from the transformer encoder with the shape of sequence length, batch size, embedding dimension.
The input is the CLS embedding for each sequence.
Similarly, a linear layer for masked language modeling (MLM) predicts the mask tokens in the input sequence.
It takes the output from the transformer encoder and produces predictions across the vocabulary.
The expected shape of the output is sequence length, batch size, vocab size.
The `forward` method defines the forward pass of the BERT model.
It takes bert_inputs, the input tokens, and segment_labels as inputs and returns the predictions for NSP and MLM.
```python
class BERT(torch.nn.Module):
    def __init__(self, vocab_size, d_model=768, n_layers=12, heads=12, dropout=0.1):
        """
        vocab_size: The size of the vocabulary.
        d_model: The size of the embeddings (hidden size).
        n_layers: The number of Transformer layers.
        heads: The number of attention heads in each Transformer layer.
        dropout: The dropout rate applied to embeddings and Transformer layers.
        """
        super().__init__()
        self.d_model = d_model
        self.n_layers = n_layers
        self.heads = heads

        # Embedding layer that combines token embeddings and segment embeddings
        self.bert_embedding = BERTEmbedding(vocab_size, d_model, dropout)

        # Transformer Encoder layers
        self.encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=heads, dropout=dropout,batch_first=False)
        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=n_layers)

        # Linear layer for Next Sentence Prediction
        self.nextsentenceprediction = nn.Linear(d_model, 2)

        # Linear layer for Masked Language Modeling
        self.masked_language = nn.Linear(d_model, vocab_size)

    def forward(self, bert_inputs, segment_labels):
        """
        bert_inputs: Input tokens.
        segment_labels: Segment IDs for distinguishing different segments in the input.
        mask: Attention mask to prevent attention to padding tokens.

        return: Predictions for next sentence task and masked language modeling task.
        """

        padding_mask = (bert_inputs == PAD_IDX).transpose(0, 1)
        # Generate embeddings from input tokens and segment labels
        my_bert_embedding = self.bert_embedding(bert_inputs, segment_labels)

        # Pass embeddings through the Transformer encoder
        transformer_encoder_output = self.transformer_encoder(my_bert_embedding,src_key_padding_mask=padding_mask)

        next_sentence_prediction = self.nextsentenceprediction(transformer_encoder_output[ 0,:])

        # Masked Language Modeling: Predict all tokens in the sequence
        masked_language = self.masked_language(transformer_encoder_output)

        return  next_sentence_prediction, masked_language
```

## Creating an instance of the BERT model
Let's now see how to set up the parameters and create an instance of the BERT model.
The embedding dimension is 10, and the vocabulary size is determined based on the length of vocab.
This mini BERT model has two transformer layers and an initial number of attention heads set to two.
The drop out rate is 0.1.
With these parameters, you can create the BERT model instance for further use.
```python
EMBEDDING_DIM = 10

# Define parameters
vocab_size = 147161  # Replace VOCAB_SIZE with your vocabulary size
d_model = EMBEDDING_DIM  # Replace EMBEDDING_DIM with your embedding dimension
n_layers = 2  # Number of Transformer layers
initial_heads = 12 # Initial number of attention heads
initial_heads = 2
# Ensure the number of heads is a factor of the embedding dimension
heads = initial_heads - d_model % initial_heads

dropout = 0.1  # Dropout rate

# Create an instance of the BERT model
model = BERT(vocab_size, d_model, n_layers, heads, dropout)
```

## Evaluation
Now that the model is created, the next step is to train it on the data and assess its performance.
To facilitate this process, you can define and evaluate function.
This function will be used to evaluate the BERT model's performance and to assess the model's progress during the training phase.
First, the loss function is defined.
The loss function is cross entropy loss, which you can use to calculate the loss between the predicted and the actual values.
The evaluate function takes several arguments, data loader, model, `loss_fn`, and device.
Within the function, the BERT model is put into evaluation mode using `model.eval()`, this disables dropout and other training specific behaviors.
Variables are initialized to keep track of the `total_loss`, `total_next_sentence_loss`, `total_mask_loss` and total number of batches.
The evaluation is performed with gradients turned off using `torch.no_grad()`, this saves memory and computation since gradients are not needed for validation.
The function iterates through the batches in the provided data loader.
A forward pass is performed with the BERT model to obtain predictions for the next sentence and mask language tasks.
The two losses are then calculated and summed up to obtain the total loss for the batch.
You can calculate the average loss, average next sentence loss, and average mask loss by dividing the total losses by the total number of batches.
The average losses are printed and returned from the function.
```python
PAD_IDX=0
loss_fn_mlm = nn.CrossEntropyLoss(ignore_index=PAD_IDX)# The loss function must ignore PAD tokens and only calculates loss for the masked tokens
loss_fn_nsp = nn.CrossEntropyLoss()

def evaluate(dataloader=test_dataloader, model=model, loss_fn_mlm=loss_fn_mlm, loss_fn_nsp=loss_fn_nsp, device=device):
    model.eval()  # Turn off dropout and other training-specific behaviors

    total_loss = 0
    total_next_sentence_loss = 0
    total_mask_loss = 0
    total_batches = 0
    with torch.no_grad():  # Turn off gradients for validation, saves memory and computations
        for batch in dataloader:
            bert_inputs, bert_labels, segment_labels, is_nexts = [b.to(device) for b in batch]

            # Forward pass
            next_sentence_prediction, masked_language = model(bert_inputs, segment_labels)

            # Calculate loss for next sentence prediction
            # Ensure is_nexts is of the correct shape for CrossEntropyLoss
            next_loss = loss_fn_nsp(next_sentence_prediction, is_nexts.view(-1))

            # Calculate loss for predicting masked tokens
            # Flatten both masked_language predictions and bert_labels to match CrossEntropyLoss input requirements
            mask_loss = loss_fn_mlm(masked_language.view(-1, masked_language.size(-1)), bert_labels.view(-1))

            # Sum up the two losses
            loss = next_loss + mask_loss
            if torch.isnan(loss):
                continue
            else:
                total_loss += loss.item()
                total_next_sentence_loss += next_loss.item()
                total_mask_loss += mask_loss.item()
                total_batches += 1

    avg_loss = total_loss / (total_batches + 1)
    avg_next_sentence_loss = total_next_sentence_loss / (total_batches + 1)
    avg_mask_loss = total_mask_loss / (total_batches + 1)

    print(f"Average Loss: {avg_loss:.4f}, Average Next Sentence Loss: {avg_next_sentence_loss:.4f}, Average Mask Loss: {avg_mask_loss:.4f}")
    return avg_loss
```

## Training the model
Before the training starts, define the optimizer for training the BERT model.
The optimizer is Adam.
Let's look at the training loop.
Within each epoch, you can iterate through the training data and batches.
For each batch, perform the forward pass, calculate the loss, and update the models parameters through back propagation and gradient clipping.
After each epoch, you can print the average training loss and evaluate the model's performance on the test set.
You can also save the model after each epoch.
```python
# Define the optimizer
optimizer = Adam(model.parameters(), lr=1e-4, weight_decay=0.01, betas=(0.9, 0.999))

# Training loop setup
num_epochs = 1
total_steps = num_epochs * len(train_dataloader)

# Define the number of warmup steps, e.g., 10% of total
warmup_steps = int(total_steps * 0.1)

# Create the learning rate scheduler
scheduler = get_linear_schedule_with_warmup(optimizer,
                                            num_warmup_steps=warmup_steps,
                                            num_training_steps=total_steps)

# Lists to store losses for plotting
train_losses = []
eval_losses = []

for epoch in tqdm(range(num_epochs), desc="Training Epochs"):
    model.train()
    total_loss = 0

    for step, batch in enumerate(tqdm(train_dataloader, desc=f"Epoch {epoch + 1}")):
        bert_inputs, bert_labels, segment_labels, is_nexts = [b.to(device) for b in batch]

        optimizer.zero_grad()
        next_sentence_prediction, masked_language = model(bert_inputs, segment_labels)

        next_loss = loss_fn_nsp(next_sentence_prediction, is_nexts)
        mask_loss = loss_fn_mlm(masked_language.view(-1, masked_language.size(-1)), bert_labels.view(-1))

        loss = next_loss + mask_loss
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        optimizer.step()
        scheduler.step()  # Update the learning rate

        total_loss += loss.item()

        if torch.isnan(loss):
            continue
        else:
            total_loss += loss.item()

    avg_train_loss = total_loss / len(train_dataloader) + 1
    train_losses.append(avg_train_loss)
    print(f"Epoch {epoch+1} - Average training loss: {avg_train_loss:.4f}")

    # Evaluation after each epoch
    eval_loss = evaluate(test_dataloader, model, loss_fn_nsp, loss_fn_mlm, device)
    eval_losses.append(eval_loss)
```

## Next sentence prediction
Now, let's check how the models perform.
For this to find a function called `predict_nsp`, which predicts whether a second sentence follows the first sentence using a pretrained BERT model.
The function takes two sentences, a BERT model and a tokenizer as inputs.
The function tokenizes the input sentences using the `tokenizer.encode_plus` method, which returns a dictionary of tokenized inputs.
The tokenized inputs are then converted into tensors and move to the appropriate device for processing.
The BERT model is used to make predictions by passing the token and segment tensors as input.
The first element of the logits tensor is selected and unsqueeze to add an extra dimension, making its shape 1, 2.
The logits are passed through a `softmax` function to obtain probabilities, and the prediction is obtained by taking the argmax.
Finally, the prediction is interpreted and returned as a string, indicating whether or not the second sentence follows the first.
For example, two sentences are passed to the `predict_nsp` function along with a BERT model and tokenizer.
The result is printed indicating whether the second sentence follows the first or not based on the models prediction.
```python
# Initialize the tokenizer with the BERT model's vocabulary
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model.eval()

def predict_nsp(sentence1, sentence2, model, tokenizer):
    # Tokenize sentences with special tokens
    tokens = tokenizer.encode_plus(sentence1, sentence2, return_tensors="pt")
    tokens_tensor = tokens["input_ids"].to(device)
    segment_tensor = tokens["token_type_ids"].to(device)

    # Predict
    with torch.no_grad():
        # Assuming the model returns NSP predictions first
        nsp_prediction, _ = model(tokens_tensor, segment_tensor)
        # Select the first element (first sequence) of the logits tensor
        first_logits = nsp_prediction[0].unsqueeze(0)  # Adds an extra dimension, making it [1, 2]
        logits = torch.softmax(first_logits, dim=1)
        prediction = torch.argmax(logits, dim=1).item()

    # Interpret the prediction
    return "Second sentence follows the first" if prediction == 1 else "Second sentence does not follow the first"

# Example usage
sentence1 = "The cat sat on the mat."
sentence2 = "It was a sunny day"

print(predict_nsp(sentence1, sentence2, model, tokenizer))
```

## Masked language model
Next, you can define a function to perform MLM, using the pretrained BERT model.
The function takes a sentence, a BERT model, and a tokenizer as inputs.
The function tokenizes the input sentence using the tokenizer and converts it into token IDs, including the special tokens.
The result is stored in the `tokens_tensor` variable.
Dummy segment labels filled with zeros are created as segment labels.
The BERT model is used to make predictions by passing the token tensor and segment labels as input.
The MLM logits is extracted as predictions.
The position of the mask token is identified using the `nonzero` method and stored in the mask token index variable.
Remember that all tokens accept the mask token are zero padded. 
The predicted index for the mask token is obtained by taking the argmax of the MLM logits at the corresponding position, which is of the shape, batch size, sequence length, vocab size.
The predicted index is converted back to a token using the `convert_ids_to_tokens` method of the tokenizer.
The original sentence is then replaced with the predicted token at the position of the mask token, resulting in the predicted sentence.
The `predict_mlm` function is called with an example sentence, a BERT model and a tokenizer.
The predicted sentence is printed with the mask token replaced by the predicted token.
```python
def predict_mlm(sentence, model, tokenizer):
    # Tokenize the input sentence and convert to token IDs, including special tokens
    inputs = tokenizer(sentence, return_tensors="pt")
    tokens_tensor = inputs.input_ids.to(device)

    # Create dummy segment labels filled with zeros, assuming it's needed by your model
    segment_labels = torch.zeros_like(tokens_tensor).to(device)

    with torch.no_grad():
        # Forward pass through the model, now correctly handling the output tuple
        output_tuple = model(tokens_tensor, segment_labels)

        # Assuming the second element of the tuple contains the MLM logits
        predictions = output_tuple[1]  # Adjusted based on your model's output

        # Identify the position of the [MASK] token
        mask_token_index = (tokens_tensor == tokenizer.mask_token_id).nonzero(as_tuple=True)[1]

        # Get the predicted index for the [MASK] token from the MLM logits
        predicted_index = torch.argmax(predictions[0, mask_token_index.item(), :], dim=-1)
        predicted_token = tokenizer.convert_ids_to_tokens([predicted_index.item()])[0]

        # Replace [MASK] in the original sentence with the predicted token
        predicted_sentence = sentence.replace(tokenizer.mask_token, predicted_token, 1)

    return predicted_sentence


# Example usage
sentence = "The cat sat on the [MASK]."
print(predict_mlm(sentence, model, tokenizer))
```
## Recap
- To create a data loader, you will create a custom dataset class to load data from a CSV file into a PyTorch dataset.
- Use a collate function and create corresponding data loaders for the train and test datasets.
- To create a BERT model, you'll define a class for embedding operations and positional encodings, and a BERT model class, both subclasses of `torch.nn.module`.
- In the BERT model class, you will define the embedding layer, transformer encoder layers, linear layers for NSP and MLM, and the forward pass of the BERT model.
- As part of BERT model training, you'll define a function to evaluate the BERT model's performance. Define the optimizer for training the BERT model, define the training loop, and view the loss.