# Decoder Models - PyTorch Implementation-Causal LM

## Introduction to decoder model
- Decoder models leverage just the decoder of a transformer model.
- At every stage for a specific word, the attention layers can only access the words placed before it in the sentence.
- Such models are usually termed as autoregressive models.
- However, the pre-training of decoder models typically incorporates the prediction of the next word in the given sentence. Therefore, these models are most useful in text generation.
- The decoder models are similar to the generative pre-trained transformer or GPT.

## PyTorch Implementation
Let's begin by understanding how to create a decoder model for future text tokens using PyTorch and causal LM.
Internet Movie Database or IMDB test and validation splits, are useful for training and evaluating the decoder model.
Each data record includes a sentiment label, often ignored, and a textural element is prioritized to create training data.
```python
train_iter, val_iter = IMDB()
data_itr=iter(train_iter)
next(data_itr)

# (1, "The movie starts ...")
```

### Text pipeline
However, customized vocabulary is useful for tokenizing and mapping the textual data to indices.
To achieve this, first load the IMDB dataset.
In neural language processing, or NLP, certain special tokens, such as unknown tokens, UNK, padding tokens, pad, and end-of-sentence tokens, EOS are used.
These tokens serve widely recognized purposes, such as an unknown token, UNK, represents words not found in the vocabulary, typically during inference.
However, the padding token, PAD, equalizes the lengths of sequences in a batch for models that require inputs of the same size.
The end-of-sentence token, EOS, signifies the end of a sequence, which is crucial for models to understand when a sentence or sequence of tokens ends.
```python
# Define special symbols and indices
UNK_IDX, PAD_IDX, EOS_IDX = 0, 1, 2
# Make sure the tokens are in order of their indices to properly insert them in vocab
special_symbols = ['<unk>', '<pad>', '<|endoftext|>' ]

def yield_tokens(data_iter):

    for _,data_sample in data_iter:
        yield  tokenizer(data_sample)

vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=special_symbols, special_first=True)
vocab.set_default_index(UNK_IDX)

text_to_index=lambda text: [vocab(token) for token in tokenizer(text)]
index_to_en = lambda seq_en: " ".join([vocab.get_itos()[index] for index in seq_en])
```

### Creating data for next token prediction
Next, language models predict words by analyzing the previous text, where context length acts as a hyperparameter.
Let's look at the process visualization steps.
First, specify context size, which means block size and input text.
Next, randomly select a point in sequence from the text of length block size and shift the source by one token to generate a target sequence.
This function helps to check several parameters such as block size, text, random sample stop, random start, and source sequence.
```python
def get_sample(block_size, text):
    # Determine the length of the input text
    sample_leg = len(text)
    # Calculate the stopping point for randomly selecting a sample
    # This ensures the selected sample doesn't exceed the text length
    random_sample_stop = sample_leg - block_size

    # Check if a random sample can be taken (if the text is longer than block_size)
    if random_sample_stop >= 1:
        # Randomly select a starting point for the sample
        random_start = torch.randint(low=0, high=random_sample_stop, size=(1,)).item()
        # Define the endpoint of the sample
        stop = random_start + block_size

        # Create the input and target sequences
        src_sequence = text[random_start:stop]
        tgt_sequence= text[random_start + 1:stop + 1]

    # Handle the case where the text length is exactly equal or less the block size
    elif random_sample_stop <= 0:
        # Start from the beginning and use the entire text
        random_start = 0
        stop = sample_leg
        src_sequence= text[random_start:stop]
        tgt_sequence = text[random_start + 1:stop]
        # Append an empty string to maintain sequence alignment
        tgt_sequence.append( '<|endoftext|>')

    return src_sequence, tgt_sequence
```

Let's invoke the function "get_sample" using the block size of 10.
You can observe the source and target sequences and verify that both sequences are 10 tokens long.
```python
block_size=10
src_sequences, tgt_sequence=get_sample( block_size, text)

print("src: ",src_sequences)
print("tgt: ",tgt_sequence)
```

Based on your verification, you've observed that one token shifted the source forward.
![[Pasted image 20250320170302.png|300]]

### Collate function
Next, use the collate function in parallel to collate the previous functions.
The function "get_sample" retrieves the source analysis of the previous text, and helps target using block sizes as a global variable.
It means that padding helps to ensure the sequences are of the same size in a batch.
```python
BLOCK_SIZE=30
def collate_batch(batch):
    src_batch, tgt_batch = [], []
    for _,_textt in batch:
      src_sequence,tgt_sequence=get_sample(BLOCK_SIZE,tokenizer(_textt))
      src_sequence=vocab(src_sequence)
      tgt_sequence=vocab(tgt_sequence)
      src_sequence= torch.tensor(src_sequence, dtype=torch.int64)
      tgt_sequence = torch.tensor(tgt_sequence, dtype=torch.int64)
      src_batch.append(src_sequence)
      tgt_batch.append(tgt_sequence)


    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX, batch_first=False)
    tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX, batch_first=False)

    return src_batch.to(DEVICE), tgt_batch.to(DEVICE)

BATCH_SIZE=1
dataloader = DataLoader(train_iter, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)
val_dataloader= DataLoader(val_iter , batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)
```

Next, compare the source and target indices to reveal that the tokens shift sequentially.
![[Pasted image 20250320170621.png|400]]
### Masking future tokens: Causal mask
Now, let's understand how to create a causal mask to prevent the model from being influenced by future words.
First, use the function, "generate_square_subsequent_mask".
This function helps to form a diagonal matrix, transposes it, and set zero values to negative infinity
```python
def generate_square_subsequent_mask(sz,device=DEVICE):
    mask = (torch.triu(torch.ones((sz, sz), device=device)) == 1).transpose(0, 1)
    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))
    return mask
```
![[Pasted image 20250320170745.png|400]]

However, PyTorch includes the function "generates_square_subsequent_mask" to create causal masks with sequence length as the input.
You can see that the outcome of this function is an upper triangular matrix filled with negative infinities, which are dimmentioned equally in rows and columns to the sequence length.
PyTorch creates a causal mask for transformers, ensuring predictions for a token that are dependent on the previously visible tokens.
They also use a function such as "generate_square_subsequent_mask" and create_mask to complete this task.
```python
def create_mask(src,device=DEVICE):
    src_seq_len = src.shape[0]
    src_mask = generate_square_subsequent_mask(src_seq_len)
    src_padding_mask = (src == PAD_IDX).transpose(0, 1)
    return src_mask,src_padding_mask
```

### Padding mask
Further, the encoder ignores the zero padding using a padding mask, where padding tokens are converted to true.
![[Pasted image 20250320170959.png|400]]

## Custom GPT model architecture
Now, let's explore the role of Custom GPT model architecture in creating a decoder model to predict the next token.
The Custom GPT model consists of an embedding layer, a positional encoding layer, a transformer encoder, and a linear layer that is lm_head for language modeling.
First, the embedding layer maps each input token to a dense vector of size embed_size.
However, the positional encoding layer adds positional information to the input embeddings.
Further, the transformer encoder consists of multiple layers of multi-head self attention, adding a source mask.
This helps the encoder to behave like a decoder.
Finally, the linear layer outputs the logits over the vocabulary size.
```python
class CustomGPTModel(nn.Module):
    def __init__(self, embed_size,vocab_size, num_heads, num_layers, max_seq_len=500,dropout=0.1):

        super().__init__()
        self.init_weights()
        self.embed = nn.Embedding(vocab_size, embed_size)
        self.positional_encoding = PositionalEncoding(embed_size, dropout=dropout)

		# Remaining layers are part of the TransformerDecoder
        encoder_layers = nn.TransformerEncoderLayer(d_model=embed_size, nhead=num_heads, dropout=dropout)
        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers=num_layers)
        self.embed_size = embed_size
        self.lm_head = nn.Linear(embed_size, vocab_size)

    def init_weights(self):
      for p in self.parameters():
          if p.dim() > 1:
              nn.init.xavier_uniform_(p)

    def create_mask(src,device=DEVICE):
        src_seq_len = src.shape[0]
        src_mask = nn.Transformer.generate_square_subsequent_mask(src_seq_len)
        src_padding_mask = (src == PAD_IDX).transpose(0, 1)
        return src_mask,src_padding_mask
```
![[Pasted image 20250320171325.png]]

Next, the forward method of Custom GPT model takes as an input sequence of tokens x, and adds the positional encoding.
Further, add the source mask, SRC mask, and padding mask to the encoder to get the output as a sequence of contextual embeddings.
This output is converted to logits.
```python
    def forward(self,x,src_mask=None,key_padding_mask=None):
        # Add positional embeddings to the input embeddings
        x = self.embed(x)* math.sqrt(self.embed_size) #src = self.embedding(src) * math.sqrt(self.d_model)
        x = self.positional_encoding(x)

        if src_mask is None:
            """Generate a square causal mask for the sequence. The masked positions are filled with float('-inf').
            Unmasked positions are filled with float(0.0).
            """
            src_mask, src_padding_mask = create_mask(x)

        output = self.transformer_encoder(x, src_mask,key_padding_mask)
        x = self.lm_head(x)

        return x
```
## Recap
- Each data record includes a sentiment label and a textual element to create training data.
- To predict the decoder model for the next token, you need to specify the context size and randomly select a point in the sequence.
- Collate function can be used in parallel to collate the previous functions.
- To create the causal mask
	- use the function "generate_square_subsequent_mask".
	- To form a diagonal matrix, transpose it
	- set the zero value to negative infinity.
- Custom GPT model architecture helps create a decoder model to predict the next token.
	Here, the embedding layer maps each input token to a dense vector of size embedded size, and the linear layer outputs the logits over the vocabulary size.
