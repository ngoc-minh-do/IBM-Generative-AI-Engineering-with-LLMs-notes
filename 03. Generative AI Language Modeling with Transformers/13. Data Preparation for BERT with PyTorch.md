# Data Preparation for BERT with PyTorch

## Tokenization and vocabulary building

For data preparation, the initial steps involve tokenization and constructing a vocabulary.
You can follow the standard PyTorch pipeline with two nodes.
First, initialize the tokenizer using the get tokenizer function.
In this case, the tokenizer is set to tokenize text using the basic English model.
Please note that actual BERT uses word piece tokenization.
Following that, you'll define special symbols and their corresponding indices.
- `PAD` is used to represent padding tokens.
- `CLS` represents the start of a sequence.
- `SEP` is used as a separator between sequences.
- `MASK` is used for masked language modeling (MLM)
- `UNK` represents unknown tokens.
```python
tokenizer = get_tokenizer("basic_english")

def yield_tokens(data_iter):
    for label, data_sample in data_iter:
        yield tokenizer(data_sample)

# Define special symbols and indices
PAD_IDX,CLS_IDX, SEP_IDX,  MASK_IDX,UNK_IDX= 0, 1, 2, 3, 4

# Make sure the tokens are in order of their indices to properly insert them in vocab
special_symbols = ['[PAD]','[CLS]', '[SEP]','[MASK]','[UNK]']

#create data splits
train_iter, test_iter = IMDB(split=('train', 'test'))

#create vocab : vocab is only built using train data
vocab=build_vocab_from_iterator(yield_tokens(all_data_iter),specials=special_symbols,special_first=True)

vocab.set_default_index(UNK_IDX)
VOCAB_SIZE=len(vocab)
print(VOCAB_SIZE)
```

## Text masking
The masking function is used to perform masking operations on tokens.
It takes a token as input and decides whether to mask it.
If the mask is false, the function immediately returns the original token with a pad label.
If the mask is true, the function randomly selects an operation with a 50% chance for each option.
In case 1, the token is replaced with mask, and the label is set to a random token from the vocabulary.
In case 2, the token remains unchanged, and the label is set to the same token.
In case 3, the token is replaced with mask and the label is set to the original token.
The function then returns the modified token and its label.
```python
def Masking(token):
    # Decide whether to mask this token (20% chance)
    mask = bernoulli_true_false(0.2)

    # If mask is False, immediately return with '[PAD]' label
    if not mask:
        return token, '[PAD]'

    # If mask is True, proceed with further operations
    # Randomly decide on an operation (50% chance each)
    random_opp = bernoulli_true_false(0.5)
    random_swich = bernoulli_true_false(0.5)

    # Case 1: If mask, random_opp, and random_swich are True
    if mask and random_opp and random_swich:
        # Replace the token with '[MASK]' and set label to a random token
        mask_label = index_to_en(torch.randint(0, VOCAB_SIZE, (1,)))
        token_ = '[MASK]'

    # Case 2: If mask and random_opp are True, but random_swich is False
    elif mask and random_opp and not random_swich:
        # Leave the token unchanged and set label to the same token
        token_ = token
        mask_label = token

    # Case 3: If mask is True, but random_opp is False
    else:
        # Replace the token with '[MASK]' and set label to the original token
        token_ = '[MASK]'
        mask_label = token

    return token_, mask_label
```

## MLM preparations

The prepare for MLM function is used to prepare tokenized text for BERT's MLM training.
The function takes a list of tokens as input and initializes lists to store the process sentences, labels for each token, and raw tokens if needed.
It applies BERT's MLM masking strategy to each token, replacing it with a mask token and assigning a label indicating whether the token was masked or left unchanged.
The function checks if the token is a sentence delimiter.
If a valid sentence is found with more than two tokens, it adds the current sentence and its labels to the respective lists.
If raw tokens are included, the original tokens are stored as well.
The function continues this process until all tokens are processed.
Any remaining tokens are added as a sentence if present.
Finally, the function returns the prepared lists for BERT's MLM training.
```python
def prepare_for_mlm(tokens, include_raw_tokens=False):
    """
    Prepares tokenized text for BERT's Masked Language Model (MLM) training.

    """
    bert_input = []  # List to store sentences processed for BERT's MLM
    bert_label = []  # List to store labels for each token (mask, random, or unchanged)
    raw_tokens_list = []  # List to store raw tokens if needed
    current_bert_input = []
    current_bert_label = []
    current_raw_tokens = []

    for token in tokens:
        # Apply BERT's MLM masking strategy to the token
        masked_token, mask_label = Masking(token)

        # Append the processed token and its label to the current sentence and label list
        current_bert_input.append(masked_token)
        current_bert_label.append(mask_label)

        # If raw tokens are to be included, append the original token to the current raw tokens list
        if include_raw_tokens:
            current_raw_tokens.append(token)

        # Check if the token is a sentence delimiter (., ?, !)
        if token in ['.', '?', '!']:
            # If current sentence has more than two tokens, consider it a valid sentence
            if len(current_bert_input) > 2:
                bert_input.append(current_bert_input)
                bert_label.append(current_bert_label)
                # If including raw tokens, add the current list of raw tokens to the raw tokens list
                if include_raw_tokens:
                    raw_tokens_list.append(current_raw_tokens)

                # Reset the lists for the next sentence
                current_bert_input = []
                current_bert_label = []
                current_raw_tokens = []
            else:
                # If the current sentence is too short, discard it and reset lists
                current_bert_input = []
                current_bert_label = []
                current_raw_tokens = []

    # Add any remaining tokens as a sentence if there are any
    if current_bert_input:
        bert_input.append(current_bert_input)
        bert_label.append(current_bert_label)
        if include_raw_tokens:
            raw_tokens_list.append(current_raw_tokens)

    # Return the prepared lists for BERT's MLM training
    return (bert_input, bert_label, raw_tokens_list) if include_raw_tokens else (bert_input, bert_label)
```

Let's check a sample MLM output.
Each token in a sentence is labeled depending on the masking operation that's applied to that token.
In this example, the first the is masked.
Therefore, BERT input is mask and its BERT label is the.
Tokens, sun, sets, behind, and the last the are not changed, so their corresponding labels are pad.
Distant is masked and replaced with a random token.
Therefore, BERT input is mask and its BERT label is human scaled.
Finally, mountains and period are unchanged, so their corresponding labels are pad.
![[Pasted image 20250321000735.png|500]]

## NSP preparations
The process for NSP function is used to prepare data for the next sentence prediction, or NSP, task in BERT training.
This function takes a list of tokenized sentences and a corresponding list of masked labels as inputs.
It verifies that both lists have the same length and enough sentences.
The function initializes lists to store the sentence pairs for BERT input.
The masked labels for the sentence pairs and binary labels indicating whether the sentences are consecutive or not.
Using a while loop, the function generates sentence pairs until there are at least two available indices remaining.
It randomly selects whether to create a next sentence scenario or a not next sentence scenario.

For the next sentence scenario, it selects two consecutive sentences, appends them with CLS and SEP tokens, and assigns the corresponding labels and a 1 as the is next label.
For the not next sentence scenario, it selects two random distinct sentences, appends them with CLS and SEP tokens and assigns the corresponding labels and a 0 as the is next label.
After processing each pair, the used indices are removed from the available indices list.

Finally, the function returns the prepared lists for BERT's NSP task, the sentence pairs, the masked labels, and the is next labels.
```python
def process_for_nsp(input_sentences, input_masked_labels):
    """
    Prepares data for Next Sentence Prediction (NSP) task in BERT training.

    Args:
    input_sentences (list): List of tokenized sentences.
    input_masked_labels (list): Corresponding list of masked labels for the sentences.

    Returns:
    bert_input (list): List of sentence pairs for BERT input.
    bert_label (list): List of masked labels for the sentence pairs.
    is_next (list): Binary label list where 1 indicates 'next sentence' and 0 indicates 'not next sentence'.
    """
    if len(input_sentences) < 2:
       raise ValueError("must have two same number of items.")


    # Verify that both input lists are of the same length and have a sufficient number of sentences
    if len(input_sentences) != len(input_masked_labels):
        raise ValueError("Both lists must have the same number of items.")

    bert_input = []
    bert_label = []
    is_next = []

    available_indices = list(range(len(input_sentences)))

    while len(available_indices) >= 2:
        if random.random() < 0.5:
            # Choose two consecutive sentences to simulate the 'next sentence' scenario
            index = random.choice(available_indices[:-1])  # Exclude the last index
            # append list and add  '[CLS]' and  '[SEP]' tokens
            bert_input.append([['[CLS]']+input_sentences[index]+ ['[SEP]'],input_sentences[index + 1]+ ['[SEP]']])
            bert_label.append([['[PAD]']+input_masked_labels[index]+['[PAD]'], input_masked_labels[index + 1]+ ['[PAD]']])
            is_next.append(1)  # Label 1 indicates these sentences are consecutive

            # Remove the used indices
            available_indices.remove(index)
            if index + 1 in available_indices:
                available_indices.remove(index + 1)
        else:
            # Choose two random distinct sentences to simulate the 'not next sentence' scenario
            indices = random.sample(available_indices, 2)
            bert_input.append([['[CLS]']+input_sentences[indices[0]]+['[SEP]'],input_sentences[indices[1]]+ ['[SEP]']])
            bert_label.append([['[PAD]']+input_masked_labels[indices[0]]+['[PAD]'], input_masked_labels[indices[1]]+['[PAD]']])
            is_next.append(0)  # Label 0 indicates these sentences are not consecutive

            # Remove the used indices
            available_indices.remove(indices[0])
            available_indices.remove(indices[1])

    return bert_input, bert_label, is_next
```

Let's check a sample output.
Special symbols, CLS and SEP, are first added to the input sentences.
BERT label is created using the prepare for MLM function.
In the first example, the second sentence follows the first sentence.
Therefore, is next is 1.
In the second example, the second sentence does not follow the first sentence, so is next is 0.
![[Pasted image 20250321001132.png|500]]

## Creating training-ready inputs for BERT
The prepare BERT final inputs function is used to prepare the final input lists for BERT training.

It takes the BERT inputs, BERT labels, and is next lists as inputs, and pads are reformatted, reshaped, and converted into a tensor.

Inside the function, there is a nested function called `zero_pad_list_pair`, that is used to zero-pad the input pairs with pad tokens to ensure they have the same length.
This is important for maintaining consistent input shapes during training.

The function also includes Lambda functions, flatten and tokens to index for flattening nested lists and transforming tokens to their corresponding vocabulary indices respectively.

The function iterates over each input pair, creates segment labels for each pair of sentences, and apply zero padding to the input pairs and segment labels using the zero-pad list pair function.
In BERT, segment embeddings are used to differentiate between the two input sentences.
This is necessary because BERT's input consists of a pair of sentences for NSP or question answering type tasks, and the model needs to understand which tokens belong to which sentence.

If the to tensor flag is set to true, the function flattens the padded inputs and labels, transforms the tokens to indices, and converts them to PyTorch sensors.
Otherwise, if to tensor is false, the flattened padded inputs and labels, segment labels and is next values are appended directly to the final output lists.
```python
def prepare_bert_final_inputs(bert_inputs, bert_labels, is_nexts,to_tenor=True):
    """
    Prepare the final input lists for BERT training.
    """
    def zero_pad_list_pair(pair_, pad='[PAD]'):
        pair=deepcopy(pair_)
        max_len = max(len(pair[0]), len(pair[1]))
        #append [PAD] to each sentence in the pair till the maximum length reaches
        pair[0].extend([pad] * (max_len - len(pair[0])))
        pair[1].extend([pad] * (max_len - len(pair[1])))
        return pair[0], pair[1]

    #flatten the tensor
    flatten = lambda l: [item for sublist in l for item in sublist]
    #transform tokens to vocab indices
    tokens_to_index=lambda tokens: [vocab[token] for token in tokens]

    bert_inputs_final, bert_labels_final, segment_labels_final, is_nexts_final = [], [], [], []

    for bert_input, bert_label,is_next in zip(bert_inputs, bert_labels,is_nexts):
        # Create segment labels for each pair of sentences
        segment_label = [[1] * len(bert_input[0]), [2] * len(bert_input[1])]

        # Zero-pad the bert_input and bert_label and segment_label
        bert_input_padded = zero_pad_list_pair(bert_input)
        bert_label_padded = zero_pad_list_pair(bert_label)
        segment_label_padded = zero_pad_list_pair(segment_label,pad=0)

        #convert to tensors
        if to_tenor:

            # Flatten the padded inputs and labels, transform tokens to their corresponding vocab indices, and convert them to tensors
            bert_inputs_final.append(torch.tensor(tokens_to_index(flatten(bert_input_padded)),dtype=torch.int64))
            #bert_labels_final.append(torch.tensor(tokens_to_index(flatten(bert_label_padded)),dtype=torch.int64))
            bert_labels_final.append(torch.tensor(tokens_to_index(flatten(bert_label_padded)),dtype=torch.int64))
            segment_labels_final.append(torch.tensor(flatten(segment_label_padded),dtype=torch.int64))
            is_nexts_final.append(is_next)

        else:
          # Flatten the padded inputs and labels
            bert_inputs_final.append(flatten(bert_input_padded))
            bert_labels_final.append(flatten(bert_label_padded))
            segment_labels_final.append(flatten(segment_label_padded))
            is_nexts_final.append(is_next)

    return bert_inputs_final, bert_labels_final, segment_labels_final, is_nexts_final

```

Let's check a sample output.
Sentences are zero-padded, and each token is mapped to its vocab index; CLS to one, he to 33, SEP to two, pad to zero.
Mask labels are also padded and mapped to vocab indices.
In this case, all tokens are unchanged except the token he, which is masked.
Next, segment labels are created, where tokens of the first sentence are labeled with 1, tokens of the second sentence are labeled with 2, and zero paddings are labeled with 0.
Finally, is next is returned to indicate that the second sentence does not follow the first one.
![[Pasted image 20250321001608.png|500]]

## Creating training-ready CSV file from IMDB
This code writes the processed Internet Movie Database, or IMDB, data to a CSV file.
```python
csv_file_path ='train_bert_data_new.csv'
with open(csv_file_path, mode='w', newline='', encoding='utf-8') as file:
    csv_writer = csv.writer(file)
    csv_writer.writerow(['Original Text', 'BERT Input', 'BERT Label', 'Segment Label', 'Is Next'])

    # Wrap train_iter with tqdm for a progress bar
    for n, (_, sample) in enumerate(tqdm(train_iter, desc="Processing samples")):
        # Tokenize the sample input
        tokens = tokenizer(sample)
        # Create MLM inputs and labels
        bert_input, bert_label = prepare_for_mlm(tokens, include_raw_tokens=False)
        if len(bert_input) < 2:
            continue
        # Create NSP pairs, token labels, and is_next label
        bert_inputs, bert_labels, is_nexts = process_for_nsp(bert_input, bert_label)
        # add zero-paddings, map tokens to vocab indices and create segment labels
        bert_inputs, bert_labels, segment_labels, is_nexts = prepare_bert_final_inputs(bert_inputs, bert_labels, is_nexts)
        # convert tensors to lists, convert lists to JSON-formatted strings
        for bert_input, bert_label, segment_label, is_next in zip(bert_inputs, bert_labels, segment_labels, is_nexts):
            bert_input_str = json.dumps(bert_input.tolist())
            bert_label_str = json.dumps(bert_label.tolist())
            segment_label_str = ','.join(map(str, segment_label.tolist()))
            # Write the data to a CSV file row-by-row
            csv_writer.writerow([sample, bert_input_str, bert_label_str, segment_label_str, is_next])
```

The output will be as shown here.
![[Pasted image 20250321001823.png|500]]
Let's check an example of the outputs of data preparations.
Given a corpus, pairs of sentences are created as input.
Input is tokenized and numericalized.
Special tokens, CLS and SEP, are added.
Then it'll be zero-padded.
Next, the masking strategy is applied.
According to the masking strategy, BERT labels are created where every token has a label zero, except for mask tokens.
Segment labels are also created that show to which sentence each token belongs.
Finally, the is next label shows whether the second sentence follows the first one.
![[Pasted image 20250321001916.png|500]]
# Recap
- For data preparation
	- Initialize a tokenizer using `get_tokenizer` function
	- define special symbols with their corresponding indices.
- The `masking` function
	- perform masking operations on tokens
	- takes a token as input
	- decides whether to mask it.
- `prepare_for_mlm` function
	- takes a list of tokens as input
	- initializes lists to store the process sentences, labels for each token, and raw tokens, if needed.
	- It applies BERT's MLM masking strategy to each token, replacing it with a mask token and assigning a label indicating whether the token was masked or left unchanged.
- `process_for_nsp` function
	- takes a list of tokenized sentences and a corresponding list of masked labels as inputs.
	- It verifies that both lists have the same length and enough sentences.
	- It initializes lists to store the sentence pairs for BERT input, the masked labels for the sentence pairs and binary labels indicating whether the sentences are consecutive or not.
- `prepare_bert_final_inputs` function
	- prepare the final input lists for BERT training.
	- It takes the `bert_inputs`, `bert_labels`, and `is_nexts` lists as inputs.