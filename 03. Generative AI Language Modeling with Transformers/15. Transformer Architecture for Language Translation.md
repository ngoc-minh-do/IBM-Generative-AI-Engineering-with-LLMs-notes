# Transformer Architecture for Language Translation

## Transformer
Historically, language translation models like RNNs and LSTMs processed text sequentially word by word.
This method was slow and ineffective for long text sequences, as retaining context over large spans of text proved challenging.
Transformers have revolutionized language translation by processing entire text sequences simultaneously.
This approach greatly accelerates translation tasks and enhances the handling of extended text sequences, enabling a deeper and more coherent understanding of context.
While transformers boast several other advantages, these are particularly pivotal in improving language translation outcomes.
![[Pasted image 20250321230800.png|500]]

## Transformers for translation
Let's explore the architecture of a sequence to sequence transformer model widely used in machine translation.
Initially, the source text is tokenized.
Each token is mapped to its corresponding index in the vocabulary and converted into word embeddings.
Positional encodings are added with these embeddings to maintain the word's order.
The encoder processes the embeddings, outputting contextual embeddings, referred to as memory, which encapsulates all the information needed to translate the phrase.
In the decoding phase, translation unfolds one word at a time, starting with the beginning of sentence `<BOS>` token.
Every newly generated token is then embedded and positionally encoded, leveraging the memory from the encoder.
The decoder predicts the next token, in this case `I`, which then becomes the input for subsequent decoding steps.
It's then converted into word embeddings and positionally encoded using the memory.
The next token is generated, in this case `go`, and is passed to the decoder.
This recursive process is repeated until the full sequence is translated, ensuring a translation that is both coherent and contextually precise.
![[Pasted image 20250321231122.png|400]]

## Encoder
Let's delve into the encoder's inner workings and trace the data flow within it.
A source sentence enters the embedding layer, where each token is converted into a d dimensional embedding vector.
Positional encoding is then applied.
Following this, the vectors pass through multi-head attention, allowing the model to focus on different parts of the sentence simultaneously and then through a normalization layer.
Subsequently, a feedforward network processes the vectors to produce outputs of the same dimensionality, D.
A final normalization layer completes the encoder's operations.
The resulting memory is a tensor with dimensions equal to the sequence length times the embedding size D.
![[Pasted image 20250321231418.png|300]]

## Decoder
The decoder architecture mirrors the decoder only model with a few key distinctions.
One significant variation is the addition of a cross attention layer, which hones in on the encoder's memory output.
This crucial layer enables the decoder to reference the full context of the input sequence.
Masking is another vital component of the decoder's mechanism.
It ensures the model sequentially predicts each word, considering only the preceding tokens in the target sequence, which is essential for the autoregressive generation process.
The transformation of contextual embeddings into logits is performed by a linear layer mapping the vector to the vocabulary's dimensionality.
These logits score all possible words, guiding the model to predict the subsequent word.
For instance, the process begins with the `BOS` token.
The decoder then utilizes the memory to produce the token, `I`.
This token is fed back into the decoder combined with the memory to generate the next word `go`.
This procedure repeats until the `EOS` token signals the end of the sequence or a predetermined length is achieved.
![[Pasted image 20250321231613.png|400]]

## Cross attention
The decoder uses cross attention to attend to the encoder's hidden representations.
It computes attention scores between each target position in the decoder and all source positions in the encoder.
These attention scores reflect the relevance or importance of each source position to the current target position.
By incorporating cross attention, the decoder can attend to different parts of the input sequence based on their relevance to the current decoding step.
This mechanism helps the model capture long range dependencies and align the input and output sequences effectively, which is crucial for accurate translation.
![[Pasted image 20250321232246.png|500]]
## Recap
- unlike RNNs, transformers process the entire text sequences simultaneously for language translation.
- In an encoder
	- a source sentence enters the embedding layer and then positional encoding is applied.
	- Following this, the vectors pass through the multi head attention and normalization layer.
	- Subsequently, a feedforward network processes the vectors and a final normalization layer completes the encoder's operations.
- In a decoder
	- The cross attention layer enables the decoder to reference the full context of the input sequence.
	- Masking ensures the model sequentially predicts each word, considering only the preceding tokens in the target sequence.
	- The linear layer transforms the contextual embeddings into logits, mapping the vector to the vocabulary's dimensionality.
- Cross attention computes attention scores between each target position in the decoder and all source positions in the encoder.