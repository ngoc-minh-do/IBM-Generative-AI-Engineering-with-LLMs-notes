- Generative Al architectures enable machines to comprehend human language and generate responses like human
- Generative AI start with rule-based system and then shifted to machine learning approaches
	A significant leap happened with deep learning
- Transformer represent the latest in this evolution
- Significant advancements have been made in machine translation, chat bot conversation, sentiment analysis and text summarization
- LLM models аrе foundation models that use AI and deep learning with vast data sets. They are called large language models due to the size of the training data set.
- Example of LLM are:
	- GPT: 
		- Acts as a decoder
		- Adept at generating text 
		- Used in chatbots
	- BERT (Bidirectional Encoder Representations from Transformers)
		- Utilizes encoder-only transformer architecture
		- Understands the context of a word in a sentence
		- Used in sentiment analysis and question answering
	- BART (Bi-directional and Auto-Regressive Transformers) and T5 (Text to Text Transfer Transformer)
		- uses encoder - decoder architecture
		- Versatile (linh hoạt) for various NLP tasks