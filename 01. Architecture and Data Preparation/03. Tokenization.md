# Tokenization

is a process to breaking a sentence into smaller pieces. The token help the model understand the text better.

## Tokenization methods

1. Word-based
2. Character-based
3. Subword-based

### Word-based tokenization

- Divides text into individual words
- Advantages: Preserves semantic meaning
- Disadvantage: Increases the model's overall vocabulary
```python
import nltk

nltk.download("punkt")
from nltk.tokenize import word_tokenize

text = "Unicorns are real. I saw a unicorn yesterday. I couldn't see it today."
token = word_tokenize(text)

# Output
# ['Unicorns', 'are', 'real', '.', 'I', 'saw', 'a', 'unicorn', 'yesterday', '.', 'I', 'couldn't', 'see', 'it', 'today', '.']
```
### Character-base tokenization

- Splits text into individual characters
- Advantages: Smaller vocabularies
- Disadvantage: 
	- May not convey the same information as entire words
	- Increases input dimensionality and computational needs
```python
# Input text: `This is a sentence.`
# Output text: `['T', 'h', 'i', 's', 'i', 's', 'a', 's', 'e', 'n', 't', 'e', 'n', 'c', 'e', '.']`
```
### Subword-based tokenization

- Frequently used words unsplit, infrequent words broken down
- Advantages: Combines the advantages of word-base and character-based tokenization
- Algorithms:
	- WordPiece
	- Unigram
	- SentencePiece
```python
# Input text: `The power of AI is unbelievable.`
# Output text: `['The', 'power' ,'of', 'AI', 'is', 'un', 'believe' 'able']`
```
#### WordPiece algorithm
- Evaluates the benefits and drawback of splitting and merging two symbols
```python
from transformers import BertTokenizer

tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
tokenizer.tokenize("IBM taught me tokenization.")

# Output
# ['ibm', 'taught', 'me', 'token', '##ization', '.']
# `##` indicate the word should be attach to the previous word without a space
```
#### Unigram: 
- Breaks text into smaller pieces
- Narrows down a large list of possibilities based on the frequency of appearance

#### SentencePiece
- Segments text into manageable parts and assigns unique IDs

	Unigram & SentencePiece example:
```python
from transformers import XLNetTokenizer

tokenizer = XLNetTokenizer.from_pretrained("xlnet-base-cased")
tokenizer.tokenize("IBM taught me tokenization.")

# Output
# ['_Ibm', '_taught', '_me', '_token', 'ization', '.']
# `_` indicate they are new words preceded by a space in original text
```

## Tokenization and indexing in Pytorch

- Use `torchtext` library for tokenization text from a dataset into individual words or sub words
- Use the `build_vocab_from_iterator` function
	- Creates a vocabulary from tokens
	- Assigns each token a unique index

Tokenization
```python
dataset = [
    (1,"Introduction to NLP"),
    (2,"Basics of PyTorch"),
    (1,"NLP Techniques for Text Classification"),
    (3,"Named Entity Recognition with PyTorch"),
    (3,"Sentiment Analysis using PyTorch"),
    (3,"Machine Translation with PyTorch"),
    (1," NLP Named Entity,Sentiment Analysis,Machine Translation "),
    (1," Machine Translation with NLP "),
    (1," Named Entity vs Sentiment Analysis  NLP ")]

from torchtext.data.utils import get_tokenizer
tokenizer = get_tokenizer("basic_english")
tokenizer(dataset[0][1])

# Output
# ['introduction', 'to', 'nlp']
```

Token indices
```python
def yield_tokens(data_iter):
    for  _,text in data_iter:
        yield tokenizer(text)
my_iterator = yield_tokens(dataset) 

vocab = build_vocab_from_iterator(yield_tokens(dataset), specials=["<unk>"])
vocab.set_default_index(vocab["<unk>"])
vocab.get_stoi() # Dictionary mapping tokens to indices.
# {
# 'to': 19,
# ...
# }

vocab(['introduction', 'to', 'nlp'])
# [14, 19, 1]
```

Creating tokens and indices
```python
def get_tokenized_sentence_and_indices(iterator):
    tokenized_sentence = next(iterator)  # Get the next tokenized sentence
    token_indices = [vocab[token] for token in tokenized_sentence]  # Get token indices
    return tokenized_sentence, token_indices

tokenized_sentence, token_indices = get_tokenized_sentence_and_indices(my_iterator)
next(my_iterator)

print("Tokenized Sentence:", tokenized_sentence)
print("Token Indices:", token_indices)

# Tokenized Sentence: ['introduction', 'to', 'nlp']
# Token Indices: [14, 19, 1]
# ...
```

You can add special tokens such as `<bos>` at the beginning and `<eos>` at the end of a tokenized sentence