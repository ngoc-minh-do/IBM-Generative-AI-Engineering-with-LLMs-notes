# Reinforcement Learning from Human Feedback (RLHF)

## Learning objective
- Calculate rewards using a reward function.
- Demonstrate how the model incorporates human feedback in fine tuning the language models.

## Reinforcement Learning from Human Feedback
Imagine a large number of monkeys randomly pressing keys on typewriters for an infinite amount of time.
Eventually, they will produce any given texts, such as the complete works of Shakespeare.
But what if you incorporate a reward for them such as a banana? 
Would you need fewer monkeys or less time? 
![[Pasted image 20250403231750.png|400]]

Let's understand this with reinforcement learning from human feedback or RLHF using a monkey example.
First, a monkey begins typing to generate a relevant word.
Once a monkey types a relevant word, the document is submitted for review and undergoes a review process.
If the document is satisfactory, the monkey receives a banana as a reward.
The monkey will repeat this action but eventually type random words.
A subset of these random words may also earn a reward and the process is repeated.
Eventually, the monkey will create Shakespeare.
![[Pasted image 20250403231843.png|400]]

## Reward calculation
Let's understand reward calculation using RLHF.
To do so, use a reward function represented as r(X, Y).
Next, insert the query `which country owns Antarctica?`
The reward function provides human feedback for the inserted query.
The first response is `?9dfsa`.
![[Pasted image 20250403231944.png|400]]

This response is irrelevant and gets a zero score.
Next repeat the process for different responses.
The received response is `no country owns Antarctica`.
This response is accurate and gets a score of 0.9.
![[Pasted image 20250403232011.png|400]]

Finally, the ideal response is `Antarctica is governed by an international treaty`.
This response is the most accurate and gets a satisfactory score of 1.

## Rollouts
Similarly, you should have various queries and responses to train a sample.
For reviewing the sampling process, focus on rollouts of queries and responses.
To do so, consider a table of queries displayed on a screen.
The first column, an index n represents the query number.
The second column represents queries
![[Pasted image 20250403232054.png|300]]

The equation x approximate d represents a random use of the sample from this table.
For example, select query number 1 as the random sample.
`The largest ocean is ?`.
Next represent the language model as a table.
The first row represents a query which is `the largest ocean is ?`, where n equals one, and each row below shows the possible response including `the Pacific Ocean`, `the Atlantic Ocean`, and so on.
Y approximate D1 indicates that Y is randomly sampled from the table with k as the index of each response.
![[Pasted image 20250403232130.png|300]]

Let's take another query.
`Can you give me some python code` where n two? You can see a different table for this query where each row has a different response indexed by k and Y sampled from D2.
![[Pasted image 20250403232323.png|300]]

You can repeat this process for each query question, is it a kids book, which country owns Antarctica, and so on.
This process is known as rollout.

## Expected reward
Now let's understand the concept of expected reward using a table before a language model is used.
For this, look at the empirical formula, it approximates the expected reward by averaging the rewards over multiple queries and their respective responses.
In this formula:
- capital n represents the total number of queries
- small n represents an individual query
- capital k denotes the number of responses per query 
- k represents an individual response.
![[Pasted image 20250403232432.png|300]]

Transforming the empirical formula will help to determine the actual expected value formula.
This actual value formula represents the expectation over the data distribution and the model's response distribution for the given response.
![[Pasted image 20250403232547.png|400]]

Using the expected reward for a single query by incorporating the probability of each response for the given input.
Thus, the expected reward is summarized as p( Y by X) the probability of response Y given input X.
![[Pasted image 20250403232637.png|400]]

## Reinforcement Learning from Human Feedback
Let's learn how to incorporate human feedback.
For example, take a pretrained large language model or LLM that needs to be fine tuned.
This model is represented as a policy showing the model's response distribution provided as an input query, a box labeled model.
Next introduce a query, `who made this course` represented as a blue box.
You can see a response `he looks like Brad Pitt` represented in a blue box and the input to the reward model will be query response.
![[Pasted image 20250403232827.png|400]]
![[Pasted image 20250403233004.png|200]]

To evaluate and generate a reward for query plus response, let's use a pre-trained reward model.
The input query is `who made this course` and the related response is `he looks like Brad Pitt`.
Based on the query and response, the reward model generates a reward value of -10,000.
![[Pasted image 20250403233043.png|400]]

Now let's see how this is useful in fine tuning the policy.
First, create an agent or LLM represented as green with a set of learnable parameters theta and the reward model represented in orange.
Next introduce a query X as an input to the agent to generate a response Y.
Here query and response are known as rollout.
The reward function processes x and y and uses the reward for training the LLM and updating theta.
![[Pasted image 20250403233142.png|500]]

This example shows a set of rollouts for the given query.
Which country owns Antarctica? The query is introduced as the input to the agent.
You can see this query on the left side of the reward model indicating that the agent is processing the query.
Here, agent generates several responses followed by rewards for each query.
This reward updates the model parameters theta for each response.
For example, the first response is `?9dfsa` with a reward of 0.
The second response is, `Antarctica is`, with a reward of 0.0021.
![[Pasted image 20250403233255.png|500]]

The third response is `Penguin overlords` with a reward of 0.09.
The fourth response is `Antarctica is a country` with a reward of 0.02.
The fifth response is `no country owns Antarctica` with a reward of 0.09 and the final response `Antarctica is governed by an international treaty` with a reward of 1.
These generated responses are random from the policy distribution.
The rewards help in optimizing the learnable parameters of policy distribution.
![[Pasted image 20250403233517.png|500]]
## Recap
- The reward function provides human feedback for the inserted query to receive relevant responses and provide scores.
- Use the rollouts of queries and responses to review the sampling process.
- The expected rewards help understand how an agent performs in the language model using an empirical formula and averaging the rewards over multiple queries and responses.
- RLHF uses response distribution as an input query to fine tune the pretrained LLMs.
- You can use a pre-trained reward model to evaluate and generate a reward for the query plus response.
- You can update the model parameters theta for each response generated by the agents.
