# Reward Modeling with Hugging Face

## Learning objective
- Describe the dataset from the Hugging Face, explain how to pre-process the dataset and reward modeling 
- Apply the low-rank adaptation or LoRA configuration in reward modeling.
- Describe the reward trainer, explain reward loss 
- Demonstrate the model evaluation in reward modeling.

## Dataset
Let's begin by understanding the dataset.
Consider a dataset Dahoas/synthetic- instruction-gptj-pairwise from Hugging Face usually designed for training and evaluating instruction following models.
This dataset trains models to understand and follow robust instructions by learning from pairs of good and bad responses.
```python
# Load the Dahoas/synthetic-instruct-gptj-pairwise dataset 
dataset = load_dataset("Dahoas/synthetic-instruct-gptj-pairwise")
# Display the dataset
print(dataset)
# DatasetDict({
#    train: Dataset({
#        features: ['prompt', 'chosen', 'rejected'],
#        num_rows: 33143
#    })
# })
```

Here is a sample data point from the dataset.
You can see that every data point consists of a prompt, a text prompt that the model should respond to, the preferred response to the prompt, called chosen, and the disliked response to the prompt, called rejected.
![[Pasted image 20250331230911.png|500]]

## Model and tokenizer setup
Next, for the training process to define the score function, set-up the tokenizer and the model.
![[Pasted image 20250331232721.png|200]]

For example, train the GPT-2 model for sequence classification as a score function.
This model helps to determine the quality of responses.
The model output is a single class or one neuron at the output layer, which produces a scalar value output representing the score.
```python
from transformers import GPT2Tokenizer, GPT2ForSequenceClassification

# Define the model name or path
model_name_or_path = "gpt2"

# Initialize tokenizer and model
tokenizer = GPT2Tokenizer.from_pretrained(model_name_or_path, use_fast=True)
model = GPT2ForSequenceClassification.from_pretrained(model_name_or_path, num_labels=1)

# Add special tokens if necessary
tokenizer.pad_token = tokenizer.eos_token
model.config.pad_token_id = model.config.eos_token_id

# Define the maximum length
max_length = 1024
```

## Preprocessing
The get response function prepares data by structuring it as a query and response pair, making it easy to read and test.
```python
get_res=lambda dataset,res:[  "\n\nHuman: "+prompt + "\n\nAssistant: "+resp for prompt, resp in zip(dataset["train"]["prompt"], dataset["train"][res])]
```

Next, apply this function to the data set.
```python
chosen_samples=get_res( dataset,'chosen')
rejected_samples=get_res( dataset,'rejected')
print('chosen',chosen_samples[0])
print('rejected',rejected_samples[0])
```

The screen displays the data set after applying the get response function.
![[Pasted image 20250331233145.png|500]]

Defining a function `add_combined_columns` takes a single data point as an example and adds two new columns, `prompt_chosen` and `prompt_rejected`.
The prompt chosen column combines the prompt with the chosen response, labeling the dialogue parts human and assistant.
However, prompt rejected column combines the prompt with the rejected response and the same labeled format.
```python
# Define a function to combine 'prompt' with 'chosen' and 'rejected' responses
def add_combined_columns(example):
    # Combine 'prompt' with 'chosen' response, formatting it with "Human:" and "Assistant:" labels
    example['prompt_chosen'] = "\n\nHuman: " + example["prompt"] + "\n\nAssistant: " + example["chosen"]
    
    # Combine 'prompt' with 'rejected' response, formatting it with "Human:" and "Assistant:" labels
    example['prompt_rejected'] = "\n\nHuman: " + example["prompt"] + "\n\nAssistant: " + example["rejected"]
    
    # Return the modified example
    return example
```

Next, the Matt method applies the function to each example in the training.
```python
# Apply the function to each example in the 'train' split of the dataset
dataset['train'] = dataset['train'].map(add_combined_columns)
dataset['train']
```

The screen displays the resultant data.
![[Pasted image 20250331233537.png|500]]

Next, filter the samples shorter than the specified max length to ensure that the dataset meets the required length criteria.
```python
get_max_len= lambda samples: max([len(sample) for sample in samples])

find_short = lambda dataset, max_length: [
    i for i, (chosen, rejected) in enumerate(zip(dataset['prompt_chosen'], dataset['prompt_rejected']))
    if len(chosen) < max_length or len(rejected) < max_length
]

max_length=1024
subset_indices=find_short (dataset['train'], max_length)
dataset['train'] = dataset['train'].select(subset_indices)
subset_indices[0:10]
# [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
```

Let's define the pre-process function and call this function a single data point to observe its newly formatted keys.

The pre-process function tokenizes the prompt chosen and prompt projected keys for the reward trainer.
The chosen key represents the preferred responses.
However, the rejected key represents the disliked responses.
Tokenizing these keys enables the model to process and understand the differences between high and low-quality responses.
By providing chosen and rejected inputs, the reward trainer can differentiate and prioritize better responses which helps in training models to follow instructions.

- The field input IDs chosen and input IDs rejected represent the token IDs for the chosen and rejected responses.
	These IDs are the numerical representation of the text used by the model.
- The attention masks chosen and attention mask rejected fields also contain attention masks for the chosen and rejected responses.

Upon calling the function for a single data point of the dataset, you will get the return keys from the function.
```python
# Define a preprocessing function to tokenize the 'prompt_chosen' and 'prompt_rejected' keys
def preprocess_function(examples):
    # Tokenize the 'prompt_chosen' text with truncation and padding to the maximum length
    tokenized_chosen = tokenizer(examples['prompt_chosen'], truncation=True, max_length=max_length, padding="max_length")
    
    # Tokenize the 'prompt_rejected' text with truncation and padding to the maximum length
    tokenized_rejected = tokenizer(examples['prompt_rejected'], truncation=True, max_length=max_length, padding="max_length")
    
    # Return the tokenized inputs as a dictionary
    return {
        "input_ids_chosen": tokenized_chosen["input_ids"],  # Token IDs for 'chosen' responses
        "attention_mask_chosen": tokenized_chosen["attention_mask"],  # Attention masks for 'chosen' responses
        "input_ids_rejected": tokenized_rejected["input_ids"],  # Token IDs for 'rejected' responses
        "attention_mask_rejected": tokenized_rejected["attention_mask"],  # Attention masks for 'rejected' responses
    }

example=preprocess_function(dataset['train'][0])
example.keys()
# dict_keys(['input_ids_chosen', 'attention_mask_chosen', 'input_ids_rejected', 'attention_mask_rejected'])
```

At this stage, the data point contains the text tokens for every label.
![[Pasted image 20250331234153.png|400]]

In the training dataset, apply the pre-process function to each sample using the map method and tokenize the prompt chosen and prompt rejected texts.
Considering the batched equals true parameter enables the function to process multiple samples and improves efficiency.
It is important to note that this is not part of the training process.
However, it just explains the workings of the scoring function.
```python
train_str={'chosen': [sample for sample in dataset['train'] ['prompt_chosen']], 'rejected':[sample for sample in dataset['train'] ['prompt_rejected']]}

dataset['train'] = dataset['train'].map(preprocess_function, batched=True, remove_columns=['prompt',"chosen", "rejected",'prompt_chosen', 'prompt_rejected'])
```
![[Pasted image 20250331234813.png|500]]

Finally, let's split the dataset into training and testing.
```python
split_dataset = dataset['train'].train_test_split(test_size=0.2)

# Create a DatasetDict to hold train and test splits
dataset_dict = DatasetDict({
    'train': split_dataset['train'],
    'test': split_dataset['test'],
})
```
![[Pasted image 20250331234932.png|500]]

## LoRA configuration
You can use LoRA to train the model by initiating the LoRA config configuration for a sequence classification task.
This configuration specifies several parameters and is created using the lower config class from the parameter-efficient fine-tuning or PET library.
```python
peft_config = LoraConfig(
    task_type=TaskType.SEQ_CLS,
    inference_mode=False,
    r=8,
    lora_alpha=32,
    lora_dropout=0.1,
    target_modules=["attn.c_attn", "attn.c_proj"]  # Target attention layers
)
```

## Training
Let's use the training arguments class from the transformers library to define the training arguments to configure various parameters for the training process.
- The per-device train batch size equals 3, which sets the batch size per device, GPU, or CPU to three.
	However, you can try a larger batch depending on the size of GPU.
- The num train epochs equals 3, which specifies the number of training epochs to 3.
- Next, the gradient accumulation steps equals 8, accumulates gradients over eight steps before performing a backward or update pass.
	It also increases the batch size.
- Finally, the learning rate equals 1.41*e-5 sets the learning rate for the optimizer to 1.41*e-5.
```python
# Define training arguments

training_args = TrainingArguments(
    per_device_train_batch_size=3,  # Set to 3
    num_train_epochs=3,  # Set to 3
    gradient_accumulation_steps=8,
    learning_rate=1.41e-5,
    output_dir="./model_output3",
    logging_steps=10,
    eval_strategy="steps",
    eval_steps=500,
    save_steps=500,
    save_total_limit=2,
)
```

Next is a reward trainer, a specialized trainer designed to train the reward function.
First, train the process using the reward trainer orchestrate.
Then handle tasks, such as batching, optimization, evaluation, and saving model checkpoints.
It trains the model to learn from the feedback signals and improve their ability to generate high-quality responses.
You can modify them using various parameters such as
- model to train the model.
- Args represents the training arguments, typically an instance of training arguments.
- Tokenizer, use it to process the text inputs.
- Train dataset for training the model.
- Eval data set for evaluating the model
- peft_config to configure LoRA.
```python
# Initialize RewardTrainer
trainer = RewardTrainer(
    model=model,
    args=training_args,
    tokenizer=tokenizer,
    train_dataset=dataset_dict['train'],
    eval_dataset=dataset_dict['test'],
    peft_config=peft_config,
)
```

Next, user reward trainer to train, save, and evaluate the model by leveraging the trainer dot train method to initiate the training process.
The model learns from the training dataset, optimizes its parameters to improve performance, and stores the variable metrics using the evaluate method.
```python
# output_dir="./model_output3"

# # Train the model
# trainer.train()

# # Save the model
# trainer.save_model(output_dir)

# # Evaluate the model
# metrics = trainer.evaluate()
# print(metrics)

# model.config.save_pretrained("./backup")
```

Using the given code, let's plot the training loss.
```python
log_file = f"extracted_model/model_output3/checkpoint-2500/trainer_state.json"

# Read the log file
with open(log_file, 'r') as f:
    logs = json.load(f)

# Extract training loss values
steps = []
losses = []
for log in logs["log_history"]:
    if "loss" in log:
        steps.append(log["step"])
        losses.append(log["loss"])

# Plot the training loss
plt.figure(figsize=(10, 5))
plt.plot(steps, losses, label="Training Loss")
plt.xlabel("Steps")
plt.ylabel("Loss")
plt.title("Training Loss Over Time")
plt.legend()
plt.show()
```

The screen displays proper convergence of the loss.
![[Pasted image 20250331235411.png|400]]

## Evaluate the model
Now convert the tokenizing process, generate a score, and compare the outputs in two separate functions.
The first function tokenizes the text and generates the model's output scores.
```python
# Function to make a prediction and get the logits
def predict_and_get_logits(text):
    # Tokenize the input text
    inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True, max_length=512)
    inputs = {k: v.to(device) for k, v in inputs.items()}

    # Perform the forward pass
    with torch.no_grad():
        outputs = model(**inputs)
    
    # Extract the logits from the outputs
    logits = outputs.logits.squeeze().item()  # Assuming binary classification and batch size of 1
    
    return logits
```

However, the second function performs the pairwise comparison of these scores.
```python
# Function to compare two texts
def compare_texts(text1, text2):
    logit1 = predict_and_get_logits(text1)
    logit2 = predict_and_get_logits(text2)

    if logit1 > logit2:
        print("selected---------")
        print(text1, f"score: {logit1}")

        return text1
    else:
        print("selected---------")
        print(text2,  f"score: {logit2}")

        return text2
```

## Calculate win rate
Finally, evaluate the model's performance by calculating the win rate.
For example, if the model assigns a higher score for better response and accuracy, market is 1.
Otherwise, market is zero.
The model evaluation process starts by defining n, the number of samples to be evaluated.
Next, initialize a counter correct selections to track the number of correct identifications of the preferred response.
The code then iterates over the first n pairs of chosen and rejected responses from the training dataset.
```python
# Define the number of samples to evaluate
N = 10

# Initialize a counter for correct selections
correct_selections = 0

# Iterate over the first N pairs of chosen and rejected responses
for chosen, rejected in zip(train_str['chosen'][0:N], train_str['rejected'][0:N]):
    # Print the chosen response for reference
    print("Chosen Response:\n", chosen)
    
    # Use the compare_texts function to determine which response is better
    selected_text = compare_texts(chosen, rejected)
    
    # Check if the selected text is the chosen response
    if selected_text == chosen:
        correct_selections += 1

# Calculate the accuracy as the ratio of correct selections to the total number of samples
accuracy = correct_selections / N

# Print the accuracy
print("Accuracy:", accuracy)    
```

The screen displays the output with 100% win rate.
It's important to note that this data is based on the synthetic dataset.
However, the top models achieve win rates 60-70%.
![[Pasted image 20250331235623.png|500]]

## Recap
- A data set synthetic-instruct-gptj-pairwise from Hugging Face is usually designed for training and evaluating instruction following models.
- Defining the pre-process function helps format the keys and tokenize the data for the reward trainer.
- Training arguments class from the transformers library defines the training arguments to configure various parameters for the training process.
- Reward trainers orchestrate the process, save, and evaluate the model using the trainer.train method.
- The tokenizing process generates scores and compares the output of two functions to achieve the desired win rate.