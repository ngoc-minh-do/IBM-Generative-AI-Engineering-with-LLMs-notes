# DPO: Partition Function

## Learning objective
- Describe the DPO concept and its models
- Use the partition function to convert a simple distribution to a complex one for the DPO objective.

## Direct preference optimization (DPO)
Direct preference optimization, or DPO, is a reinforcement learning technique.
Designed to fine tune models based on human preferences more directly and more efficiently than traditional methods.
![[Pasted image 20250405115254.png|300]]

DPO involves collecting data on human preferences by showing users different outputs from the model and asking them to choose the better one.
![[Pasted image 20250405115234.png|300]]

This preference data is crucial for training.
![[Pasted image 20250405115311.png|500]]

## DPO and traditional methods
Unlike traditional reinforcement learning methods, which might optimize for a reward signal indirectly related to human preferences.
DPO directly optimizes the models' parameters to produce outputs that better align with human choices.
![[Pasted image 20250405115410.png|400]]

## Reward-based and reward-free methods
Novel applications like ChatGPT leverage reward-based methods that first develop a reward model and then use actor critic algorithms such as a proximal policy optimization or PPO.
![[Pasted image 20250405115436.png|400]]

However, in academic benchmarks, state of the art results are often attained through reward-free methods like direct preference optimization, DPO.
This is because reinforcement learning, or RL, is extremely complex and involves training reward models.
![[Pasted image 20250405115727.png|400]]

## Direct preference optimization
DPO involves three models 
- the reward function
- the target decoder
- the reference model.

The reward function uses an encoder model.
![[Pasted image 20250405120218.png|200]]

For example, consider a reward model designed for evaluating the relevance of text on LLMs.
If your input text is `this is a` and the response is `cat`, the reward model would give a low score of 0.1.
The score would be low because cat has nothing to do with LLMs.
Conversely, if the response is `reward function`, which is more relevant, the score would be high, such as 0.99.
This is because the reward function is related to LLMs.
![[Pasted image 20250405120053.png|200]]

You also have a target decoder model with parameters theta and a reference model.
![[Pasted image 20250405120359.png|300]]

Running all three models on your GPU is challenging, but the main goal is to obtain a policy PI and its parameters that maximize the following objective.
Where beta is a regularization term measuring the divergence from the reference model.
This optimization shown here requires advanced methods from reinforcement learning or RL.
This RL objective function is usually solved by proximal policy optimization or PPO.
In DPO, you can convert this complex problem into a simpler objective function that is more straightforward to optimize.
![[Pasted image 20250405120427.png]]

## Partition function
The partition function is a broad topic, but right now you will only use it for a straightforward example to convert a simple distribution to a more complex one.
![[Pasted image 20250405120551.png|300]]

First let's explore the logistic function sigma, which maps any real number to a value between 0 and 1.
This function forms the basis for the logistics probability function, which calculates the probability of y being 0 or 1 given x.
Lets call it p ref.
As you are going to create a new distribution from it.
![[Pasted image 20250405120616.png|400]]

First let's plot the probability of y being 0 given x.
This probability is represented by 1 minus sigma of x and is shown in blue.
As x increases, the probability of y being 0 decreases.
Next you'll plot the probability of y being 1 given x.
This probability is represented by the sigma of x and is shown in reduced as x increases, the probability of y being 1 also increases.
The partition function z of x is the sum of the probabilities of y being 0 and y being 1 given x.
This ensures that the total probability is 1 for each x.
Since this is a valid probability function, it is redundant.
![[Pasted image 20250405120712.png|400]]

You can scale the probability of y =0 as a function of x using an exponential function.
This function is always positive and is symmetric along the y axis.
However, this is not a true probability since it must meet the criteria of a probability function.
Similarly, you can scale the probability of y =1 using a gaussian function which is also positive.
It is shown here in red as a bell-shaped curve centered at a specific x value.
Again, this is not a true probability function as it needs to meet the probability function criteria of summing one for each x.
![[Pasted image 20250405120757.png|400]]

The partition function z plays a crucial role in normalizing probabilities and custom probability functions.
Let's start by plotting the partition function in green.
It is not even close to one.
![[Pasted image 20250405120831.png|400]]

Next you'll plot the scaled probability function for y=0 as a function of x shown in blue.
![[Pasted image 20250405120854.png|400]]

After scaling by z you can see that the function transforms significantly.
![[Pasted image 20250405120914.png|300]]

Repeat this process for the scaled probability function for y =1 plotted in red.
![[Pasted image 20250405120931.png|300]]

Again, after scaling, the function changes noticeably.
Its simple to verify visually summing the probabilities for both values of y should yield 1 for any value of x.
This demonstrates how normalization affects probability functions, ensuring that they meet criteria of a valid probability distribution.
![[Pasted image 20250405121001.png|300]]
## Recap
- DPO
	- is a reinforcement learning technique designed to fine-tune models based on human preferences more directly and efficiently than traditional methods.
	- DPO involves collecting data on human preferences by showing users different outputs from the model and asking them to choose the better one.
	- DPO involves three models: the reward function, which uses an encoder model, the target decoder and the reference model.
	- In DPO you can convert a complex problem into a simpler objective function that is more straightforward to optimize.
- The partition function plays a crucial role in normalizing probabilities and custom probability functions.
