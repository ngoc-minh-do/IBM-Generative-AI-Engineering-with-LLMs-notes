# DPO: Optimal Solution

## Learning objective
- Find a closed form solution RL objective function
- derive the DPO objective
- derive the optimal solution to a DPO problem.

## Objective function in RL
Objective functions are fundamental to machine learning or ML.
They coordinate algorithms and data to reveal patterns, trends, and insights.
These mathematical tools guide ML models through the learning process, aiming for accurate predictions.
Essentially, an objective function measures the difference between an ML models predicted outcomes and the actual target values.
This performance metric is crucial, providing a clear target for optimization.
![[Pasted image 20250405121242.png|500]]

## Kullback-Leibler (KL) divergence
Let's learn to find a closed form solution RL objective function.
Let Pi star be the desired policy and Pi_ref, an arbitrary policy, both representing the probability of y given x.
The Kullback Leibler or KL divergence measures the difference between these two probability distributions, shown here as a distribution and obtained via sampling.
![[Pasted image 20250405121358.png|500]]

It is minimized to zero if and only if the Pi star and Pi_ref are identical.
![[Pasted image 20250405121420.png|300]]

To better understand, let's visualize this with a one dimensional example using two Gaussian distributions.
On the y-axis, you have the possible values of y.
The green Gaussian centered at zero represents Pi star, while the red Gaussian, initially centered at -4, represents Pi_ref.
![[Pasted image 20250405121455.png|400]]

As they overlap, the KL divergence reaches zero.
By manipulating the equations, you can formulate the RL objective function problem to minimize the KL divergence, aligning the Pi star with Pi_ref, and ensuring your learned policy matches the desired behavior with some clever, yet simple math.
![[Pasted image 20250405121517.png|400]]

The first trick is converting a maximum to a minimum.
Here you have a function f(w) plotted on the graph.
The red dot marks the point where f(w) reaches its maximum value, denotated as w hat, which is found by taking the arg max of f(w).
![[Pasted image 20250405121625.png|300]]

Next, you'll transform this function to find its minimum by negating the function.
The red dot now moves to the point where the negated function reaches its minimum value, effectively turning the arg max into an arg min.
This simple transformation allows you to switch between finding maximum and minimum values of a function efficiently.
![[Pasted image 20250405121645.png|300]]

In the same manner, multiplying a function by a scalar does not change the location of the minimum.
Here, the function g(w) is plotted on the graph.
The location of the minimum is along the horizontal axis.
![[Pasted image 20250405121723.png|400]]

Multiplying the function by a scalar, c does not change the location.
![[Pasted image 20250405121915.png|400]]

The function is scaled by c resulting in a new function, c* f(w).
![[Pasted image 20250405122000.png|300]]

## Deriving the DPO objective
Notice that although the shape of the function changes, the minimum remains in the same location.
Let's now reformulate the RL objective using the last two examples.
First, start with the initial equation.
![[Pasted image 20250405122056.png]]

To make the optimization easier, multiply the entire expression by a -1, transforming the maximization problem into a minimization problem.
Notice how the terms are now inverted.
![[Pasted image 20250405122124.png]]

Then multiply the expression by 1/Beta.
None of the operations affected the location of the optimum.
![[Pasted image 20250405122153.png]]

Finally, express everything as an expectation value.
This helps to simplify the optimization process.
![[Pasted image 20250405122204.png]]

Let's focus on the objective and simplify the equation further.
Here, you can see the logarithm of the ratio between your policy and the reference policy minus a term involving the reward.
![[Pasted image 20250405122223.png|400]]

Now, let's reformulate the DPO objective with some simple algebra.
Starting from the initial objective, you have the log ratio of your policy and reference policy minus the reward term scaled by Beta.
![[Pasted image 20250405122247.png|400]]

Next, express the reward term as a logarithm, allowing us to combine the logarithms in a subsequent step.
![[Pasted image 20250405122305.png|400]]

By combining the logarithms, simplify the equation to show the log of the ratio divided by the exponential of the reward term.
![[Pasted image 20250405122314.png|400]]

To normalize, add and subtract a normalization term z(x), ensuring the distribution sums to one.
![[Pasted image 20250405122331.png|400]]

Combining this normalization, adjust your equation to include z(x) explicitly.
![[Pasted image 20250405122344.png|400]]

The denominator contains a distribution, the reward weighted distribution, which will be shown as the reward policy Pi_r.
![[Pasted image 20250405122417.png|400]]

For the KL divergence, the extra log of z must be eliminated.
![[Pasted image 20250405122429.png|300]]

Here, the function f(w) is plotted in the graph.
The red dot marks the location of the minimum value of this function.
![[Pasted image 20250405122516.png|300]]

Next, show that subtracting a constant c does not change the location of the minimum.
The function is adjusted by subtracting the constant c, resulting in a new function, f(w) - c.
Notice that although the vertical position of the function changes, the x value of the minimum does not change.
![[Pasted image 20250405122534.png|300]]

Starting from the simplified expression, you'll formulate the objective function.
![[Pasted image 20250405122609.png|300]]

Now minimize the expected value and simplify by noting that the constant term z(x) is not a parameter of interest.
This leads to a cleaner formulation of your objective.
![[Pasted image 20250405122623.png|400]]

Next, express the objective as minimizing the KL divergence between the policy and the reward policy over the data, which was the initial goal.
![[Pasted image 20250405122709.png|400]]
![[Pasted image 20250405122718.png|400]]

Therefore, the policy that minimizes this expression is simply the reward policy and the optimal solution to the problem.
![[Pasted image 20250405122810.png|400]]

## Optimal solution
The optimal solution scales the reference model to the reward function with the Beta parameter controlling the constant.
Consider the input token x being `this is a`, setting Beta to one.
In the following table, the first column shows two outputs of the LLM, and the second column shows the probability of those outputs.
The first output is cats.
Since this is not unusual, the reference model assigns it a probability of 0.8.
The second output is the reward function.
As this is less likely, the reference model assigns it a probability of 0.1.
If the reward model shown in the third column is optimized for questions related to LLMs, cats would receive a lower score than the reward function, and the probability for the reward function would increase.
By taking the product of these probabilities and normalizing, the new model would assign a higher probability of approximately one to reward function and zero over cats.
Changing Beta will change how the model weighs the reference model to the reward function.
![[Pasted image 20250405122905.png|400]]

## Intractable partition function
Calculating this partition function is essentially impractical.
Let's illustrate this with specific examples.
![[Pasted image 20250405122950.png|400]]

First, for a sequence length of one z(x), sums over all words in your vocabulary, V, such as UBA, aaron, and zyzzyiva.
![[Pasted image 20250405123007.png|400]]

Next, for a sequence length of two, z(x) sums over all possible pairs of words in your vocabulary, creating a much larger set V^2.
![[Pasted image 20250405123027.png|400]]

Finally, generalizing for a sequence length of T, z(x) sums over all possible sequences of length T in your vocabulary V^T.
This exponential growth in the number of terms makes the partition function increasingly difficult to compute as T increases.
However, with the reward policy, you can now tackle this complexity.
![[Pasted image 20250405123041.png|400]]
## Recap
- objective functions
	- coordinate algorithms and data to reveal patterns, trends, and insights to produce accurate predictions.
	- They measure the difference between an ML models predicted outcomes and the actual target values.
- The Kullback Leibler or KL divergence measures the difference between two probability distributions, the desired and the arbitrary policy.
- The optimal solution scales the reference model to the reward function with the Beta parameter controlling the constant.