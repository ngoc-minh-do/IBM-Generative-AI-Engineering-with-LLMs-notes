# Basics of Instruction-Tuning
## Learning objective
- Define instruction tuning 
- Describe the instruction tunning process
- Explore special symbols and prompts format
- Describe instruction masking.

## Instruction tuning
- Instruction tuning is also known as supervised fine tuning or SFT.
- It trains models with expert datasets.
- Instruction tuning enhances the model's performance by providing specific instructions and contexts and ensures that the tasks are handled accurately.
- Instruction tuning is generally used before performing direct preference optimization or DPO and reinforcement learning from human feedback or RLHF.
- It establishes a strong foundational understanding, leading to precise and reliable model outputs.
![[Pasted image 20250330182335.png|400]]

## Train GPT-like model
- First, pre-train the model by predicting the next word in the sequence in GPT.
- Next, use instruction tuning with expert labeled examples.
- Lastly, apply RLHF or DPO.
![[Pasted image 20250330182439.png|400]]

## Instruction tuning
Let's delve into instruction tuning with an example.
Consider a causal language model in a large language model or LLM trained using instruction tuning for educational purposes.
This model is designed to answer questions such as, `which is the largest ocean`? The key difference between the causal language model and the GPT-like model depends on its specialized training and educational content.
![[Pasted image 20250330182624.png|350]]![[Pasted image 20250330182704.png|350]]

Instruction tuning has several key considerations.
The model requires instructions and answers.
One counterintuitive aspect is that the model will generate the instruction as well as the response as it generates the likelihood of the whole sequence, including the response.
For example, when you have an input, `the largest ocean is?`, the model generates the instruction and the response as displayed on the screen.
![[Pasted image 20250330182950.png|400]]

This process is relatively simple to manage.
Using instruction tuning models, perform various tasks by interpreting and executing instructions more effectively.

It uses datasets with templates and three components: instructions, input, and output.
Let's review each of them.
- The instructions provide directions or commands to the user to specify what the user wants the model to do.
	It also defines tasks or actions that the model should perform.
	You can provide instructions to the model for generating text, translating languages, summarizing content, answering questions, or performing specific computations.
- Input in the model is the data or context that should be processed to fulfill the instruction.
	The input data can be a text passage, a list of items, a question, or any other data that the model requires to perform the instructed task.
	It is important to note that not all datasets include the input, meaning they only include instruction and output.
- Lastly, the output defines the expected result or responses.
![[Pasted image 20250330183448.png|300]]
![[Pasted image 20250330183722.png|400]]
![[Pasted image 20250330183542.png|400]]

## Special symbols
Now, let's learn about the role of special symbols in instruction tuning.
Consider the example instruction, `create a Python function that applies a linear function`, followed by the output define function of x returns a linear function.
![[Pasted image 20250330204757.png|400]]

This is difficult for humans to read.
You can see that the output is adjusted to define function of x and return x^2+3x, illustrating the importance of special symbols like backslash N in formatting.
This approach ensures that the model correctly interprets and processes the structured data.
Note that this example does not include input.
![[Pasted image 20250330204907.png|400]]

Additionally, it's vital to adjust the prompt format to maintain compatibility with the different model's tokenizers using special tokens to format prompts.
This approach helps in correct interpretation of certain parts of the input.
![[Pasted image 20250330205026.png|400]]

For example, using triple hashtag `instruction` and triple hashtag `response` ensures that the model's tokenizer has assigned appropriate token indices or IDs, which help the model process correctly.
This model's tokenizer is important for models such as Facebook's open pre-trained transformer language models.
![[Pasted image 20250330205101.png|400]]

However, other models or datasets may use different tokens for the instruction and response.
For example, triple hashtag `human` and triple hashtag `assistant`.
![[Pasted image 20250330205126.png|400]]

## Instruction masking
In instruction masking, the goal is to focus the loss calculation on specific critical tokens rather than all output tokens.
Similar to generative pre-training, the objective is to predict the input shifted one time step forward, but the loss is slightly modified.
Given the sequence, the special token `instruction` precedes the prompt.
`The largest ocean is`, and the special token `output` precedes the `Pacific Ocean` response.
The model then learns to predict the shifted sequence.
`The largest ocean is? Output: Pacific Ocean EOS`.
![[Pasted image 20250330205613.png|400]]

To understand how the loss is modified, let's break down the output logits after passing through the final linear layer.
![[Pasted image 20250330205725.png|400]]
![[Pasted image 20250330205748.png|400]]

Consider the input tokens denoted by Omega at timestamp T (w_t) and the corresponding output logits for the timestamp shown in red.
In PyTorch, cross-entropy loss is typically computed between the model's output logits and the next token in the sequence shifted at one point in time.
![[Pasted image 20250330205847.png|500]]

For clarity, write the loss as the actual predicted token Omega hat in the loss calculation at the time index T that you would like to predict.
![[Pasted image 20250330205922.png|500]]

In generative pre-training, you can sum the loss over tokens in a batch.
However, instruction masking restrict the loss calculations to key tokens.
For example, the input `instruction: the largest ocean is`.
The predicted output would be the shifted sequence displayed in yellow boxes in regular fine tuning.
However, the model learns to predict the `Pacific Ocean and EOS` in instruction tuning.
![[Pasted image 20250330210113.png|400]]

This targeted approach ensures that the model focuses on the important parts of the sequence.
![[Pasted image 20250330210145.png|400]]

## Considerations for instruction masking
- Recent studies show that the unmasked instructions sometimes show better performance for smaller datasets.
- In contrast, some libraries mask the instructions by default.
	In Hugging Face, you should specify if the instructions are masked using a function called `DataCollatorForCompletionOnlyLM`. 
- However, special tokens are typically masked.
![[Pasted image 20250330210257.png|400]]
## Recap
- Instruction tuning involves training models with expert-curated datasets.
- For instruction tuning, the model requires instructions and answers.
- Instruction tuning helps perform a wide variety of tasks by interpreting and executing instructions more effectively.
- To generate a response, instruction tuning uses 3 components: instructions, input, and output.
- You can adjust the prompt format to maintain compatibility with the different model's tokenizers.
- Lastly, the instruction masking focuses on the loss calculation of specific tokens.