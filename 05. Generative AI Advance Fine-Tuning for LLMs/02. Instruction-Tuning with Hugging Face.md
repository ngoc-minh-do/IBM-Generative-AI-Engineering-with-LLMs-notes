# Instruction-Tuning with Hugging Face

## Learning objective
- Explore how to load a dataset and format it 
- Describe model creation, and training arguments.
- Define collator and training loss 
- Describe pipeline for text generation.

## Instruction-tuning
Instruction-based fine-tuning is referred to as instruction GPT.
It trains the language models by following specific instructions to generate appropriate responses.
Here, dataset plays an important role by providing structured examples of instructions, contexts, and responses.
Using this approach, the model handles various tasks effectively.
The context and instructions are linked together to form a single input sequence for the model to understand and use to generate the desired response.
![[Pasted image 20250330214247.png|500]]

## Load dataset
Let's first load the CodeAlpaca-20k dataset, a programming code dataset.
This dataset includes the components such as instruction, a unique task description for the model.
The input is an optional context or input for the task present in about 40% of the examples.
```python
dataset = load_dataset("json", data_files="CodeAlpaca-20k.json", split="train")
dataset
# Dataset({
#	features: ['instruction', 'input', 'output'],
#	num_rows: 20022
# })

dataset = dataset.filter(lambda example: example["input"] == '')
dataset = dataset.shuffle(seed=42)
```

Lastly, the `output` is an answer to the instruction.
Next, split the remaining dataset into 80% for training and 20% for validation.
Here, let's drop the samples that contain an input value.
```python
dataset_split = dataset.train_test_split(test_size=0.2, seed=42)
dataset_split
# DatasetDict({
#    train: Dataset({
#        features: ['output', 'instruction', 'input'],
#        num_rows: 7811
#    })
#    test: Dataset({
#        features: ['output', 'instruction', 'input'],
#        num_rows: 1953
#    })
#})

train_dataset = dataset_split['train']
test_dataset = dataset_split['test']

dataset[1]
```
![[Pasted image 20250330221332.png|500]]

## Format data
For fine-tuning the dataset, it is important to preprocess the data by creating functions that generate the prompt.
For training the `formatting_prompts_func` function, uses the dataset as input.
For every element in the dataset, it formats the `instruction` and the `output` into a template with the format triple hashtag instruction, followed by triple hashtag response.
Adding end of the sentence or EOS token, at the end of the text indicates that the model should stop generating text at this point.
```python
def formatting_prompts_func(mydataset):
    output_texts = []
    for i in range(len(mydataset['instruction'])):
        text = (
            f"### Instruction:\n{mydataset['instruction'][i]}"
            f"\n\n### Response:\n{mydataset['output'][i]}</s>"
        )
        output_texts.append(text)
    return output_texts
```

Finally, the `formatting_prompts_func_no_response` function, acts similarly to the `formatting_prompts_func`, except that it doesn't include responses.
This function prepares samples for response generation when validating the model.
```python
def formatting_prompts_func_no_response(mydataset):
    output_texts = []
    for i in range(len(mydataset['instruction'])):
        text = (
            f"### Instruction:\n{mydataset['instruction'][i]}"
            f"\n\n### Response:\n"
        )
        output_texts.append(text)
    return output_texts
```

## Create formatted dataset
For creating a formatted dataset, a code block generates instructions.
The first code block strips and tokenizes the input.
It splits and tokenizes the responses for evaluation.
```python
expected_outputs = []
instructions_with_responses = formatting_prompts_func(test_dataset)
instructions = formatting_prompts_func_no_response(test_dataset)

for i in tqdm(range(len(instructions_with_responses))):
    tokenized_instruction_with_response = tokenizer(instructions_with_responses[i], return_tensors="pt", max_length=1024, truncation=True, padding=False)
    tokenized_instruction = tokenizer(instructions[i], return_tensors="pt")
    expected_output = tokenizer.decode(tokenized_instruction_with_response['input_ids'][0][len(tokenized_instruction['input_ids'][0])-1:], skip_special_tokens=True)
    expected_outputs.append(expected_output)
```

The second code block defines a list dataset class obtained from the dataset and creates a torch dataset from the list.
This class is useful in generating a dataset object from the instructions.
```python
class ListDataset(Dataset):
    def __init__(self, original_list):
        self.original_list = original_list
    
    def __len__(self):
        return len(self.original_list)
    
    def __getitem__(self, i):
        return self.original_list[i]

instructions_torch = ListDataset(instructions)
```

## Model initialization and LoRA configuration
Let's fine-tune `facebook/opt-350m` model by loading a base model from Hugging Face.
You can use a parameter-efficient fine-tuning or peft method, low-ranking adaptation, or LoRA to save time and convert the base model into a peft method suitable for LoRA.
Next, fine-tune LoRA by defining the lora_config object from the peft library with specific parameters such as LoRA rank, target modules, and a task type for causal LLM.
Apply this configuration to the model using GT PET model, converting it into a LoRA model.
```python
# Base model
model = AutoModelForCausalLM.from_pretrained("facebook/opt-350m").to(device)

lora_config = LoraConfig(
    r=16,  # Low-rank dimension
    lora_alpha=32,  # Scaling factor
    target_modules=["q_proj", "v_proj"],  # Modules to apply LoRA
    lora_dropout=0.1,  # Dropout rate
    task_type=TaskType.CAUSAL_LM  # Task type should be causal language model
)

model = get_peft_model(model, lora_config)
```

## Training arguments
For training the model, define supervised fine-tuning or SFT config.
The output directory defines the location for the model and training files.
You can customize the number of training epochs and batch sizes for training and evaluation.
During training, the evaluation strategy defines the time for evaluation.
Set the evaluation strategy to evaluate after each EPOC completes training.
The MAX sequence length controls the maximum length of the output responses.
Set FP 16 to true for efficient training to enable 16-bit floating point precision.
```python
training_args = SFTConfig(
    output_dir="/tmp",
    num_train_epochs=10,
    save_strategy="epoch",
    fp16=True,
    per_device_train_batch_size=2,  # Reduce batch size
    per_device_eval_batch_size=2,  # Reduce batch size
    max_seq_length=1024
)
```

## Define the collator
Now define the collator, using the `DataCollatorForCompletionOnlyLM` from the transformers reinforcement learning or TRL library that prepares data batches for training language models on text completion tasks and masks the instruction part of the input when calculating the loss.
```python
response_template = "### Response:\n"
collator = DataCollatorForCompletionOnlyLM(response_template, tokenizer=tokenizer)
```

Additionally, the collator helps
- add padding tokens for shorter sequences to ensure uniform length within a batch and truncate longer sequences.
- It creates attention masks for differentiating the actual instruction and padding tokens to prevent them from affecting model learning.
- It also helps aggregate sequences by combining multiple sequences into a single batch for efficient processing.
- Lastly, it helps handle special tokens for managing the batch from start to finish, including padding tokens.
![[Pasted image 20250330223220.png|400]]

## Define the trainer
Next to define the trainer, create the `SFTTrainer` object by passing the `formatting_func` to the trainer to handle data pre-processing.
Then set the `packing` parameter to false to not pack multiple short examples.
Here, the MAX prompt length parameter controls the prompt length.
Lastly, pass the collator to the SFT trainer.
```python
trainer = SFTTrainer(
    model,
    train_dataset=train_dataset,
    formatting_func=formatting_prompts_func,
    args=training_args,
    packing=False,
    data_collator=collator,
)

trainer.train()
```

## Training loss
You can plot the training loss saved during training in the trainer state.
```python
train_loss = [log["loss"] for log in log_history_lora if "loss" in log]

# Plot the training loss
plt.figure(figsize=(10, 5))
plt.plot(train_loss, label='Training Loss')

plt.xlabel('Steps')
plt.ylabel('Loss')
plt.title('Training Loss')
plt.legend()
plt.show()
```
![[Pasted image 20250330224753.png|400]]

## Text generation pipeline
Let's explore the pipeline for text generation and hugging face to simplify the process by automatically handling model loading, tokenization, padding, and text generation parameters.
To evaluate the model's text generation, use a text generation pipeline from the transformers library.

First, define the pipeline using the pipeline class with the model and tokenizer.
For this, let's set the task to text generation for generating text like a regular chatbot.
The max length defines the maximum length of the generated text.
Lastly, set the return full text to false only returns the response without the instruction.
```python
gen_pipeline = pipeline("text-generation", 
                        model=model, 
                        tokenizer=tokenizer, 
                        device=device, 
                        batch_size=2, 
                        max_length=50, 
                        truncation=True, 
                        padding=False,
                        return_full_text=False)
```

Next, generate text by passing input to the pipeline to get responses using functions such as num beams which generates higher-quality text.
The early stopping parameter controls the stopping condition for beam-based methods.
Once the pipeline is created, pass the input to generate responses.
```python
with torch.no_grad():
    pipeline_iterator= gen_pipeline(instructions_torch[:3],
                                num_beams=5,
                                early_stopping=True,)
generated_outputs_lora = []
for text in pipeline_iterator:
    generated_outputs_lora.append(text[0]["generated_text"])
```

Here, you can see a generated sample response for the fine-tuned model.
```python
for i in range(3):
    print('@@@@@@@@@@@@@@@@@@@@')
    print('@@@@@ Instruction '+ str(i+1) +': ')
    print(instructions[i])
    print('\n\n')
    print('@@@@@ Expected response '+ str(i+1) +': ')
    print(expected_outputs[i])
    print('\n\n')
    print('@@@@@ Generated response '+ str(i+1) +': ')
    print(generated_outputs_lora[i])
    print('\n\n')
    print('@@@@@@@@@@@@@@@@@@@@')
```
![[Pasted image 20250330225216.png|500]]

## Evaluate the model using BLEU score
Finally, let's evaluate the model using the bilingual evaluation under study or blue score, it is challenging to evaluate language models using quantitative metrics.
Bleu is a useful metric for coding, and Sacrebleu is a standardized variant of bleu.
The results show that the fine-tuned model achieves a Sacrebleu score of 14.7 by 100, significantly better than the 0.4 by 100 achieved by the base model.
Therefore, the instruction fine-tuned model generates responses that align better with the expected responses in the dataset.
```python
sacrebleu = evaluate.load("sacrebleu")
results = sacrebleu.compute(predictions=generated_outputs_base,
                                 references=expected_outputs)
print(round(results["score"], 1))
# 0.4

sacrebleu = evaluate.load("sacrebleu")
results_lora = sacrebleu.compute(predictions=generated_outputs_lora,
                                 references=expected_outputs)
print(round(results_lora["score"], 1))
# 14.7
```

## Recap
- First, load a dataset using CodeAlpaca 20k dataset, a programming code dataset.
- Format the dataset using the formatting prompts func function, which leverages the dataset as input.
- To create a formatted dataset, use two code blocks to generate instructions with and without responses.
- To create a model, fine-tune Facebook's Opt-350 M model by loading a base model from the Hugging face.
- Define the collator using data collator for completion only LM to prepare data batches for training language models 
- Define the trainer by creating the SFT trainer object.
- Finally, generate a text pipeline from the transformer's library and evaluate the model's text generation using the Bleu score.