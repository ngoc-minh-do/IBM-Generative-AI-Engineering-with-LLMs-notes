# Proximal Policy Optimization (PPO)

## Learning objective
- Explain policy gradient and PPO methods.
- Review the policy gradient objective function and kullback-Leibler or KL penalty coefficient.
- Optimize the sample using the log-derivative trick 
- Demonstrate how to maximize the objective function using the gradient ascent toy example.

## Reinforcement learning from human feedback
Let's begin by creating an agent or large language model (LLM) with a set of learnable parameters theta and the reward model.
Next, let's introduce a query x as the input to the agent to generate a response y, the query and response that is rollout where the reward function processes the x and y.
The generated reward trains the LLM and updates the learnable parameter data.
![[Pasted image 20250403233926.png|400]]

## Policy gradient methods and PPO
Now let's understand the policy gradient and PPO methods.
`Policy gradient methods` objective function is what you want to maximize and `proximal policy optimization` is one method to achieve this.
These methods comprise different aspects as illustrated in the Venn diagram.
First, you have `policy gradient methods` forming the foundation of various reinforcement learning algorithms.
Next, `clipped surrogate objectives and other methods` stabilize training by ensuring updates to the policy that are not too drastic.
The `KL penalty coefficient` regulates the divergence between old and new policies, maintaining stability during training.
![[Pasted image 20250403234142.png|400]]

Another important aspect is the advantage function which estimates the reward.
In this video, you will only review the general policy gradient objective function and the KL penalty coefficient.
![[Pasted image 20250403234256.png|400]]

## Proximal policy optimization (almost)
The objective function of the policy gradient methods is similar to the score which needs to be maximized.
To do so, start with the reward function encoder which estimates the reward for the pairs of inputs x and y. r(X, Y)
![[Pasted image 20250403234403.png|200]]

Next, introduce the model that needs to be fine tuned.
The model is represented as the policy PI or LLM, such as a GPT like model that can be instruction fine tuned. pi(Y|X)
![[Pasted image 20250403234440.png|300]]

![[Pasted image 20250403234544.png|300]]

Lets look at the first step in the optimization process for the given data query x, derive the sample response y from the given data set and estimate the reward.
The estimated reward is represented as the expected reward sample for the policy PI to find the parameter theta.
![[Pasted image 20250403234618.png|500]]

In the second step, extend the entire dataset and estimate the expected reward over all the queries.
![[Pasted image 20250403234635.png|400]]

However, your goal is to find the optimal policy that maximizes the expected reward.
![[Pasted image 20250403234657.png|400]]

Additionally, introduce a reference model as a regularizing term to ensure that the model does not stay too far from the original.
![[Pasted image 20250403234728.png|300]]

Next, control the hyperparameter beta.
![[Pasted image 20250403234753.png]]

It is challenging to solve this problem because you are trying to find theta from sampling.
![[Pasted image 20250403234819.png]]
![[Pasted image 20250403234831.png]]

To address this, even basic policy gradient methods require a foundational understanding of reinforcement learning statistics.
The log derivative trick akin to techniques used in Monte Carlo methods provides insight into optimizing this problem.
Both methods involve estimating gradients to optimize a function based on sample data.
It is important to note that the log derivative trick addresses only one aspect of the PPO problem.
![[Pasted image 20250403234858.png|500]]

## Log derivative trick
To calculate the derivative find the policy that maximizes the objective function, the expected rewards here to make this process easy, ignore the regulation term.
![[Pasted image 20250403234950.png|500]]

First simplify the expression by focusing on the expected reward for the individual queries, noting that the derivative cannot be directly computed in this form.
![[Pasted image 20250403235015.png|500]]

Next, convert the individual queries or expressions to an analytical distribution, allowing direct optimization of the parameters with respect to theta.
![[Pasted image 20250403235044.png|400]]
![[Pasted image 20250403235146.png|400]]

To find the best parameter, take the gradient and highlight the gradient of the log transformation.
![[Pasted image 20250403235324.png|400]]

Next, rearrange the expression into traceable form and substitute the gradient back into the expression.
![[Pasted image 20250403235341.png|500]]
This converts the sampling expression, allowing gradient calculations using samples.
![[Pasted image 20250403235418.png|500]]
![[Pasted image 20250403235446.png|500]]

Finally, factor out the gradient to complete the transformation.
![[Pasted image 20250403235504.png|500]]

## Tips for training
Lets review certain tips to train a model.
- When training a model, regularly evaluate it using human feedback.
- Start training a model with a more moderate beta value and increase the temperature to explore more options.
![[Pasted image 20250403235550.png|400]]

## Recap
- The policy gradient method maximizes the objective function and PPO helps to achieve this maximization.
- To optimize the policy, derive the sample response, estimate the reward, and extend the dataset.
- You can calculate the log derivative by identifying a policy that maximizes the objective function by simplifying the expression and converting it into analytical distributions.
- To train the model, regularly evaluate the model using human feedback, use the moderate beta value and increase the temperature.