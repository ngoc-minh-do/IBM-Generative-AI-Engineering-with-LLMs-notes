# From Optimal Policy to DPO

## Learning objective
- Train a generative causal LLM using direct preference optimization or DPO
- derive the DPO objective and find an expression to maximize it.
- Use the Bradley-Terry model to understand loss and convert it to cost.

## RLHF challenges
Reinforcement learning from human feedback or RLHF is an effective technique to optimize LLMs, but it poses challenges such as computational complexity, non differentiability, and instability.
![[Pasted image 20250405123335.png|400]]

DPO reformulates the problem to address these issues by leveraging a closed form optimal policy as a function of the reward.
![[Pasted image 20250405123429.png|400]]

## DPO dataset

To begin with, let's use a scoring function dataset to train the generative causal LLM using DPO.
Human evaluators assign scores to responses, but assigning precise numerical scores is challenging.
The table shows queries and responses in the first two columns and scores in the third.
The first row is the query, the second row is the higher scoring response, response A, and the third row is the lower scoring response, response B.
Ranking responses is easier than assigning scores.
![[Pasted image 20250405123503.png|400]]

The second table arranges responses by popularity without numerical scores, simplifying evaluation for human evaluators.
Your focus will be on pairwise ranking of two samples, but the method can extend to multiple samples.
For consistency, let's follow the same notion as the DPO literature.
WIN represents response A and L, loss represents response B.
![[Pasted image 20250405123525.png|400]]

Using sampling notation where D is the dataset, and the Tilda, that is the wavy line, represents sampling values.
X, Yw and Yl are drawn from the dataset as shown in the tables example.
![[Pasted image 20250405123546.png|400]]

## Deriving the DPO objective
In the original Bradley-Terry model, the loss is the log of the Sigmoid functions difference between the scores of the winning response and the losing response.
![[Pasted image 20250405123637.png|400]]

Using the sampling notation, transform this summation into an expected value over the dataset D.
![[Pasted image 20250405123657.png|400]]

Now let's focus on the argument within the log and Sigmoid functions, which essentially represent the loss function.
The difference between the scores of the winning and losing responses.
![[Pasted image 20250405123711.png|400]]

## Reward policy is the optimal solution
To solve the given direct preference optimization or DPO problem, you'll need to find the reward policy where Pi or policy r is the optimal solution.
Here, X is the query, Y is the response, and Z is the partition function.
Piref is the reference model, R is the reward function, and Beta is the regularization parameter.
![[Pasted image 20250405123814.png|400]]

## Intractable partition function
The main issue is that you can't solve the partition function Z as it involves summing over every possible combination.
However, with some clever, yet simple math, you can eliminate the need to calculate the partition function and find a formulated cost function for your causal LLM.
This allows you to directly train your model based on the Bradley-Terry model forgoing the difficult training process of PPO.
![[Pasted image 20250405123847.png|400]]

## Substituting the optimal solution into the Bradley-Terry model
Now you'll derive the DPO objective, starting with the optimal solution.
![[Pasted image 20250405124005.png|400]]

First, isolate the exponential term and multiply both sides by the partition function.
![[Pasted image 20250405124021.png|400]]
![[Pasted image 20250405124043.png|400]]

Then take the natural logarithm of both sides to linearize the exponential term and solve for the reward function.
Now you have the reward model in terms of the optimal solution.
![[Pasted image 20250405124105.png|400]]
![[Pasted image 20250405124129.png|400]]
![[Pasted image 20250405124150.png|400]]

Plug in the values of the equations for the positive and the negative samples, representing the winning responses in blue and the losing responses in red.
![[Pasted image 20250405124211.png|400]]

Recall the loss function for the Bradley-Terry model
![[Pasted image 20250405124245.png|400]]

Subtracting the reward model for two samples by substituting these expressions into the equation not only eliminates the need for the partition function, but the new loss function shown here is now the function of the LLM and its reference model.
This eliminates the need for a separate reward function, giving an expression to maximize the DPO objective.
![[Pasted image 20250405124305.png|400]]
![[Pasted image 20250405124327.png|500]]

## Understanding the loss
Let's simplify the expression step by step to better understand it.
First, start with the initial equation for the loss function for one sample output.
![[Pasted image 20250405124406.png|400]]

Next, set the Beta to one, simplifying the equation by removing the Beta scaling factor.
![[Pasted image 20250405124422.png|400]]

To further simplify, replace the reference model with the constant C, which means you are randomly selecting any word in the vocabulary with equal probability.
![[Pasted image 20250405124438.png|400]]

Using the laws of logarithms, you can combine the terms inside the logarithm.
![[Pasted image 20250405124448.png|300]]

Finally, set the argument of the logarithm to a single variable, U, representing the ratio of probabilities of the positive samples and the negative sample.
This final form shows the loss as a function of the logarithm of U.
![[Pasted image 20250405124506.png|200]]

Now, let's plot the loss as a function of u.
Starting with the initial equation, let's consider when the policy of the winning response, given the query, is less than the policy of the losing response.
If the policy of the winning response increases, but remains smaller than the losing response, this corresponds to the range 0-1.
Therefore, as U increases, the model gets better.
Plotting the loss as a function of U, you can see that as the probability of the winning response increases, the loss decreases.
![[Pasted image 20250405124600.png|500]]

When the policy of the winning response is larger than the policy of the losing response, U ranges from one to infinity, representing a higher policy of preferred completions.
As U increases, indicating a higher positive policy, the loss continues to decrease as shown in the corresponding section of the graph.
In a similar manner to the Bradley-Terry model, you can convert the loss to a cost.
![[Pasted image 20250405124621.png|500]]

## Reformulate back to minimizing the log-likelihood
Let's start with the initial equation.
Here, the loss function is expressed with the negative of the Sigmoid of the logarithms of the ratios of the winning and losing policies scaled by Beta.
![[Pasted image 20250405124656.png|400]]

Next, insert this expression into the Bradley-Terry model, this reformulates the equation to minimize the log likelihood transforming the loss into a cost.
To implement this Pi torch, you can write a loss function and calculate the loss or you can use hugging faces built in DPO trainer.
![[Pasted image 20250405124709.png|500]]
## Recap
- DPO leverages a closed form optimal policy as a function of the reward to reformulate the problem.
- To solve the given DPO problem, you'll need to find the reward policy, which is given by the following expression.
	![[Pasted image 20250405124737.png|400]]

- Subtracting the reward model for two samples eliminates the need for the partition function, and the new loss function becomes a function of the LLM and its reference model.
	![[Pasted image 20250405124759.png|400]]
- The loss function is expressed with the negative of the Sigmoid of the logarithms of the ratios of the winning and losing policies scaled by Beta.
	![[Pasted image 20250405124834.png]]