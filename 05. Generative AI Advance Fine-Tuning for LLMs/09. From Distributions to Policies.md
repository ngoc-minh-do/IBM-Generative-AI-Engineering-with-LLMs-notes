# From Distributions to Policies
[MUSIC] Welcome to, "From distributions to policies." After watching this video, you'll be able to describe how to generate policies using a language model for distributing and applying rollouts into policies.
In reinforcement learning, a policy is a specific strategy or mapping that an agent uses to decide its actions based on the current state of its environment.
Let's look at the importance of policy.
Policies in reinforcement learning or RL, determine distributions for generating sequences of actions.
In LLMs, policies can guide the generation process, allowing models to explore various text generation paths and leading to more diverse and contextually appropriate outputs.
In RL, policies are used to perform tasks such as, playing games by guiding actions toward goals.
For LLMs, policies enhance decision-making and text based tasks, helping models learn optimal responses, thereby improving accuracy and relevance in language understanding and generation.
Unlike conventional AI methods, RL policy is used randomness to explore unseen possibilities.
This approach helps LLMs perform better by generating creative diverse responses and adapting to new contexts, making the model more robust and versatile.
Lets delve into policy a language model as a distribution in terms of reinforcement learning, the language model generates responses based on the inserted query by following policy distribution.
This is represented as y follows the policy given x, where x is the input sequence of length m and y is the output sequence of the total sequence of length n.
For example, consider the query which is the largest ocean.
The model generates various possible responses.
Each possible response is known as rollout.
The first realization is the Pacific Ocean.
The second is that the Pacific Ocean is the largest ocean on Earth.
The third is that the Atlantic Ocean is 155 million is the Atlantic Ocean.
To clarify the relationship between the policy and language model as a function of omega, consider the relationship y follows the policy given x.
The detailed policy distribution is represented as a function of omega, showing how probabilities of the response are computed based on the previous tokens.
For the query, which is the largest ocean, the model first calculates the probability distribution for different responses.
For example, in the probability of Atlantic, you can see that the given query followed by the probability of the Atlantic Ocean, inserted as previous words where k is the response index.
Finally, let's look at the rollouts.
Rollouts refer to how the model generates different responses for each query.
For this lets look at the initial set of queries.
Consider the query which is the largest ocean.
The model generates several random responses.
In this example, it is five each termed as a rollout.
The process continues with the query can you give me some python code where the model generates several rollouts? It is important to note that the definition of rollout in library such as hugging face differs from that in reinforcement learning or RL, where RL is a reward included in the definition.
Let's recap, in this video you've learned to use distribution as a policy.
In reinforcement learning, a policy is an agent specific strategy or mapping to decide its actions.
Policies help to generate actions in LLMs, enhance learning and decision-making, and boost performance through exploration.
Following the policy distribution, the language model generates responses based on the inserted query.
You can consider the relationship y follows the policy given x to clarify the relationship between the policy and the language model as a function of omega.
Rollouts are how the model generates different responses for each query.
The rollout libraries, such as hugging face differ from the reinforcement learning.
[MUSIC]