# DPO: Optimal Solution

Welcome to DPO optimal solution.
After watching this video, you'll be able to find a closed form solution RL objective function, derive the DPO objective, and derive the optimal solution to a DPO problem.
Objective functions are fundamental to machine learning or ML.
They coordinate algorithms and data to reveal patterns, trends, and insights.
These mathematical tools guide ML models through the learning process, aiming for accurate predictions.
Essentially, an objective function measures the difference between an ML models predicted outcomes and the actual target values.
This performance metric is crucial, providing a clear target for optimization.
Let's learn to find a closed form solution RL objective function.
Let Pi star be the desired policy and Pi_ref, an arbitrary policy, both representing the probability of y given x.
The Kullback Leibler or KL divergence measures the difference between these two probability distributions, shown here as a distribution and obtained via sampling.
It is minimized to zero if and only if the Pi star and Pi_ref are identical.
To better understand, let's visualize this with a one dimensional example using two Gaussian distributions.
On the y-axis, you have the possible values of y.
The green Gaussian centered at zero represents Pi star, while the red Gaussian, initially centered at -4, represents Pi_ref.
As they overlap, the KL divergence reaches zero.
By manipulating the equations, you can formulate the RL objective function problem to minimize the KL divergence, aligning the Pi star with Pi_ref, and ensuring your learned policy matches the desired behavior with some clever, yet simple math.
The first trick is converting a maximum to a minimum.
Here you have a function f(w) plotted on the graph.
The red dot marks the point where f(w) reaches its maximum value, denotated as w hat, which is found by taking the arg max of f(w).
Next, you'll transform this function to find its minimum by negating the function.
The red dot now moves to the point where the negated function reaches its minimum value, effectively turning the arg max into an arg min.
This simple transformation allows you to switch between finding maximum and minimum values of a function efficiently.
In the same manner, multiplying a function by a scalar does not change the location of the minimum.
Here, the function g(w) is plotted on the graph.
The location of the minimum is along the horizontal axis.
Multiplying the function by a scalar, c does not change the location.
The function is scaled by c resulting in a new function, c* f(w).
Notice that although the shape of the function changes, the minimum remains in the same location.
Let's now reformulate the RL objective using the last two examples.
First, start with the initial equation.
To make the optimization easier, multiply the entire expression by a -1, transforming the maximization problem into a minimization problem.
Notice how the terms are now inverted.
Then multiply the expression by 1/Beta.
None of the operations affected the location of the optimum.
Finally, express everything as an expectation value.
This helps to simplify the optimization process.
Let's focus on the objective and simplify the equation further.
Here, you can see the logarithm of the ratio between your policy and the reference policy minus a term involving the reward.
Now, let's reformulate the DPO objective with some simple algebra.
Starting from the initial objective, you have the log ratio of your policy and reference policy minus the reward term scaled by Beta.
Next, express the reward term as a logarithm, allowing us to combine the logarithms in a subsequent step.
By combining the logarithms, simplify the equation to show the log of the ratio divided by the exponential of the reward term.
To normalize, add and subtract a normalization term z(x), ensuring the distribution sums to one.
Combining this normalization, adjust your equation to include z(x) explicitly.
The denominator contains a distribution, the reward weighted distribution, which will be shown as the reward policy Pi_r.
For the KL divergence, the extra log of z must be eliminated.
Here, the function f(w) is plotted in the graph.
The red dot marks the location of the minimum value of this function.
Next, show that subtracting a constant c does not change the location of the minimum.
The function is adjusted by subtracting the constant c, resulting in a new function, f(w) - c.
Notice that although the vertical position of the function changes, the x value of the minimum does not change.
Starting from the simplified expression, you'll formulate the objective function.
Now minimize the expected value and simplify by noting that the constant term z(x) is not a parameter of interest.
This leads to a cleaner formulation of your objective.
Next, express the objective as minimizing the KL divergence between the policy and the reward policy over the data, which was the initial goal.
Therefore, the policy that minimizes this expression is simply the reward policy and the optimal solution to the problem.
The optimal solution scales the reference model to the reward function with the Beta parameter controlling the constant.
Consider the input token x being this is a, setting Beta to one.
In the following table, the first column shows two outputs of the LLM, and the second column shows the probability of those outputs.
The first output is cats.
Since this is not unusual, the reference model assigns it a probability of 0.8.
The second output is the reward function.
As this is less likely, the reference model assigns it a probability of 0.1.
If the reward model shown in the third column is optimized for questions related to LLMs, cats would receive a lower score than the reward function, and the probability for the reward function would increase.
By taking the product of these probabilities and normalizing, the new model would assign a higher probability of approximately one to reward function and zero over cats.
Changing Beta will change how the model weighs the reference model to the reward function.
Calculating this partition function is essentially impractical.
Let's illustrate this with specific examples.
First, for a sequence length of one z(x), sums over all words in your vocabulary, V, such as UBA, aaron, and zyzzyiva.
Next, for a sequence length of two, z(x) sums over all possible pairs of words in your vocabulary, creating a much larger set V^2.
Finally, generalizing for a sequence length of T, z(x) sums over all possible sequences of length T in your vocabulary V^T.
This exponential growth in the number of terms makes the partition function increasingly difficult to compute as T increases.
However, with the reward policy, you can now tackle this complexity.
Let's recap.
In this video, you learned that objective functions coordinate algorithms and data to reveal patterns, trends, and insights to produce accurate predictions.
They measure the difference between an ML models predicted outcomes and the actual target values.
The Kullback Leibler or KL divergence measures the difference between two probability distributions, the desired and the arbitrary policy.
The optimal solution scales the reference model to the reward function with the Beta parameter controlling the constant.