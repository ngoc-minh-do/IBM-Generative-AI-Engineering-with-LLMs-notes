# PPO Trainer
- Explore proximal policy optimization or PPO and the steps to initialize PPO configurations.
- Demonstrate how to generate texts with PPO and reference models
- Explain comparisons between models.

## Initialize the PPO configuration
Let's begin by exploring how to initialize PPO configuration.
The PPO config class is useful for specifying the model and learning rate for PPO training.
The model name specifies the model that needs to be fine tuned.
Lastly, set the learning rate.
```python
config = PPOConfig(model_name="lvwerra/gpt2-imdb", learning_rate=1.41e-5)
```

To load the model and the reference model.
Use the reference model to stabilize training by Kullback-Leibler or KL divergence between the current policy that is the model and the reference policy that is the reference model.
AutoModelForCausalLMWithValueHead, extends auto model for causal LM class for reinforcement learning.
```python
model_1 = AutoModelForCausalLMWithValueHead.from_pretrained(model_name)

ref_model = AutoModelForCausalLMWithValueHead.from_pretrained(model_name)
```

## Collator function
The collateral function is crucial for preparing data batches in a format suitable for the PPO trainer.
It ensures that each feature from the data samples is grouped together.
The output is demonstrated with an input toy example.
```python
def collator(data):
    return dict((key, [d[key] for d in data]) for key in data[0])
```
![[Pasted image 20250404003101.png]]
```python
batch = collator(data)
batch
```
![[Pasted image 20250404003130.png|500]]

## Initialize PPO trainer
Now, let's understand the PPO trainer.
The PPO trainer processes query samples and optimizes the chatbots policy, handling complex tasks to ensure high quality responses.

Let's initialize PPO trainer with the specified configuration and components.
First, configure the settings for PPO training.
Using configuration settings such as learning rate and model name.
Next, fine tune the primary model using PPO and input the reference model.
Further, input the tokenizer for processing the input text.
Next, insert the dataset to provide the input data for the model to train it.
Lastly, the data collator handles the batching and formatting of the input data.
```python
ppo_trainer = PPOTrainer(config, model_1, ref_model, tokenizer, dataset=dataset, data_collator=collator)

device = ppo_trainer.accelerator.device
if ppo_trainer.accelerator.num_processes == 1:
    device = 0 if torch.cuda.is_available() else "cpu"  
print(device)
```

## Proximal policy optimization
In PPO, the list stats all stores the training statistics for each batch.
Set the sentiment score change score to 1 to provide a higher reward for positive sentiment and encourage the chatbot to generate positive responses.
On the other hand, setting the sentiment score to zero will increase the chances of generating higher reward for negative sentiment, resulting in negative responses.
```python
all_stats = []
change_score = 1
```

The screen displays the training loop code for the PPO algorithm using sentiment analysis.
The training loop iterates over batches of data for the PPO trainer and data loader.
Now perform the following steps.
For each number, extract input IDs, queries from each batch, and query tensors for each query tensor in the batch.
Select a random sample output length and use it to set the maximum number of new tokens to generate a response using PPO trainer and store it.
Next, append the generated response tensor to the response tensors list, trimming it to the generated length.
Now decode the response tensors to text and add them to the batch.
Concatenate the query and response for each pair to form the text and apply the sentiment analysis pipeline to the texts.
Now extract the sentiment score and convert it to a Tensor.
Lastly, collect the statistics by performing a PPO step with the queries, responses, and rewards.
Run the training step method to train the model.
Log the statistics along with the batch and rewards and append the statistics to the all_stats list.
```python
for epoch, batch in tqdm(enumerate(ppo_trainer.dataloader)):
    query_tensors = batch["input_ids"]
    print(f"epoch {epoch}")

    #### Get response from gpt2
    response_tensors = []
    for query in query_tensors:
        gen_len = output_length_sampler()
        generation_kwargs["max_new_tokens"] = gen_len
        response = ppo_trainer.generate(query, **generation_kwargs)
        response_tensors.append(response.squeeze()[-gen_len:])
    batch["response"] = [tokenizer.decode(r.squeeze()) for r in response_tensors]

    #### Compute sentiment score
    texts = [q + r for q, r in zip(batch["query"], batch["response"])]
    pipe_outputs = sentiment_pipe(texts, **sent_kwargs)
    negative_scores = [
           item["score"]
           for output in pipe_outputs
           for item in output
           if item["label"] == sentiment
       ]
   rewards = [torch.tensor(score) for score in negative_scores]

    #### Run PPO step
    stats = ppo_trainer.step(query_tensors, response_tensors, rewards)
    ppo_trainer.log_stats(stats, batch, rewards)
    
    all_stats.append(stats)
```

The PPO returns rewards and losses to the dictionary.
Let's write a function to display these values.
```python
stats = ppo_trainer.step(query_tensors, response_tensors, rewards)
ppo_trainer.log_stats(stats, batch, rewards)
```

Setting related to objective equals true, helps obtain statistics for optimizing the model parameters, such as model loss and value loss.
![[Pasted image 20250404010716.png|400]]

On the other hand, setting related to objects equals false, helps obtain additional metrics relevant to reinforcement learning, such as advantage estimates, and reward calculations.
![[Pasted image 20250404010754.png|400]]

## PPO loss and mean reward
Now it's time to save the model.
```python
model_dir = "ppo-good"
os.makedirs(model_dir, exist_ok=True)

model_1.save_pretrained(model_dir)
tokenizer.save_pretrained(model_dir)
```

Next, plot the graphs for PPO training loss and PPO mean reward over time.
```python
loss_values = [stat['ppo/loss/total'] for stat in all_stats]
reward_values = [stat['ppo/mean_scores'] for stat in all_stats]

# Plotting the loss
plt.figure(figsize=(12, 6))
plt.subplot(2, 1, 1)
plt.plot(loss_values, label='Total Loss', color='b')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('PPO Training Loss over Time')
plt.legend()
plt.grid(True)

# Plotting the rewards
plt.subplot(2, 1, 2)
plt.plot(reward_values, label='Mean Reward', color='g')
plt.xlabel('Epoch')
plt.ylabel('Reward')
plt.title('PPO Mean Reward over Time')
plt.legend()
plt.grid(True)

# Show the plots
plt.tight_layout()
plt.show()    
```

You can see that the loss decreases over time.
However, PPO mean reward increases over time.
![[Pasted image 20250404010958.png|500]]

## PPO with negative sentiment
Train the model zero with negative sentiment by setting the change score value to zero for comparison.
```python
model_0 = AutoModelForCausalLMWithValueHead.from_pretrained(model_name)

change_score = 1
```

## Generate text with PPO and reference model
Next to generate text using PPO and reference model, you should allocate the device per CPU and GPU availability.
```python
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
# Set the pipeline device
pipeline_device = 0 if device.type == "cuda" else -1

gen_kwargs = {"min_length": -1, "top_p": 1.0, "do_sample": True, "pad_token_id": tokenizer.eos_token_id}
```

Now, define and decode a function for tokenizing the input text, generates a response using the provided model.
You cannot use the text generation pipeline with the auto-model for causal LM with value head class.
```python
def generate_some_text(input_text,my_model):
# Tokenize the input text
    input_ids = tokenizer(input_text, return_tensors='pt').input_ids.to(device)
    generated_ids = my_model.generate(input_ids,**gen_kwargs )

    # Decode the generated text
    generated_text_ = tokenizer.decode(generated_ids[0], skip_special_tokens=True)

    return generated_text_
```

You can see the responses from the three models using the input text, such as Model 1, which is trained on positive sentiments and generates a positive response.
Model 0 is trained on the negative sentiments and has generated a negative response.
However, the reference model or the original model provides a neutral response.
![[Pasted image 20250404011403.png|500]]

Now let's compare Model 0 and Model 1.
The screen displays examples of the generated responses from the positive and negative sentiment models with their sentiment scores.
You can see that the positive model generates a positive response and scores.
In contrast, the negative model generates negative responses and scores.
![[Pasted image 20250404011421.png|500]]
## Recap
- The PPO config class is useful for specifying the model and learning rate for PPO training.
- PPO training updates the model using Kullback-Leibler divergence.
- The collateral function is vital for preparing data batches in a format suitable for the PPO trainer.
- The PPO trainer collects dialogue samples and optimizes the chatbot policy.
- In PPO, the list stats_all stores the training statistics for each batch.
- Upon plotting a graph of PPO loss and PPO mean reward over time shows that the PPO loss decreases and PPO mean reward increases over time.