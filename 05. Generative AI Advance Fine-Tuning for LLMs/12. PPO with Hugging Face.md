# PPO with Hugging Face

## Learning objective
- Describe the scoring function for sentiment analysis
- Explain the dataset and its tokenization using Hugging Face.

## Scoring function
The sentiment analysis for responses is useful as a scoring function, rewarding positive responses over negative ones.
A reward function in reinforcement learning with proximal policy optimization or PPO, provides feedback on the quality of the policy's actions.
It also evaluates the quality of generated responses from a generative model such as a chatbot.
Let's initialize a sentiment analysis pipeline using a pre-trained model, fine-tuned in the Internet Movie Database or IMDB reviews.
```python
sentiment_pipe = pipeline("sentiment-analysis", model="lvwerra/distilbert-imdb", device=device)
```

Now, apply the sentiment pipeline to show the results on two texts.
The sent_kwargs dictionary contains parameters for the sentiment analysis pipeline.
This dictionary specifies that all scores should be returned, the applied function is none, and the batch size is two.
Now run the pipeline object for the defined texts.
You can see the output as two corresponding negative and positive sentiment values for each text.
```python
sent_kwargs = {"top_k":None, "function_to_apply": "none", "batch_size": 2}

text = "this movie was really bad!!"
sentiment_pipe(text, **sent_kwargs)
```
![[Pasted image 20250404000627.png|400]]

The scores from the sentiment analysis pipeline evaluate the quality or relevance of generated responses, indicating the model's confidence in the likelihood of generating positive responses.
![[Pasted image 20250404000756.png|500]]

Now iterating over the pipe_outputs list, extracting the score from each output, converting the output into a tensor, and storing it in the rewards list would represent the model's confidence in the likelihood of generating positive responses to use them as rewards.
```python
rewards = [torch.tensor(output[1]["score"]) for output in pipe_outputs]
```
![[Pasted image 20250404000942.png|400]]

## Dataset and dataset tokenization.
The IMDB dataset contains 50,000 movie reviews.
However, let's only use the review text for analysis.
Elements from DS with a length of 200 or less are filtered out, keeping only those DS with a length greater than 200.
```python
dataset_name = "imdb"
ds = load_dataset(dataset_name, split = "train")
```
![[Pasted image 20250404001301.png|400]]

From the sequences, the length sampler helps to vary the text lengths for data processing, enhances model robustness, and simulates realistic training conditions.
It also ensures efficient training by managing text input lengths and maintaining performance.
The length sampler ranges between input minimum text length and input maximum text length.
```python
from trl.core import LengthSampler

ds = ds.rename_columns({"text": "review"})
ds = ds.filter(lambda x: len(x["review"]) > 200, batched=False)

input_min_text_length, input_max_text_length=2, 8
input_size = LengthSampler(input_min_text_length, input_max_text_length)
```

Next, load a pre-trained tokenizer associated with the causal LLM model, and set the padding token as the end-of-sentence or EOS token.
```python
tokenizer = AutoTokenizer.from_pretrained("lvwerra/gpt2-imdb")
tokenizer.pad_token = tokenizer.eos_token
```

Now tokenize the review text into the input IDs, truncate the tokenized sequence to the desired length, and assign it to the input IDs.
```python
def tokenize(sample):
	sample["input_ids"] = tokenizer.encode(sample["review"])[: input_size()]
	sample["query"] = tokenizer.decode(sample["input_ids"])
	return sample
```

After tokenization, you can see the sample text where input IDs and queries are created.
You can use this as the input to the model.
![[Pasted image 20250404001811.png|500]]

Let's combine all the steps discussed until now into a single function to build a dataset.
```python
def build_dataset(dataset_name="imdb", input_min_text_length=2, input_max_text_length=8,tokenizer=tokenizer):
    # load imdb with datasets
    ds = load_dataset(dataset_name, split="train")
    ds = ds.rename_columns({"text": "review"})
    ds = ds.filter(lambda x: len(x["review"]) > 200, batched=False)

    input_size = LengthSampler(input_min_text_length, input_max_text_length)

    def tokenize(sample):
        sample["input_ids"] = tokenizer.encode(sample["review"])[: input_size()]
        sample["query"] = tokenizer.decode(sample["input_ids"])
        return sample

    ds = ds.map(tokenize, batched=False)
    ds.set_format(type="torch")
    return ds
    
dataset = build_dataset()
```

The screen displays two differences in the dataset before and after processing it.
Two keys are added, and the number of rows has decreased since the text shorter than 200 characters is removed.
![[Pasted image 20250404001953.png|400]]

Here's an example from the dataset before and after cleaning and tokenizing it.
![[Pasted image 20250404002149.png|500]]
## Recap
- In the reward function, PPO provides feedback on the quality of actions taken by the policy.
- The sentiment analysis pipeline scores, evaluate the generated responses quality.
- The scores for the generated responses are extracted from the `pipe_outputs` list.
- The `lengthSampler` varies text lengths for data processing, enhances model robustness, and simulates realistic training conditions.