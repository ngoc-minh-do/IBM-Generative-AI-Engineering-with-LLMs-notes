# PPO with Hugging Face
Welcome to PPO with Hugging Face.
After watching this video, you'll be able to describe the scoring function for sentiment analysis and explain the dataset and its tokenization using Hugging Face.
Let's first learn about the scoring function.
The sentiment analysis for responses is useful as a scoring function, rewarding positive responses over negative ones.
A reward function in reinforcement learning with proximal policy optimization or PPO, provides feedback on the quality of the policy's actions.
It also evaluates the quality of generated responses from a generative model such as a chatbot.
Let's initialize a sentiment analysis pipeline using a pre-trained model, fine-tuned in the Internet Movie Database or IMDB reviews.
Now, apply the sentiment pipeline to show the results on two texts.
The sent_kwargs dictionary contains parameters for the sentiment analysis pipeline.
This dictionary specifies that all scores should be returned, the applied function is none, and the batch size is two.
Now run the pipeline object for the defined texts.
You can see the output as two corresponding negative and positive sentiment values for each text.
The scores from the sentiment analysis pipeline evaluate the quality or relevance of generated responses, indicating the model's confidence in the likelihood of generating positive responses.
Now iterating over the pipe_outputs list, extracting the score from each output, converting the output into a tensor, and storing it in the rewards list would represent the model's confidence in the likelihood of generating positive responses to use them as rewards.
Next is dataset and dataset tokenization.
The IMDB dataset contains 50,000 movie reviews.
However, let's only use the review text for analysis.
Elements from DS with a length of 200 or less are filtered out, keeping only those DS with a length greater than 200.
From the sequences, the length sampler helps to vary the text lengths for data processing, enhances model robustness, and simulates realistic training conditions.
It also ensures efficient training by managing text input lengths and maintaining performance.
The length sampler ranges between input minimum text length and input maximum text length.
Next, load a pre-trained tokenizer associated with the causal LLM model, and set the padding token as the end-of-sentence or EOS token.
Now tokenize the review text into the input IDs, truncate the tokenized sequence to the desired length, and assign it to the input IDs.
After tokenization, you can see the sample text where input IDs and queries are created.
You can use this as the input to the model.
Let's combine all the steps discussed until now into a single function to build a dataset.
The screen displays two differences in the dataset before and after processing it.
Two keys are added, and the number of rows has decreased since the text shorter than 200 characters is removed.
Here's an example from the dataset before and after cleaning and tokenizing it.
Let's recap.
In this video, you learned about the PPO and its process of generating responses and scores using Hugging Face.
In the reward function, PPO provides feedback on the quality of actions taken by the policy.
The sentiment analysis pipeline scores, evaluate the generated responses quality.
The scores for the generated responses are extracted from the pipe_outputs list.
The length sampler varies text lengths for data processing, enhances model robustness, and simulates realistic training conditions.