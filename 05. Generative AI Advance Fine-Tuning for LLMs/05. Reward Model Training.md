# Reward Model Training

## Learning objective
- Explain the reward model loss 
- demonstrate how to create and train a reward model.

## Introduction
The reward model loss is a discrepancy between the predicted responses and their scores generated by a model and the actual response and the score received from the modeled environment or system.

This is where reward model training has come in place.
Reward model training is an advanced technique that trains a model to identify desired outputs generated by another model and assigns scores to the outcome based on its relevance and accuracy.
This score helps fine-tune the generative model to produce more accurate outputs.
![[Pasted image 20250330232727.png|400]]

## Response evaluation
Let's understand how to train a language model to generate rewards effectively.
To do so, first train the scoring function.
Here, human evaluators assign scores to the responses.
However, it is challenging for humans to provide precise numerical scores.
The table displays that the first and second columns represent the queries and responses while the third represents the assigned scores.
Now, if you look at the rows, the pattern is consistent.
The first row represents the query, the second represents the higher-scoring response that is response A, and the third represents the low-scoring response that is response B.
However, it is much easier for humans to rank responses instead of assigning them to specific scores.
![[Pasted image 20250330232846.png|400]]

The second table shows that the responses are arranged based on scores without assigning them numerical scores.
This ranking approach simplifies the evaluation process and makes it easy for human evaluators to provide accurate feedback.
The pairwise ranking method leverages ranking multiple samples.
However, in this video, let's focus on two samples.
![[Pasted image 20250330232944.png|400]]

Here, the goal is to develop a reward function that holds the inequality such that the reward for the better response is higher than the reward for the bad response.
![[Pasted image 20250330233030.png|400]]

Now, adopt an OpenAI notation where `r` is a function of X and Y.
Here, X is a query, Ya is the chatbot's output, the positive response, and Yb is the negative response.
It is important to note that the hat notation on Y is deleted from here because Y is a random variable sampled from the decoder model or a person used to generate it.
The parameter ɸ represents the learnable parameters of the transformer model.
A decoder model like the bidirectional representation of transformers or BERT model, and generative pretrained transformers or GPT.
![[Pasted image 20250330233130.png|400]]

## Reward model loss
Let's understand how to generate reward model loss.
To understand this, let's take an example where X is a query, Ya is a good response and Yb is a bad response.
![[Pasted image 20250330233235.png|400]]

To generate a reward, insert X and Ya in the model.
The encoder model generates responses as contextual embeddings represented in blue.
Similar to the classification, pass the classification token output through a linear layer with 768 inputs and one output numerical value denotated as Za, represented as blue.
This represents that the estimated reward is a good example.
![[Pasted image 20250330233326.png|300]]

Now, repeat the process for the bad sample where the output embeddings are shown in red.
Here, the goal is to ensure that the reward for Ya that is Za is higher than the reward for Yb that is Zb.
![[Pasted image 20250330233349.png|400]]

## Reward model loss: Bradley-Terry model
Next, use the Bradley-Terry model to understand the reward loss model by generating the cost or loss function.
First find the parameters that ensure a good response receives a higher reward than a bad response.
Let's understand this better using a geometric argument similar to the support vector machines rather than a purely probabilistic one.

For example, consider a dataset.
First convert the comparison inequality into a difference emphasizing maximizing the gap between the rewards of the good and bad responses.
![[Pasted image 20250331225113.png|500]]
![[Pasted image 20250331225130.png|500]]

Next, apply the sigmoid function to interpret the difference as a probability that response A is better than response B.
![[Pasted image 20250331225148.png|500]]

Lastly, transform this into a minimization problem by multiplying it with -1.
This approach simplifies the parameter optimization, meaning that finding the optimal parameter value ɸ minimizes the difference.
However, reducing the negative sigmoid function helps achieve maximum rewards for a good response over a bad response.
![[Pasted image 20250331225204.png|500]]

Based on the parameter ɸ, the delta represents the difference between rewards for a good and bad response.
Additionally, delta measures the reward quality for a good response compared to a bad response for ɸ.
It means that as delta increases, the law should decrease since the reward function difference is increasing.
This approach is represented in tabular form.
The table displays the impact of different ɸ values on the reward difference delta and the corresponding loss function, the negative sigmoid of delta.
- The first column in the table represents different ɸ values.
- The second column shows delta values, the difference in rewards between the good and bad responses.
	The positive values in the table represent the preference for a good response.
- The third column shows the loss as a function of the delta.
	As the delta increases, the loss decreases.
Let's look at the graph where the X represents the delta and the Y shows the value of loss as a function to the delta.
To evaluate the loss for different ɸ values, use the values from the table and plot a graph.
The graph shows that as delta increases the loss decreases, indicating that when the loss is minimized, the delta has a higher value as expected.
![[Pasted image 20250331225443.png|500]]

To estimate the parameter, use the maximum likelihood estimation by multiplying each samples sigmoid output independently.
If you multiply this output by a -1, it converts the problem into a minimization represented by plotting a graph with ɸ on the x axis and the loss on the y axis.
![[Pasted image 20250331225623.png|500]]

To simplify this optimization, apply a log function directly to the likelihood and scale it using a -1, resulting in a negative log likelihood.
The log function is monotonically increasing and does not affect the location of the minimum.
The graph shows that both approaches have the same minimum.
The log function transforms the products into sums simplifying the differentiation process and gradient descent can help find the optimal parameters.
![[Pasted image 20250331225656.png|500]]
## Recap
- Reward model training is an advanced technique that trains a model to identify desired outputs generated by another model and assign scores to the outcome based on its relevance and accuracy.
- Training the scoring function helps generate rewards effectively.
- Generating reward model loss, the encoder model generates responses as contextual embeddings.
- Using the Bradley-Terry reward loss model, you can understand the reward loss model by generating the cost or loss function.