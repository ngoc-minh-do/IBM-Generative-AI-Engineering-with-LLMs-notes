# Large Language Models (LLMs) as Distributions
## Learning objective
- Describe the application of LLMs in distribution and sampling.
- Explain how the transformer model generates probabilities for different words 
- Describe how to generate parameters for changing sequences.

## Dataset
Let's look at the table representing different queries.
The questions in the table are simple, but queries can vary, including programming question answers or summarization.
![[Pasted image 20250403222903.png|400]]

Assume that you can train the reward function on a similar dataset.
X approximate D represents this distribution process from this dataset, meaning the random samples come from the given table.
![[Pasted image 20250403223024.png|400]]

## Sampling from a Causal LLM
To demonstrate how LLMs are recognized as distribution and explore sampling from it, a language model takes sample query X and generates a response Y, where Y is a random variable.
Consider a query, `which is the largest ocean?`
For this query, the model generates various responses based on the probability distribution such as the `Pacific Ocean`, followed by `Pacific Ocean is the largest ocean on Earth`.
`Pacific Ocean is 155 million square kilometers and Atlantic Ocean`.
![[Pasted image 20250403223239.png|300]]

You can represent this as Y distributed Pi Y/XY, where the policy Pi is a distribution.
However, let's focus on generating each token in Y.
![[Pasted image 20250403223324.png|]]

## Sampling
The transformer model generates probabilities for different words using the softmax function applied to the final layer for a query, `which is the largest ocean?` 
The bar graph represents the visualization of the possible words.
The x-axis denotes the possible word at the time Omega t, while the y-axis denotes the probabilities for each word.
Next, update the bar graph with the actual probabilities for each word.
For example, the softmax function calculates the probability for the word `Atlantic`, and you can update the bar graph accordingly.
Similarly for the next words, `Pacific` and `Indian`, the softmax function calculates the probabilities, and you can update the bar graph accordingly.
The ellipsis(...) indicate the rest of the words in the vocabulary for consideration.
![[Pasted image 20250403223504.png|500]]

At the time t, you have various realizations for Omega t, where words are selected based on their probabilities.
![[Pasted image 20250403223814.png|500]]

For example, realization 1 is `Pacific`, realization 2 is  `Pacific`, and realization 3 is `Atlantic`, and so on.
Counting realization for each word, you can find that the number of times a word occurs would be proportional to the distribution generated by the softmax function as shown in the table, where the first column is the word and the second column is the number of realizations for that word.
![[Pasted image 20250403223944.png|300]]

## Distribution of `t+1` based on `t`
Let's understand what happens when the value of the timestamp changes from t to t+1.
As the model generates tokens, the distribution at time t+1 depends on the previous values.
For the word `Atlantic`, the softmax function calculates the probability of the next word.
For example, if the word at a time t is specific, the probability changes accordingly and similarly for the word `Indian`.
This probability shows how the probability distribution at a time t+1 depends on the words and their probabilities at time t.
This relationship continues from t equals zero.
![[Pasted image 20250403224213.png|500]]

## Distribution of `t+1` and `t+2 based on `t`
As t increases, the distribution at time t+1 is influenced by the value t and earlier timestamps, leading to various possible sequences.
For example, `Pacific` leads to a specific distribution at timestamp t+2, similar to `Atlantic`, `Indian`, and `Arctic`.
Finally, when t receives larger sequences, you can visualize examples generated from the initial query such as the 
- `Pacific Ocean is the`.
- `Atlantic Sea is not`
- `Pacific Ocean EOS`
- `Indian Ocean if you`
- `what is your timeline`
![[Pasted image 20250403224559.png|300]]

## Example
Let's summarize the distribution process and focus on the working of the causal transformers.
Here, the input model is `the largest ocean is?` converted into tokenized word embeddings and passed through the transformer.
Select a random output instead of applying the argmax function to select the most likely sequence.

For example, you may select `Pacific` and other less likely words such as `Atlantic` or `Indian`.
![[Pasted image 20250403224947.png|500]]

Pass these words back to the model and repeat the process by selecting a random word from the probabilities generated by the model.
Here you can see the `ocean`, `sea`, and `lake`.
![[Pasted image 20250403224712.png|500]]

## Temperature parameter
Next is generation parameters, which can help change the sequences generated using LLMs.
Temperature is a hyperparameter and the softmax function that affects the probability distribution.
The temperature parameter controls the random distribution, meaning the higher Tau values make it more uniform and the lower Tau values make it less random.
![[Pasted image 20250403225216.png|500]]

Let's start with the softmax equation to see how different temperature values influence the probability distribution.
For example, calculate the probabilities at temperature 1 and observe the distribution.
![[Pasted image 20250403225326.png|500]]

Now increase the temperature to two, and the distribution becomes more uniform.
Additionally, increasing the temperature to five spreads the distribution extensively, showing higher randomness.
With the temperature of 10, the distribution becomes more flat or uniform.
![[Pasted image 20250403225405.png|500]]

Finally, at temperature of 100, the distribution is almost uniform, meaning every token is almost equally probable, indicating maximum randomness.

Each column in the given table shows several random sequence realizations.
The first row with the temperature of one shows each word similar meaning.
On the other hand, the second row with the higher temperature shows random words.
![[Pasted image 20250403225509.png|400]]

## Parameter K
Next is top K sampling with parameter K.
This method restricts the selection of the next token to the top K highest probability tokens.
For example, let's set K to 3.
First, compute the softmax values with the temperature of one and display the initial probability distribution for different words.
Next, apply top K sampling.
Select the top three highest probability tokens and filter the less preferred options.
Identify the top K indices and highlight the corresponding words.
Lastly, normalize the top K values to ensure the sum is one.
![[Pasted image 20250403225607.png|300]]

## Additional parameters
To generate sequences, look at other parameters such as beam search, top-p sampling, repetition penalty, and max and min tokens.
- The beam search parameter tracks and expands various top sequences at each step.
- The top-p sampling limits the sampling pool to the smallest tokens for which the cumulative probability exceeds the threshold p.
- The repetition penalty penalizes the repeated sequences of tokens to encourage diverse output and avoid repeated text generation.
- Lastly, the max and min tokens set the maximum or minimum number of tokens.
![[Pasted image 20250403225755.png|500]]

## Recap
- The sample query questions may provide various random responses based on the probability distribution.
- The transformer model generates probabilities for different words using the softmax function,
- selecting words at various timestamps and change the probability for those words.
- The generation parameters such as temperature, top K sampling, beam search, top-p sampling, repetition penalty, and max and min tokens help change the sequences generated using LLMs.