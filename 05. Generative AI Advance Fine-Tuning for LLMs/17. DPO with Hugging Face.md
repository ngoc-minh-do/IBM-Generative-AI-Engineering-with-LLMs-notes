# DPO with Hugging Face
Welcome to DPO with Hugging Face.
After watching this video, you'll be able to discover Hugging Face tools for direct preference optimization or DPO and introduce the libraries and resources available for model fine tuning.
You'll also implement steps for DPO fine tuning, such as pre processing data set, model creation, training, evaluation, and inference.
To fine-tune a language model via DPO is easier compared to proximal policy optimization or PPO.
There are two main steps when fine tuning with DPO.
First is data collection, where you'll gather a preference data set with positive and negative selected pairs of generation given a prompt.
The second is optimization, where you'll maximize the log likelihood of the DPO loss directly.
For DPO, let's use a dataset available on Hugging Face provided by Barra Home.
To begin, load the dataset using the following command.
This data set is divided into six splits, and each record within these splits has seven features.
You need three features, chosen, rejected, and prompt.
Essentially, the dataset provides a preferred response and a rejected response for each prompt.
To get a better understanding of the data set, you can inspect a sample record by executing this command.
This will display one entry from the dataset, showing the structure and the content of the records.
Before using the data set with the DPO trainer, you must reformat it.
Specifically, you need to extract the prompt and both the rejected and chosen responses to match the input requirements of the DPO trainer.
Define process function to remove the unwanted features and format the rejected as chosen responses.
Then using the Map method, apply the process function to the whole data set.
Finally, create the training and evaluation sets.
A sample record after pre processing the data set is shown here.
Let's look at the steps required to create and configure the model and tokenizer for your task.
First, start by loading the decoder GPT2 model.
This is done using the auto model for causal LM class from the Hugging Face transformers library.
Next, loader reference model, which is essentially another instance of the GPT2 model.
This is useful for preserving an unmodified version of the model for reference purposes.
To process the text data, you need a tokenizer.
Load the GPT2 tokenizer as follows, then configure the tokenizer by setting its pad token to the end of sequence or EOS token.
This ensures that padding is handled consistently.
For memory efficient fine tuning, you can integrate parameter efficient lower configurations.
This is applied to the attention parameters, speeding up training.
You can experiment with different weights and parameter values to optimize performance.
Let's define training arguments.
The hyperparameters are similar to other methods except for Beta.
The Beta parameter is the temperature parameter for the DPO loss, typically in the range of 0.1-0.5.
Next, define the DPO trainer.
The reference model is set to none as you are passing PEFT_config, meaning that it is the original model before adding adapter lower layers.
By running trainer.train, you can start training the model using the DPO method on the provided data set.
Now, let's plot the training loss of the model.
Training logs can be retrieved from the trainer.state, which is a JSON file.
It can be seen that training loss is decreasing during training.
To generate responses using the trained model, first, load the trained DPO model.
Also, load the original GPT2 model for comparison purposes.
You can also try the pipeline function GPT2 tokenizer.
Next, define the generation configuration for the DPO model.
For inferencing the model, define the input prompt for text generation is a higher octane gasoline better for your car.
Now encode the prompt using the tokenizer.
Then generate text using the DPO model, as well as the original GPT2 model.
Finally, decode the generated texts and print the results.
The results show that the DPO model can generate a more efficient and straightforward response.
Let's recap.
In this video, you learned that there are two main steps when fine tuning a language model with DPO, data collection, and optimization.
To fine tune a language model with DPO and Hugging Face, the first step is to preprocess the data set where you will reform at it, then define and apply the process function, and finally, create the training and evaluation sets.
The next step is to create and configure the model and tokenizer for your task.
Then you will define the training arguments and the DPO trainer.
Next, you upload the model's training laws to ensure that it is decreasing during training.
Finally, you will load the trained mo