# DPO with Hugging Face

## Learning objective
- Discover Hugging Face tools for direct preference optimization or DPO and introduce the libraries and resources available for model fine tuning.
- Implement steps for DPO fine tuning, such as pre processing data set, model creation, training, evaluation, and inference.

## Fine-tuning with DPO
To fine-tune a language model via DPO is easier compared to proximal policy optimization or PPO.
There are two main steps when fine tuning with DPO.
- First is data collection, where you'll gather a preference data set with positive and negative selected pairs of generation given a prompt.
- The second is optimization, where you'll maximize the log likelihood of the DPO loss directly.

## Dataset preprocessing
For DPO, let's use a dataset available on Hugging Face provided by Barra Home.
To begin, load the dataset using the following command.
```python
ds = load_dataset("BarraHome/ultrafeedback_binarized")
```
This data set is divided into six splits, and each record within these splits has seven features.
You need three features, chosen, rejected, and prompt.
Essentially, the dataset provides a preferred response and a rejected response for each prompt.
To get a better understanding of the data set, you can inspect a sample record by executing this command.
This will display one entry from the dataset, showing the structure and the content of the records.
```python
ds["train_prefs"][0].keys()
```
![[Pasted image 20250405125330.png|400]]

Before using the data set with the DPO trainer, you must reformat it.
Specifically, you need to extract the prompt and both the rejected and chosen responses to match the input requirements of the DPO trainer.
Define process function to remove the unwanted features and format the rejected as chosen responses.
```python
def process(row):
    # delete unwanted columns
    del row["prompt_id"]
    del row["messages"]
    del row["score_chosen"]
    del row["score_rejected"]
    # retrieve the actual response text
    row["chosen"] = row["chosen"][-1]["content"]
    row["rejected"] = row["rejected"][-1]["content"]

    return row
```

Then using the Map method, apply the process function to the whole data set.
Finally, create the training and evaluation sets.
```python
# Apply the data processing function to the dataset
ds = ds.map(
    process,
    num_proc=multiprocessing.cpu_count(),
    load_from_cache_file=False,
)

# Split the dataset into training and evaluation sets
train_dataset = ds['train_prefs']
eval_dataset = ds['test_prefs']
```

A sample record after pre processing the data set is shown here.
![[Pasted image 20250405125524.png|400]]

## Model creation
Let's look at the steps required to create and configure the model and tokenizer for your task.
First, start by loading the decoder GPT2 model.
This is done using the auto model for causal LM class from the Hugging Face transformers library.
Next, loader reference model, which is essentially another instance of the GPT2 model.
This is useful for preserving an unmodified version of the model for reference purposes.
To process the text data, you need a tokenizer.
Load the GPT2 tokenizer as follows, then configure the tokenizer by setting its pad token to the end of sequence or EOS token.
This ensures that padding is handled consistently.
```python
# Load GPT-2 model with the specified quantization configuration
model = AutoModelForCausalLM.from_pretrained("gpt2")

# Load a reference model with the same quantization configuration
model_ref = AutoModelForCausalLM.from_pretrained("gpt2")

# Load GPT-2 tokenizer
tokenizer = GPT2Tokenizer.from_pretrained("gpt2")

# Set the pad token to the end-of-sequence token
tokenizer.pad_token = tokenizer.eos_token
```
![[Pasted image 20250405125702.png|200]]

## PEFT configuration
For memory efficient fine tuning, you can integrate parameter efficient lower configurations.
This is applied to the attention parameters, speeding up training.
You can experiment with different weights and parameter values to optimize performance.
```python
# PEFT (Parameter-Efficient Finetuning) configuration
peft_config = LoraConfig(
        # The rank of the low-rank adaptation weights
        r=4,
        # The target modules to apply the low-rank adaptation to
        target_modules=['c_proj','c_attn'],
        # The task type for the low-rank adaptation
        task_type="CAUSAL_LM",
        # The scaling factor for the low-rank adaptation weights
        lora_alpha=8,
        # The dropout probability for the low-rank adaptation weights
        lora_dropout=0.1,
        # The bias mode for the low-rank adaptation
        bias="none",
)
```
![[Pasted image 20250405125753.png|200]]

## Training arguments
Let's define training arguments.
The hyperparameters are similar to other methods except for Beta.
The Beta parameter is the temperature parameter for the DPO loss, typically in the range of 0.1-0.5.
```python
# DPO configuration
from peft import get_peft_model
training_args = DPOConfig(
    # The beta parameter for the DPO loss function
    #beta is the temperature parameter for the DPO loss, typically something in the range of 0.1 to 0.5 . 
    beta=0.1,
    # The output directory for the training
    output_dir="dpo",
    # The number of training epochs
    num_train_epochs=5,
    # The batch size per device during training
    per_device_train_batch_size=1,
    # The batch size per device during evaluation
    per_device_eval_batch_size=1,
    # Whether to remove unused columns from the dataset
    remove_unused_columns=False,
    # The number of steps between logging training progress
    logging_steps=10,
    # The number of gradient accumulation steps
    gradient_accumulation_steps=1,
    # The learning rate for the optimization
    learning_rate=1e-4,
    # The evaluation strategy (e.g., after each step or epoch)
    evaluation_strategy="epoch",
    # The number of warmup steps for the learning rate scheduler
    warmup_steps=2,
    # Whether to use 16-bit (float16) precision
    fp16=False,
    # The number of steps between saving checkpoints
    save_steps=500,
    # The maximum number of checkpoints to keep
    #save_total_limit=2,
    # The reporting backend to use (set to 'none' to disable, you can also report to wandb or tensorboard)
    report_to='none'
)
```

## Defining DPO trainer
Next, define the DPO trainer.
The reference model is set to none as you are passing PEFT_config, meaning that it is the original model before adding adapter lower layers.
By running trainer.train, you can start training the model using the DPO method on the provided data set.
```python
# Create a DPO trainer
# This trainer will handle the fine-tuning of the model using the DPO technique
trainer = DPOTrainer(
        # The model to be fine-tuned
        model=model,
        # The reference model (not used in this case because LoRA has been used)
        ref_model=None,
        # The DPO training configuration
        args=training_args,
        # The beta parameter for the DPO loss function
       
        # The training dataset
        train_dataset=train_dataset,
        # The evaluation dataset
        eval_dataset=eval_dataset,
        # The tokenizer for the model
        tokenizer=tokenizer,
        # The PEFT (Parallel Efficient Finetuning) configuration
        peft_config=peft_config,
        # The maximum prompt length
        #max_prompt_length=512,
        # The maximum sequence length
        max_length=512,
    )

# Start the training process
trainer.train()
```

## Training loss
Now, let's plot the training loss of the model.
Training logs can be retrieved from the trainer.state, which is a JSON file.
It can be seen that training loss is decreasing during training.
```python
# Retrieve log_history and save it to a dataframe
log = pd.DataFrame(trainer.state.log_history)
log_t = log[log['loss'].notna()]

# Plot train and evaluation losses
plt.plot(log_t["epoch"], log_t["loss"], label = "train_loss") 
plt.legend() 
plt.show()
```
![[Pasted image 20250405130059.png|300]]

## Generation
To generate responses using the trained model, first, load the trained DPO model.
Also, load the original GPT2 model for comparison purposes.
You can also try the pipeline function GPT2 tokenizer.
Next, define the generation configuration for the DPO model.
```python
# Load the GPT-2 tokenizer
tokenizer = GPT2Tokenizer.from_pretrained('gpt2')

# Load the pre-trained GPT-2 model
gpt2_model = AutoModelForCausalLM.from_pretrained('gpt2')

# Define the generation configuration for the DPO model
# This sets the parameters for text generation
generation_config = GenerationConfig(
        # Use sampling to generate diverse text
        do_sample=True,
        # Top-k sampling parameter
        top_k=1,
        # Temperature parameter to control the randomness of the generated text
        temperature=0.1,
        # Maximum number of new tokens to generate
        max_new_tokens=25,
        # Use the end-of-sequence token as the padding token
        pad_token_id=tokenizer.eos_token_id
    )
```

## Inference
For inferencing the model, define the input prompt for text generation is a higher octane gasoline better for your car.
Now encode the prompt using the tokenizer.
Then generate text using the DPO model, as well as the original GPT2 model.
Finally, decode the generated texts and print the results.
The results show that the DPO model can generate a more efficient and straightforward response.
```python
# Define the input prompt for text generation
PROMPT = "Is a higher octane gasoline better for your car?"
# Encode the prompt using the tokenizer
inputs = tokenizer(PROMPT, return_tensors='pt')

# Generate text using the DPO model
outputs = dpo_model.generate(**inputs, generation_config=generation_config)
# Decode the generated text and print it
print("DPO response:\t",tokenizer.decode(outputs[0], skip_special_tokens=True))

# Generate text using the GPT-2 model
outputs = gpt2_model.generate(**inputs, generation_config=generation_config)
# Decode the generated text and print it
print("\nGPT2 response:\t",tokenizer.decode(outputs[0], skip_special_tokens=True))
```
![[Pasted image 20250405130314.png|500]]
## Recap
- Two main steps when fine tuning a language model with DPO 
	- data collection
	- optimization.
- To fine tune a language model with DPO and Hugging Face
	- Step 1: Data preprocessing
		- Reformat
		- Define and apply the process function 
		- Create the training and evaluation sets.
	- Step 2: create and configure the model and tokenizer for your task.
	- Step 3: Then you will define the training arguments and the DPO trainer.
	- Step 4: Plot the model's training loss to ensure that it is decreasing during training.
	- Step 5: load the trained model