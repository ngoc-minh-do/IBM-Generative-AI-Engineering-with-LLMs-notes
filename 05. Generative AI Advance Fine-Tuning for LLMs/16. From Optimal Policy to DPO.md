# From Optimal Policy to DPO
Welcome to from optimal policy to DPO.
After watching this video, you'll be able to train a generative causal LLM using direct preference optimization or DPO, derive the DPO objective and find an expression to maximize it.
You'll also use the Bradley-Terry model to understand loss and convert it to cost.
Reinforcement learning from human feedback or RLHF is an effective technique to optimize LLMs, but it poses challenges such as computational complexity, non differentiability, and instability.
DPO reformulates the problem to address these issues by leveraging a closed form optimal policy as a function of the reward.
To begin with, let's use a scoring function dataset to train the generative causal LLM using DPO.
Human evaluators assign scores to responses, but assigning precise numerical scores is challenging.
The table shows queries and responses in the first two columns and scores in the third.
The first row is the query, the second row is the higher scoring response, response A, and the third row is the lower scoring response, response B.
Ranking responses is easier than assigning scores.
The second table arranges responses by popularity without numerical scores, simplifying evaluation for human evaluators.
Your focus will be on pairwise ranking of two samples, but the method can extend to multiple samples.
For consistency, let's follow the same notion as the DPO literature.
WIN represents response A and L, loss represents response B.
Using sampling notation where D is the dataset, and the Tilda, that is the wavy line, represents sampling values.
X, Yw and Yl are drawn from the dataset as shown in the tables example.
In the original Bradley-Terry model, the loss is the log of the Sigmoid functions difference between the scores of the winning response and the losing response.
Using the sampling notation, transform this summation into an expected value over the dataset D.
Now let's focus on the argument within the log and Sigmoid functions, which essentially represent the loss function.
The difference between the scores of the winning and losing responses.
To solve the given direct preference optimization or DPO problem, you'll need to find the reward policy where Pi or policy r is the optimal solution.
Here, X is the query, Y is the response, and Z is the partition function.
Piref is the reference model, R is the reward function, and Beta is the regularization parameter.
The main issue is that you can't solve the partition function Z as it involves summing over every possible combination.
However, with some clever, yet simple math, you can eliminate the need to calculate the partition function and find a formulated cost function for your causal LLM.
This allows you to directly train your model based on the Bradley-Terry model forgoing the difficult training process of PPO.
Now you'll derive the DPO objective, starting with the optimal solution.
First, isolate the exponential term and multiply both sides by the partition function.
Then take the natural logarithm of both sides to linearize the exponential term and solve for the reward function.
Now you have the reward model in terms of the optimal solution.
Plug in the values of the equations for the positive and the negative samples, representing the winning responses in blue and the losing responses in red.
Recall the loss function for the Bradley-Terry model, subtracting the reward model for two samples by substituting these expressions into the equation not only eliminates the need for the partition function, but the new loss function shown here is now the function of the LLM and its reference model.
This eliminates the need for a separate reward function, giving an expression to maximize the DPO objective.
Let's simplify the expression step by step to better understand it.
First, start with the initial equation for the loss function for one sample output.
Next, set the Beta to one, simplifying the equation by removing the Beta scaling factor.
To further simplify, replace the reference model with the constant C, which means you are randomly selecting any word in the vocabulary with equal probability.
Using the laws of logarithms, you can combine the terms inside the logarithm.
Finally, set the argument of the logarithm to a single variable, U, representing the ratio of probabilities of the positive samples and the negative sample.
This final form shows the loss as a function of the logarithm of U.
Now, let's plot the loss as a function of u.
Starting with the initial equation, let's consider when the policy of the winning response, given the query, is less than the policy of the losing response.
If the policy of the winning response increases, but remains smaller than the losing response, this corresponds to the range 0-1.
Therefore, as U increases, the model gets better.
Plotting the loss as a function of U, you can see that as the probability of the winning response increases, the loss decreases.
When the policy of the winning response is larger than the policy of the losing response, U ranges from one to infinity, representing a higher policy of preferred completions.
As U increases, indicating a higher positive policy, the loss continues to decrease as shown in the corresponding section of the graph.
In a similar manner to the Bradley-Terry model, you can convert the loss to a cost.
Let's start with the initial equation.
Here, the loss function is expressed with the negative of the Sigmoid of the logarithms of the ratios of the winning and losing policies scaled by Beta.
Next, insert this expression into the Bradley-Terry model, this reformulates the equation to minimize the log likelihood transforming the loss into a cost.
To implement this Pi torch, you can write a loss function and calculate the loss or you can use hugging faces built in DPO trainer.
Let's recap.
In this video, you learned that, DPO leverages a closed form optimal policy as a function of the reward to reformulate the problem.
To solve the given DPO problem, you'll need to find the reward policy, which is given by the following expression.
Subtracting the reward model for two samples eliminates the need for the partition function, and the new loss function becomes a function of the LLM and its reference model.
The loss function is expressed with the negative of the Sigmoid of the logarithms of the ratios of the winning and losing policies scaled by Beta.
