# LoRA with Hugging Face and PyTorch

## Goal
- Describe how LoRA works with PyTorch and HuggingFace in large language models or LLMs.
- Explain various elements that help LoRA work with PyTorch and HuggingFace.

## LoRA
- LoRA is a lightweight training technique that significantly reduces the number of trainable parameters.
- LoRA inserts new small weights into the model to train them, making the training process faster and memory efficient.
- It also produces smaller model weights for easy storage and sharing.

## LoRA with PyTorch
### Dataset
The Internet Movie Database or IMDb dataset contains movie reviews from the IMDb.
These reviews are generally useful for binary sentiment classification.
```python
urlopened = urlopen('https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/35t-FeC-2uN1ozOwPs7wFg.gz')
tar = tarfile.open(fileobj=io.BytesIO(urlopened.read()))
tempdir = tempfile.TemporaryDirectory()
tar.extractall(tempdir.name)
tar.close()
```

You can view the class defined to traverse the IMDb dataset.
```python
class IMDBDataset(Dataset):
    def __init__(self, root_dir, train=True):
        """
        root_dir: The base directory of the IMDB dataset.
        train: A boolean flag indicating whether to use training or test data.
        """
        self.root_dir = os.path.join(root_dir, "train" if train else "test")
        self.neg_files = [os.path.join(self.root_dir, "neg", f) for f in os.listdir(os.path.join(self.root_dir, "neg")) if f.endswith('.txt')]
        self.pos_files = [os.path.join(self.root_dir, "pos", f) for f in os.listdir(os.path.join(self.root_dir, "pos")) if f.endswith('.txt')]
        self.files = self.neg_files + self.pos_files
        self.labels = [0] * len(self.neg_files) + [1] * len(self.pos_files)
        self.pos_inx=len(self.pos_files)

    def __len__(self):
        return len(self.files)

    def __getitem__(self, idx):
        file_path = self.files[idx]
        label = self.labels[idx]
        with open(file_path, 'r', encoding='utf-8') as file:
            content = file.read()
            
        return label, content
```

Next, let's use the IMDb dataset class to create iterators for training and testing datasets.
```python
root_dir = tempdir.name + '/' + 'imdb_dataset'
train_iter = IMDBDataset(root_dir=root_dir, train=True)  # For training data
test_iter = IMDBDataset(root_dir=root_dir, train=False)  # For test dataart=train_iter.pos_inx
```

This iterator converts the dataset to map style and performs a random split for training and validating them.
```python
# Convert the training and testing iterators to map-style datasets.
train_dataset = to_map_style_dataset(train_iter)
test_dataset = to_map_style_dataset(test_iter)

# Determine the number of samples to be used for training and validation (5% for validation).
num_train = int(len(train_dataset) * 0.95)

# Randomly split the training dataset into training and validation datasets using `random_split`.
# The training dataset will contain 95% of the samples, and the validation dataset will contain the remaining 5%.
split_train_, split_valid_ = random_split(train_dataset, [num_train, len(train_dataset) - num_train])
```

At this stage you can specify the device.
```python
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
device
```

### Data loader
Now, the collate_batch function customizes batches to create them from the individual samples.
```python
from torch.nn.utils.rnn import pad_sequence

def collate_batch(batch):
    label_list, text_list = [], []
    for _label, _text in batch:
        label_list.append(label_pipeline(_label))
        text_list.append(torch.tensor(text_pipeline(_text), dtype=torch.int64))

    label_list = torch.tensor(label_list, dtype=torch.int64)
    text_list = pad_sequence(text_list, batch_first=True)

    return label_list.to(device), text_list.to(device)
```

Convert the dataset objects to data loaders by applying the collate function with the batch size 64.
```python
BATCH_SIZE = 64

train_dataloader = DataLoader(
    split_train_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch
)
valid_dataloader = DataLoader(
    split_valid_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch
)
test_dataloader = DataLoader(
    test_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch
)
```

### Define TextClassifier class
Further, define a class to represent a simple text classifier, say text classifier.
It uses an embedding layer such as a hidden linear layer with rectified linear unit or ReLU activation and an output linear layer.
```python
from torch import nn

class TextClassifier(nn.Module):
    def __init__(self, num_classes,freeze=False):
        super(TextClassifier, self).__init__()
        self.embedding = nn.Embedding.from_pretrained(glove_embedding.vectors.to(device),freeze=freeze)
        # An example of adding additional layers: A linear layer and a ReLU activation
        self.fc1 = nn.Linear(in_features=100, out_features=128)
        self.relu = nn.ReLU()
        # The output layer that gives the final probabilities for the classes
        self.fc2 = nn.Linear(in_features=128, out_features=num_classes)

    def forward(self, x):
        # Pass the input through the embedding layer
        x = self.embedding(x)
        # Here you can use a simple mean pooling

        x = torch.mean(x, dim=1)
        # Pass the pooled embeddings through the additional layers
        x = self.fc1(x)
        x = self.relu(x)
        return self.fc2(x)
```

### TextClassifier to LoRA
Now, modify the hidden highlighted layer to convert it to a LoRA model.
![[Pasted image 20250326231815.png|300]]

### Define LoRA layer
Let's look at the LoRA layer class.
The LoRA layer class implements a LoRA module beginning with two low rank matrices such as A and B.

It further scales its product by a factor of alpha.
```python
class LoRALayer(torch.nn.Module):
    def __init__(self, in_dim, out_dim, rank, alpha):
        super().__init__()
        std_dev = 1 / torch.sqrt(torch.tensor(rank).float())
        self.A = torch.nn.Parameter(torch.randn(in_dim, rank) * std_dev)
        
        self.B = torch.nn.Parameter(torch.zeros(rank, out_dim))
        self.alpha = alpha

    def forward(self, x):
        x = self.alpha * (x @ self.A @ self.B)
        return x
```

The forward method performs matrix multiplication AÃ—BX and updates the low rank matrix by adding it to the original output.
![[Pasted image 20250326231857.png|400]]

During the training model you can update only A and B.
![[Pasted image 20250326142355.png|400]]

### Define LinearWithLoRA layer
However, the linear with LoRA class copies the original linear model and creates a LoRA layer object.
In the forward method, linear with LoRA class applies the original linear model and the LoRA model for the input x and adds the output together.
```python
class LinearWithLoRA(torch.nn.Module):
    def __init__(self, linear, rank, alpha):
        super().__init__()
        self.linear = linear.to(device)
        self.lora = LoRALayer(
            linear.in_features, linear.out_features, rank, alpha
        ).to(device)

    def forward(self, x):
        return self.linear(x) + self.lora(x)
```

Now, let's load a pre-trained model on the AG News dataset.
This model has four classes and a thorough language understanding because of its large sample size.
To initiate with this model, set num_classes to 4.
The pre-trained AG News model is trained with an unfrozen embedding layer.
Therefore, the model has initiated with `freeze = False`.
```python
from urllib.request import urlopen
import io

model_lora=TextClassifier(num_classes=4,freeze=False)
model_lora.to(device)

urlopened = urlopen('https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/uGC04Pom651hQs1XrZ0NsQ/my-model-freeze-false.pth')

stream = io.BytesIO(urlopened.read())
state_dict = torch.load(stream, map_location=device)
model_lora.load_state_dict(state_dict)

# Here, you freeze all layers:
for parm in model_lora.parameters():
    parm.requires_grad=False
model_lora
```

### Changing LoRA layers
The IMDb data set consists of two classes.
You can replace the final layer with a new linear layer where the number of outputs equals 2.
Next, replace the hidden layer with the LoRA layer, ensuring that A and B are updated during training.
```python
model_lora.fc2=nn.Linear(in_features=128, out_features=2, bias=True).to(device)
model_lora.fc1=LinearWithLoRA(model_lora.fc1, rank=2, alpha=0.1).to(device)
```
![[Pasted image 20250326233033.png]]

### Training LoRA
Now, let's fine-tune the model defining train model function.
```python
def train_model(model, optimizer, criterion, train_dataloader, valid_dataloader, epochs=100, model_name="my_modeldrop"):
    cum_loss_list = []
    acc_epoch = []
    best_acc = 0
    file_name = model_name
    
    for epoch in tqdm(range(1, epochs + 1)):
        model.train()
        cum_loss = 0
        for _, (label, text) in enumerate(train_dataloader):            
            optimizer.zero_grad()
            predicted_label = model(text)
            loss = criterion(predicted_label, label)
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)
            optimizer.step()
            cum_loss += loss.item()
        #print("Loss:", cum_loss)
        cum_loss_list.append(cum_loss)
        acc_val = evaluate(valid_dataloader, model, device)
        acc_epoch.append(acc_val)
        
        if acc_val > best_acc:
            best_acc = acc_val
            print(f"New best accuracy: {acc_val:.4f}")
            #torch.save(model.state_dict(), f"{model_name}.pth")
    
    #save_list_to_file(cum_loss_list, f"{model_name}_loss.pkl")
    #save_list_to_file(acc_epoch, f"{model_name}_acc.pkl")
```

First, set up the training components for the model and define the learning rate = 1.
Now, use a cross-entropy loss criterion, optimize the stochastic gradient descent or SGD, and use the learning rate scheduler to decay a factor of 0.1 at each epoch to train the model.
```python
LR=1

criterion = torch.nn.CrossEntropyLoss()
optimizer = torch.optim.SGD(model.parameters(), lr=LR)
scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)

model_name="model_lora_final2"
train_model(model, optimizer, criterion, train_dataloader, valid_dataloader, epochs=300, model_name=model_name)
```

### Loss vs accuracy
Next, evaluate the model's performance by plotting a graph loss and accuracy for each epoch and monitor the improvements in the model's performance.
```python
cum_loss_list=load_list_from_file(model_name.replace('_','-') + "-loss.pkl")
acc_epoch=load_list_from_file(model_name.replace('_','-') + "-acc.pkl")
plot(cum_loss_list,acc_epoch)
```
![[Pasted image 20250326233643.png|400]]

### Evaluate the model
You've evaluated the model's performance, defined the evaluation function, and received a model accuracy of 69% on the test data.
```python
def evaluate(dataloader, model, device):
    model.eval()
    correct = 0
    total = 0
    with torch.no_grad():
        for label, text in dataloader:
            label, text = label.to(device), text.to(device)
            outputs = model(text)
            _, predicted = torch.max(outputs.data, 1)
            total += label.size(0)
            correct += (predicted == label).sum().item()
    accuracy = 100 * correct / total
    return accuracy

evaluate(test_dataloader , model, device)
# 69.244
```

### Saving parameters 
Now, from model_LoRA.fc1, LoRA obtain the learnable parameters A, B, and alpha and save them to load the model in the future.
A and B have approximately 450 parameters.
Storing the entire linear layer would go up to 12,800 parameters, approximately 28 times more than the original.
```python
A=model_lora.fc1.lora.A
print("A",A)
print("\n Number of elements in the tensor A",A.numel())
torch.save(A, 'A.pth')

B=model_lora.fc1.lora.B
print("B",B)
print("\n Number of elements in the tensor B",B.numel())
torch.save(B, 'B.pth')

alfa_=model_lora.fc1.lora.alpha
torch.save(alfa_, 'alfa_.pth')
torch.save(model_lora.fc2.state_dict(), 'out_layer.pth')
```

### Load model using saved parameters
The primary advantage of LoRA is that it can fine-tune the model using parameters A, B, and alpha.
It also uses the output layers to classify examples.
```python
A = torch.load('A.pth')
print("A:",A.shape)

B = torch.load('B.pth')
print("B:",B.shape)

alfa_ = torch.load('alfa_.pth')
alfa_ 

model_load_lora = TextClassifier(num_classes=4,freeze=False)
model_load_lora.to(device)

urlopened = urlopen('https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/uGC04Pom651hQs1XrZ0NsQ/my-model-freeze-false.pth')

stream = io.BytesIO(urlopened.read())
state_dict = torch.load(stream, map_location=device)
model_load_lora.load_state_dict(state_dict)

model_load_lora
```

Next, let's load the model using saved parameters.
You can create the model object and pre-train the parameters to load them.
```python
model_load_lora.fc1=LinearWithLoRA(model_load_lora.fc1,rank=2, alpha=0.1)
model_load_lora.fc2=nn.Linear(in_features=128, out_features=2, bias=True).to(device)

model_load_lora.fc1.lora.A=A
model_load_lora.fc1.lora.B=B
model_load_lora.fc1.lora.alpha=alfa_ 
model_load_lora.fc2=output_layer
model_load_lora.to(device)
model_load_lora.eval()

evaluate(test_dataloader , model_load_lora, device)
# 69.18
```

### Use the model to predict
Further, the predicted function is defined using the model to illustrate the model's performance.
```python
def predict(text, model, text_pipeline):
    with torch.no_grad():
        text = torch.unsqueeze(torch.tensor(text_pipeline(text)),0).to(device)

        output = model(text)
        return imdb_label[output.argmax(1).item()]

article="""This was a lacklustre movie with very little going for it. I was not impressed."""
result = predict(article, model_load_lora, text_pipeline)
# negative review
```

## LoRA with HuggingFace
### Dataset
Let's look at LoRA with HuggingFace, which makes training the model easy.
LoRA with HuggingFace first loads the IMDb dataset to train the model.
```python
from datasets import load_dataset
imdb = load_dataset("imdb")
small_train_dataset = imdb["train"].shuffle(seed = 42)
small_test_dataset = imdb["test"].shuffle(seed = 42)
```

Here's an example of it.
![[Pasted image 20250327000139.png]]

### Tokenizer
Now, load the DistilBERT tokenizer and apply this tokenizer to create the input IDs and attention mask for the given text.
```python

from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased")

def preprocess_function(examples):
	return tokenizer(examples["text"], padding=True, truncation=True, max_length=512)

tokenized_train = small_train_dataset.map(preprocess_function, batched=True)
tokenized_test = small_test_dataset.map(preprocess_function, batched=True)
```

The screen displays the text after tokenization.
![[Pasted image 20250327000542.png|400]]

## Load model from HuggingFace
Further, load a bidirectional representation for transformers or a BERT-like model from the HuggingFace transformer library with two classes as the fine-tuning example.
![[Pasted image 20250327000637.png|200]]
```python
from transformers import AutoModelForSequenceClassification

model = AutoModelForSequenceClassification.from_pretrained("distilbert-base-uncased", num_labels=2)
```

### Define the TaskType
Next, define the task type to classify the reviews using the sequence classification type from the task type module.
```python
from peft import get_peft_model, LoraConfig, TaskType

task_type = TaskType.SEQ_CLS
```

## Define the configuration for LoRA
Further, define the configuration for LoRA and load the pre-trained classification model to define the configuration.
Set rank and LoRA scaling factor, including dropout for the larger models.
The parameter target_modules specifies the parts of the model that you want to update.
The LoRA with HuggingFace enhances the transformer parameters using parameter target modules.
Then apply LoRA to the given model using the specified configuration and initialize via the parameter-efficient fine-tuning or PEFT model.
```python
lora_config = LoraConfig(
	task_type=TaskType.SEQ_CLS,
	r=8,
	lora_alpha=16,
	lora_dropout=0.1,
	target_modules=['q_lin', 'k_lin', 'v_lin']
)

model = get_peft_model(model, lora_config)
```

## Set up the training arguments
Next, let's understand how to set the training arguments in the model.
To do so, use TrainingArguments, a class for encapsulating all the hyperparameters and configurations required for training the model.
This makes the training process simple and helps to manage various aspects of the training process.
```python
from transformers import TrainingArguments
import wandb

wandb.login()

training_args = TrainingArguments(
    output_dir="./results_lora",
    num_train_epochs=10,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=64,
    learning_rate=2e-5,
    evaluation_strategy="epoch",
    weight_decay=0.01
    report_to="wandb"
)
```

## Train the model
Now, let's train the model using a trainer.
The trainer class helps simplify the training and evaluation of transformer models.
It also handles the training loop and saves the model.
It means that you can train the model using the trainer.
```python
from transformers import Trainer

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_train,
    eval_dataset=tokenized_test,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics)

trainer.train()
```

Finally, plot a graph loss versus accuracy to validate the improvements in the model's performance.
```python
plt.plot(eval_accuracy, label = 'eval_accuracy')
```
![[Pasted image 20250327001832.png|400]]
## Recap
- In LoRA with PyTorch
	- the model uses the IMDb data set and the class to create iterators for training and testing datasets.
	- The collate_fn function collates and customizes batches from various samples.
	- The TextClassifier uses embedding layers to classify texts.
	- The LoRALayer class implements the LoRA module and pre-trains it.
	- The model is fine-tuned using the train_model function, evaluates the model's performance, and provides the model's performance accuracy.
- LoRA with HuggingFace simplifies the model training process.
	- The model uses the tokenizer to create input IDs and BERT-like models from the HuggingFace transformer library.
	- Define the task type using SequenceClassification type and define the configuration for LoRA.
	- TrainingArguments help train the arguments in the model, and trainers help the model to train the Trainer class.