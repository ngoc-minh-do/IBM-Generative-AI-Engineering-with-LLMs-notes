# Fine-Tuning with Hugging Face

## Goal
- Describe how to fine-tune your model with Hugging Face and PyTorch.
- Use the supervised fine-tuning trainer, or SFT Trainer, to fine-tune your model.

## Hugging Face
- Hugging Face is an open-source machine learning, or ML, platform 
- built-in transformers library for natural language processing, or NLP, applications.
- The platform allows users to share machine learning models and datasets and showcase their work.

## Defining the dataset
Hugging Face's built-in datasets can be loaded using the load_dataset function.
Let's load the Yelp review dataset which is a list like object consisting of user reviews and accompanying metadata from the Yelp platform.
Each review is a dictionary typically containing the text of the review itself with the key text and the star rating given by the user which ranges from 1 to 5 with the key label.
```python
from datasets import load_dataset
dataset = load_dataset("yelp_review_full")

dataset
```
![[Pasted image 20250324211523.png]]

## Tokenizing data
You can load a BERTtokenizer object to tokenize pad and truncate reviews which helps handle variable length sequences efficiently.
The tokenizer function extracts the text from the dataset example and applies the tokenizer.
You can then map this method to the dataset.
```python
# Instantiate a tokenizer using the BERT base cased model
tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")

# Define a function to tokenize examples
def tokenize_function(examples):
    # Tokenize the text using the tokenizer
    # Apply padding to ensure all sequences have the same length
    # Apply truncation to limit the maximum sequence length
    return tokenizer(examples["text"], padding="max_length", truncation=True)


# Apply the tokenize function to the dataset in batches
tokenized_datasets = dataset.map(tokenize_function, batched=True)
```

The result is that each sample text in the dataset has been tokenized, converting the text to token indices with the dictionary key input ids.
Additionally, other parameters associated with the BERT model such as attention masks have been generated for each sample.
![[Pasted image 20250324213240.png|400]]

As the model does not need text information, you can remove the text and rename the label key.
The data is then converted into PyTorch tensors.
The result is a set of tensors with the keys, labels, input ids, token type ids, and attention mask.
```python
tokenized_datasets['train'][0].keys()
# dict_keys(['label', 'text', 'input_ids', 'token_type_ids', 'attention_mask'])

tokenized_datasets = tokenized_datasets.remove_columns(["text"])
tokenized_datasets = tokenized_datasets.rename_column("label", "labels")
tokenized_datasets.set_format("torch")

tokenized_datasets['train'][0].keys()
# dict_keys(['labels', 'input_ids', 'token_type_ids', 'attention_mask'])
```

## DataLoader
Just like PyTorch, in Hugging Face, you can create a data loader object for train and test datasets.
This allows you to iterate over batches.
```python
from torch.utils.data import DataLoader

# Create a training data loader
train_dataloader = DataLoader(tokenized_datasets["train"], shuffle=True, batch_size=2)

# Create an evaluation data loader
eval_dataloader = DataLoader(tokenized_datasets["test"], batch_size=2)
```

## Loading the pretrained model
Now you are going to load a pre-trained BERT classification model with five classes from the transformer's library.
![[Pasted image 20250324213651.png|300]]

The module is designed for sequence classification.
The num labels parameter specifies the numbers of classes and determines the number of neurons in the final layer.
```python
from transformers import AutoModelForSequenceClassification

# Instantiate a sequence classification model
model = AutoModelForSequenceClassification.from_pretrained("bert-base-cased", num_labels=5)
```

## Optimizer and learning rate schedule
Lets create an optimizer and learning rate scheduler to fine-tune the model.
You can use the AdamW optimizer from PyTorch and set the device accordingly.
```python
import torch
from torchmetrics import Accuracy
from torch.optim.lr_scheduler import LambdaLR

# Define the optimizer
optimizer = AdamW(model.parameters(), lr=5e-4)

# Set the number of epochs
num_epochs = 10

# Calculate the total number of training steps
num_training_steps = num_epochs * len(train_dataloader)

# Define the learning rate scheduler
lr_scheduler = LambdaLR(optimizer, lr_lambda=lambda current_step: (1 - current_step / num_training_steps))

# Check if CUDA is available and set the device accordingly
device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")

# Move the model to the appropriate device
model.to(device)

```

## Training function
Now let's create a training function to fine-tune the model.
```python
def train_model(model,tr_dataloader):

    # Create a progress bar to track the training progress
    progress_bar = tqdm(range(num_training_steps))

    # Set the model in training mode
    model.train()
    tr_losses=[]
    # Training loop
    for epoch in range(num_epochs):
        total_loss = 0 
        # Iterate over the training data batches
        for batch in tr_dataloader:
            # Move the batch to the appropriate device
            batch = {k: v.to(device) for k, v in batch.items()}
            # Forward pass through the model
            outputs = model(**batch)
            # Compute the loss
            loss = outputs.loss
            # Backward pass (compute gradients)
            loss.backward()
            
            total_loss += loss.item()
            # Update the model parameters
            optimizer.step()
            # Update the learning rate scheduler
            lr_scheduler.step()
            # Clear the gradients
            optimizer.zero_grad()
            # Update the progress bar
            progress_bar.update(1)
        tr_losses.append(total_loss/len(tr_dataloader))
    #plot loss
    plt.plot(tr_losses)
    plt.title("Training loss")
    plt.xlabel("Epoch")
    plt.ylabel("Loss")
    plt.show()  

```

## Evaluate function
Here the evaluation function has been defined to evaluate the model's performance after fine-tuning it.
```python
def evaluate_model(model, evl_dataloader):
    # Create an instance of the Accuracy metric for multiclass classification with 5 classes
    metric = Accuracy(task="multiclass", num_classes=5).to(device)

    # Set the model in evaluation mode
    model.eval()

    # Disable gradient calculation during evaluation
    with torch.no_grad():
        # Iterate over the evaluation data batches
        for batch in evl_dataloader:
            # Move the batch to the appropriate device
            batch = {k: v.to(device) for k, v in batch.items()}

            # Forward pass through the model
            outputs = model(**batch)

            # Get the predicted class labels
            logits = outputs.logits
            predictions = torch.argmax(logits, dim=-1)

            # Accumulate the predictions and labels for the metric
            metric(predictions, batch["labels"])

    # Compute the accuracy
    accuracy = metric.compute()

    # Print the accuracy
    print("Accuracy:", accuracy.item())
```

## Train the model
You can now train the model and observe the loss reduction after each epoch.
```python
train_model(model=model,tr_dataloader=train_dataloader)
```
![[Pasted image 20250324214317.png|400]]

## SFT Trainer (supervised fine-tuning trainer)
SFT trainer, or supervised fine-tuning trainer, simplifies and automates many training tasks, making the process more efficient and less error-prone compared to training with PyTorch directly.
For the masked language model task your aim should be to predict a masked word using a transformer model.
![[Pasted image 20250324214408.png|300]]

## Loading the pretrained Masked Language Model
Lets load a masked language model and fine tune it using SFT trainer as shown here.
```python
from transformers import BertForMaskedLM

model = BertForMaskedLM.from_pretrained('bert-base-cased')
```

## Supervised fine-tuning with SFTTrainer
Here you will load the IMDB dataset which will be used to fine-tune the model.
Next you are going to define the training arguments object.
The training parameters object includes key parameters such as the learning rate and the number of epochs.
```python
from transformers import Trainer, TrainingArguments
from datasets import load_dataset

dataset = load_dataset("imdb", split="train")

training_args = TrainingArguments(
	output_dir="./trained_model",
	overwrite_output_dir=True,
	do_eval=False,
	learning_rate=5e-5,
	num_train_epochs=1,
	per_device_train_batch_size=2,
	save_total_limit=2
	logging_steps=20)
```

Finally, you will define the SFT trainer object.
The SFT trainer parameters include the model, training arguments, dataset, and the specific field key you would like to train on.
```python
from trl import SFTTrainer

trainer = SFTTrainer(
    model,
    args=training_args,
    train_dataset=dataset,
    dataset_text_field="text",
)
```

Lets train the model and see the loss for every epoch.
```python
trainer.train()
```
![[Pasted image 20250324215128.png|300]]

## Inference from the pretrained model
You can create a `pipeline` object `mask_filler` to make predictions.
The parameter `task` specifies the problem type.
The input includes the model and the tokenizer which allows you to tokenize the data and make a prediction in one step.
To make a prediction, simply input the text, `This is a [MASK] movie!`.
The output result will be an iterable object.
The key token STR will contain the predicted token value for the mask and the key score will indicate the likelihood.
The samples will be ordered based on the order of likelihood.
Observing the output, you can see that great is the most likely token.
```python
from transformers import pipeline

text = "This is a [MASK] movie!"
mask_filler = pipeline(task='fill-mask', model=model, tokenizer=tokenizer)

results = mask_filler(text)
for result in results:
	print(f"Predicted token: {result['token_str']}, Confidence: {result['score']: .2f}")
```
![[Pasted image 20250324215559.png|400]]

## Recap
- Hugging Face is an open source machine learning, or ML, platform with a built-in transformers library for natural language processing, or NLP, applications.
- Hugging Face's built-in datasets can be loaded using the load_dataset function.
- The tokenizer function extracts the text from the dataset example and applies the tokenizer.
- The num_labels parameter specifies the number of classes and determines the number of neurons in the final layer.
- The evaluation function evaluates the models performance after fine-tuning it.
- SFT trainer (supervised fine-tuning trainer):
	- simplifies and automates many training tasks
	- making the process more efficient and less error prone compared to training with PyTorch directly.
	- The SFT trainer parameters include:
		- model
		- training arguments
		- datasets
		- specific field key you would like to the train on.
