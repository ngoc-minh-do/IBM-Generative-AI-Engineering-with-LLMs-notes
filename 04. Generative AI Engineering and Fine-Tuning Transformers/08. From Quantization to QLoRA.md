# From Quantization to QLoRA

## Goal
- Define quantization and QLoRA.
- Explain the importance of QLoRA.

## QLoRA
Let's first understand what QLoRA is.
- QLoRA stands for quantized low-rank adaptation.
- It is a fine-tuning technique in machine learning designed to optimize the performance and efficiency of large language models or LLMs.
- You can achieve this by combining quantization in LoRA, allowing efficient use of computational resources without significantly sacrificing the model's accuracy.
![[Pasted image 20250328090615.png|400]]

## Quantization
Quantization reduces the precision of numerical values to a finite set of discrete levels, decreasing memory usage, and enabling efficient computation on hardware with limited precision.

For example, to convert the images color to 256 shades of gray, quantization reduces the number of gray color levels from 256 to 16, 8, 4, and 2.
These levels show a decrease in the image details by applying certain color levels.
This means that with the increasing quantization, the image looks less vibrant and remains recognizable with less memory.
![[Pasted image 20250328090826.png|200]]![[Pasted image 20250328090853.png|200]]

## Quantization range and levels
Let's define the quantization range and levels using the number range.
In QLoRA, the quantization range lies between -1 and 1.
The number of bits used to represent values determines the quantization levels.

- For example, 3-bit quantization represents eight discrete levels such as -1, -0.75, -0.5, -0.25, 0.25, 0.5, 0.75, and 1.
	Therefore, the quantized value for the numbers could be -0.75 to -0.75, -0.7 to -0.75, -0.2 to -2.50, 0.05 to 0.25, and 0.45 to 0.5.
- Similarly, 2-bit quantization represents four discrete levels such as 1, -1/2, 1/2, 1.
	Therefore, the quantized value for the example numbers could be -0.75 to -1, -0.7 to -1, -0.2 to -1/2, 0.05 to 1/2, and 0.45 to 1 /2.
![[Pasted image 20250328091143.png|400]]

## QLoRA
- QLoRA uses a unique quantization method, 4-bit normal float or NF4, and double quantization.
- It employs paged optimizers, not a quantization technique, but a memory management trick. Additionally, the paged optimizers allow large models to fit into limited memory by dynamically loading and unloading model parameters as per the requirements.
![[Pasted image 20250328091709.png|400]]

## Why quantization?
Let's understand how quantization can reduce the memory footprint in a 7 billion parameter model, considering model parameters, gradients, two optimizer states and activations.

Let's review each of them.

- In model parameters, quantization uses the FP16 model, which uses 16 bits to represent each parameter.
	Therefore, the total memory footprint for 7 billion parameters is 7 billion multiplied by two bytes, which equals 14 gigabytes.
- On the other hand, quantization uses FP16 gradients, which uses 16 bits to represent each gradient.
	The total memory footprint for 7 billion gradients is 7 billion multiplied by two bytes, which equals 14 gigabytes.
- Quantization uses FP32 optimizer states to reduce memory footprint, using 32 bits for each optimizer state.
	There are two states which leads to a total memory footprint of two times 7 billion multiplied by four bytes, which equals 56 gigabytes.
- Let's assume FP16 activations are the same size as the model parameters to reduce memory footprint quantization.
	This means that the memory footprint for the activations is 7 billion multiplied by two bytes, which equals 14 gigabytes.

=> Therefore, the total reduction in the memory footprint for FP16 is 98 gigabytes.
![[Pasted image 20250328091853.png|400]]

In contrast, a 4-bit model quantization uses four bits to represent each parameter.
- The total memory footprint for the 7 billion parameters is 7 billion multiplied 0.5 bytes, equals 3.5 gigabytes.
- The 4-bit quantized gradients use four bits to represent each gradient, which leads to the total memory footprint for the 7 billion gradients, being 7 billion multiplied 0.5 bytes, which equals 3.5 gigabytes.
- In contrast, the 8-bit quantized optimizer states use eight bits for each optimizer state.
	In this case, there are two.
	In this case, the total memory footprint for two states is two times 7 billion multiplied by one byte, which equals 14 gigabytes.
- Lastly, in 4-bit quantized activation, assume that the activations are the same size as the model parameters.
	The total memory footprint for the activations is 7 billion multiplied by four bits, which equals 3.5 gigabytes.

=> Therefore, the total reduction in memory footprint for 4-bit quantization is 24.5 gigabytes.
![[Pasted image 20250328093358.png|400]]

The table on the screen represents the comparison of the memory footprint before and after 4-bit quantization.
Therefore, the memory footprint reduces from 98 gigabytes, FP16 to 24.5 gigabytes, 4-bit quantization with a reduction of around 75%.
It is important to note that the reduction in the memory footprint leads to run models on less powerful hardware.
![[Pasted image 20250328093636.png|400]]

## Recap
- Quantization and QLoRA techniques is crucial for developing efficient and scalable machine learning models.
- QLoRA
	- is a fine-tuning technique in machine learning for optimizing performance.
	- QLoRA uses a unique quantization method, NF4 and double quantization to reduce memory footprints.
	- QLoRA reduces memory footprint up to 75%.
- Quantization
	- reduces the precision of numerical values to a finite set of discrete levels by defining the quantization range and levels.