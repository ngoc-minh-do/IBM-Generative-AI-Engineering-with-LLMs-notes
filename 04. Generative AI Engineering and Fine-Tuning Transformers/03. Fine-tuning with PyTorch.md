# Fine-tuning with PyTorch

## Goal
- Prepare the data set for loading and model definition
- describe how to fine tune the complete model and its final layer.

## Fine tuning
- Fine tuning in machine learning is the process of adapting a pre train model for specific tasks or use cases.
- It has become a fundamental deep learning technique, particularly in the training process of foundation models used for generative AI.

## Datasets
Let's look at the data sets you will be working with.
- First is the IMDB data set, which contains details of 50,000 samples of movie reviews.
	The data set is rather small and includes two classes, positive and negative.
- The second is the AG news data set, with the details of 120,000 training samples, 7,600 test samples, and four classes, world sports, business, and science and technology.

The goal is to pre-train the model on the AG news data set to build a robust understanding of language in context from diverse topics, and then fine tune the IMDB data set to specialize the model for sentiment analysis and move your reviews.

## Loading the dataset
Let's first define the class that you're going to use to load the IMDB data set.
```python
class IMDBDataset(Dataset):
    def __init__(self, root_dir, train=True):
        """
        root_dir: The base directory of the IMDB dataset.
        train: A boolean flag indicating whether to use training or test data.
        """
        self.root_dir = os.path.join(root_dir, "train" if train else "test")
        self.neg_files = [os.path.join(self.root_dir, "neg", f) for f in os.listdir(os.path.join(self.root_dir, "neg")) if f.endswith('.txt')]
        self.pos_files = [os.path.join(self.root_dir, "pos", f) for f in os.listdir(os.path.join(self.root_dir, "pos")) if f.endswith('.txt')]
        self.files = self.neg_files + self.pos_files
        self.labels = [0] * len(self.neg_files) + [1] * len(self.pos_files)
        self.pos_inx=len(self.pos_files)

    def __len__(self):
        return len(self.files)

    def __getitem__(self, idx):
        file_path = self.files[idx]
        label = self.labels[idx]
        with open(file_path, 'r', encoding='utf-8') as file:
            content = file.read()
        return label, content
```

Now load the IMDB data set into your test and train iterators.
Below are a few examples of the data set.
Zero indicates a bad review, and one indicates a good review.
```python
root_dir = tempdir.name + '/' + 'imdb_dataset'
train_iter = IMDBDataset(root_dir=root_dir, train=True)  # For training data
test_iter = IMDBDataset(root_dir=root_dir, train=False)  # For test data

start=train_iter.pos_inx
for i in range(-10,10):
    print(train_iter[start+i])
```
![[Pasted image 20250324135107.png|400]]

Next, define the tokenizer and load the glove and beddings.
```python
tokenizer = get_tokenizer("basic_english")
def yield_tokens(data_iter):
    """Yield tokens for each data sample."""
    for _, text in data_iter:
        yield tokenizer(text)
```

```python
class GloVe_override(Vectors):
    url = {
        "6B": "https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/tQdezXocAJMBMPfUJx_iUg/glove-6B.zip",
    }

    def __init__(self, name="6B", dim=100, **kwargs) -> None:
        url = self.url[name]
        name = "glove.{}.{}d.txt".format(name, str(dim))
        #name = "glove.{}/glove.{}.{}d.txt".format(name, name, str(dim))
        super(GloVe_override, self).__init__(name, url=url, **kwargs)

glove_embedding = GloVe_override(name="6B", dim=100)
```

## Loading the vocab
Now, build a vocabulary object from a pre trained glove word embedding model, assigning index values from glove and setting the default index to the token.
```python
from torchtext.vocab import GloVe,vocab
# Build vocab from glove_vectors
vocab = vocab(glove_embedding.stoi, 0,specials=('<unk>', '<pad>'))
vocab.set_default_index(vocab["<unk>"])

vocab_size=len(vocab)
# vocab_size: 40002
```

## Convertion to map-style datasets
You can use this code to convert the data set into map style data sets.
You can then perform a random split to create separate training and validation data sets and check the device.
```python
# Convert the training and testing iterators to map-style datasets.
train_dataset = to_map_style_dataset(train_iter)
test_dataset = to_map_style_dataset(test_iter)

# Determine the number of samples to be used for training and validation (5% for validation).
num_train = int(len(train_dataset) * 0.95)

# Randomly split the training dataset into training and validation datasets using `random_split`.
# The training dataset will contain 95% of the samples, and the validation dataset will contain the remaining 5%.
split_train_, split_valid_ = random_split(train_dataset, [num_train, len(train_dataset) - num_train])

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
device
```

## Data loader
Let's create the collate function that tokenizes the data set.
Converts the tokens to sequences of token indices and transforms these sequences and class labels into tensors.
```python
from torch.nn.utils.rnn import pad_sequence

def collate_batch(batch):
    label_list, text_list = [], []
    for _label, _text in batch:

        label_list.append(_label)
        text_list.append(torch.tensor(text_pipeline(_text), dtype=torch.int64))

    label_list = torch.tensor(label_list, dtype=torch.int64)
    text_list = pad_sequence(text_list, batch_first=True)

    return label_list.to(device), text_list.to(device)
```

The training validation, and test sets are then converted into data loader objects, which are processed by the collate function.
```python
BATCH_SIZE = 32

train_dataloader = DataLoader(
    split_train_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch
)
valid_dataloader = DataLoader(
    split_valid_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch
)
test_dataloader = DataLoader(
    test_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch
)
```

Here is an example from the created validation data loader object.
```python
label,seqence=next(iter(valid_dataloader))
label,seqence
```
![[Pasted image 20250324140227.png|400]]

## Define the text classifier model class
Now you are going to define the transformer based model class.
That is the encoder model class for classification and PyTorch.
This constructor initializes the test classifier with configurations such as the number of classes, vocabulary size, and transformer settings, like the number of heads and layers.
It also sets up the essential components, embeddings, positional encoding, transformer encoder, and a linear classifier for output.
The pre-trained word embeddings model, glove is used for the embedding layer.
```python
class Net(nn.Module):
    """
    Text classifier based on a pytorch TransformerEncoder.
    """
    def __init__(
        self,
        num_class,vocab_size,
        freeze=True,
        nhead=2,
        dim_feedforward=128,
        num_layers=2,
        dropout=0.1,
        activation="relu",
        classifier_dropout=0.1):

        super().__init__()

        #self.emb = embedding=nn.Embedding.from_pretrained(glove_embedding.vectors,freeze=freeze)
        self.emb = nn.Embedding.from_pretrained(glove_embedding.vectors,freeze=freeze)
        embedding_dim = self.emb.embedding_dim

        self.pos_encoder = PositionalEncoding(
            d_model=embedding_dim,
            dropout=dropout,
            vocab_size=vocab_size,
        )

        encoder_layer = nn.TransformerEncoderLayer(
            d_model=embedding_dim,
            nhead=nhead,
            dim_feedforward=dim_feedforward,
            dropout=dropout,
        )
        self.transformer_encoder = nn.TransformerEncoder(
            encoder_layer,
            num_layers=num_layers,
        )
        self.classifier = nn.Linear(embedding_dim, num_class)
        self.d_model = embedding_dim
```

The forward method applies embeddings to the input, adds positional encoding, passes the data through the transformer encoder, averages the data along the first dimension and finally classifies the data using the classifier.
```python
    def forward(self, x):
        x = self.emb(x) * math.sqrt(self.d_model)
        x = self.pos_encoder(x)
        x = self.transformer_encoder(x)
        x = x.mean(dim=1)
        x = self.classifier(x)

        return x
```

## Define the training function
The train model function trains a transformer model using the provided optimizer and loss criteria, iterating through the specified number of epoch.
It evaluates model performance on a validation data set, prints the loss per epoch and optionally saves the model and performance metrics if the validation accuracy improves.
```python
def train_model(model, optimizer, criterion, train_dataloader, valid_dataloader,  epochs=1000, save_dir="", file_name=None):
    cum_loss_list = []
    acc_epoch = []
    acc_old = 0
    model_path = os.path.join(save_dir, file_name)
    acc_dir = os.path.join(save_dir, os.path.splitext(file_name)[0] + "_acc")
    loss_dir = os.path.join(save_dir, os.path.splitext(file_name)[0] + "_loss")
    time_start = time.time()

    for epoch in tqdm(range(1, epochs + 1)):
        model.train()
        #print(model)
        #for parm in model.parameters():
        #    print(parm.requires_grad)
        
        cum_loss = 0
        for idx, (label, text) in enumerate(train_dataloader):
            optimizer.zero_grad()
            label, text = label.to(device), text.to(device)

            predicted_label = model(text)
            loss = criterion(predicted_label, label)
            loss.backward()
            #print(loss)
            torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)
            optimizer.step()
            cum_loss += loss.item()
        print(f"Epoch {epoch}/{epochs} - Loss: {cum_loss}")

        cum_loss_list.append(cum_loss)
        accu_val = evaluate_no_tqdm(valid_dataloader,model)
        acc_epoch.append(accu_val)

        if model_path and accu_val > acc_old:
            print(accu_val)
            acc_old = accu_val
            if save_dir is not None:
                pass
                #print("save model epoch",epoch)
                #torch.save(model.state_dict(), model_path)
                #save_list_to_file(lst=acc_epoch, filename=acc_dir)
                #save_list_to_file(lst=cum_loss_list, filename=loss_dir)

    time_end = time.time()
    print(f"Training time: {time_end - time_start}")
```

The function predict takes in the text and a text pipeline, which pre processes the text for machine learning.
It uses a pre-train model to predict the label of the text for text classification on a data set.
```python
def predict(text, text_pipeline, model):
    with torch.no_grad():
        text = torch.unsqueeze(torch.tensor(text_pipeline(text)),0).to(device)
        model.to(device)
        output = model(text)
        return imdb_label[output.argmax(1).item()]

predict("I like sports and stuff", text_pipeline, model)
```

## Define the evaluate function
Here, you are going to create a function to evaluate the model's accuracy on a data set.
```python
def evaluate(dataloader, model_eval):
    model_eval.eval()
    total_acc, total_count= 0, 0

    with torch.no_grad():
        for label, text in tqdm(dataloader):
            label, text = label.to(device), text.to(device)
            output = model_eval(text)
            predicted = torch.max(output.data, 1)[1]
            total_acc += (predicted == label).sum().item()
            total_count += label.size(0)
    return total_acc / total_count

evaluate(test_dataloader, model)
```

## Load the pretrained model on the AG News datasets
Now, let's fine tune the complete model.
First, create a model object and PyTorch with four neurons in the output layer.
Then using the `load_state_dict` method, load the parameters from a pre-trained model on the AG news data set.
This method loads the parameters into the model object, allowing you to fine tune it with any data set you choose.
```python
!wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/9c3Dh2O_jsYBShBuchUNlg/model-AG%20News%20small1.pth

file_name = "model_AG News small1.pth"
model_fine1 = Net(vocab_size=vocab_size, num_class=4).to(device)
model_fine1.load_state_dict(torch.load(file_name, map_location=device))
```

## Changing the output layer
As you know, the AG news data set has four classes.
Since you're fine tuning the pre-train model on the IMDB data set, which has two classes, you need to change the number of neurons in the final layer from 4 to 2.
It is important to note that you should always adjust the neurons in the final layer according to the number of categories in the data set you're fine tuning on.
![[Pasted image 20250324141447.png|400]]

## Fine-tune model pretrained on the AG News dataset
Now, let's define the loss function, optimizers and scheduler for fine tuning the model with the IMDB data set, then called the train model to finish the model.
This step is essentially identical to training a model from scratch.
```python
LR=1
criterion = torch.nn.CrossEntropyLoss()
optimizer = torch.optim.SGD(model_fine1.parameters(), lr=LR)
scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)
save_dir = ""
file_name = "model_fine1.pth"
train_model(model=model_fine1, optimizer=optimizer, criterion=criterion, train_dataloader=train_dataloader, valid_dataloader=valid_dataloader,  epochs=100,  save_dir=save_dir ,file_name=file_name )
```

## Loss versus accuracy
Here, you can see the loss versus accuracy of the model, which achieves approximately 90% accuracy on the validation data.
![[Pasted image 20250324141741.png|400]]

## Load the model pretrained on the AG News dataset
Now let's see what happens if you only fine tune the final layer of the model.
For this, you're going to use the same pre-train model on the AG news data set as before.
Freeze all layers of the model, as you only need to fine tune the final layer.
This is done by selecting the layer parameters and setting the requires grad attribute to false.
Freezing all layers except the final one speeds up training by reducing the computation and focusing optimization on fewer parameters.
The number of neurons in the output layer has also been changed from 4 to 2 since the IMDB data set has two categories in the predicted class.
```python
urlopened = urlopen('https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/9c3Dh2O_jsYBShBuchUNlg/model-AG%20News%20small1.pth')
model_fine2 = Net(vocab_size=vocab_size, num_class=4).to(device)
model_fine2.load_state_dict(torch.load(io.BytesIO(urlopened.read()), map_location=device))

# Freeze all layers in the model
for param in model_fine2.parameters():
    param.requires_grad = False

dim=model_fine2.classifier.in_features
model_fine2.classifier = nn.Linear(dim, 2)
model_fine2.to(device)
```

## Fine-tune last layer of the pretrained model
Let's define the parameters to retrain the model and fine tune it just for the final layer.
In this case, training the model is much quicker.
```python
LR=1
criterion = torch.nn.CrossEntropyLoss()
optimizer = torch.optim.SGD(model_fine2.parameters(), lr=LR)
scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)
save_dir = ""
file_name = "model_fine2.pth"
train_model(model=model_fine2, optimizer=optimizer, criterion=criterion, train_dataloader=train_dataloader, valid_dataloader=valid_dataloader,  epochs=100,  save_dir=save_dir ,file_name=file_name )
```

Here is the loss versus accuracy plot of the model.
You find tuned only on the final layer.
While training is much faster, the performance is significantly worse.
Therefore, you must trade off between fine tuning the entire model and certain key parameters.
![[Pasted image 20250324142128.png|400]]
## Recap
- Fine tuning in machine learning is the process of adapting a pre trained model for specific tasks or use cases.
- The collate function tokenizes the data set, converts the tokens to sequences of token indices and transforms these sequences and class labels into tensors.
- While defining the transformer based model class for classification and PyTorch, the constructor initializes the text classifier with configurations such as the number of classes, vocabulary size, and transformer settings, like the number of heads and layers.
- The forward method applies embeddings to the input, adds positional encoding, passes the data through the transformer encoder, averages the data along the first dimension, and finally classifies the data using the classifier.
- The `train_model` function trains a transformer model, using the provided optimizer and lost criterion, iterating through the specified number of epoch.
- If you fine tune the complete model, the model achieves approximately 90% accuracy on the validation data.
- If you only fine tune the final layer of the model, training is much faster but the performance is significantly worse.