# Using Pre-Trained TransformersÂ and Fine-Tuning

## Goal
- Review the uses of pre-trained transformer models
- describe fine-tuning and why it's necessary,
- explain the different approaches to fine-tuning

## Pre-trained transformer models
Transformer models like BERT, Llama, and GPT have revolutionized natural language processing or NLP with their attention-based architecture and ability to be pre-trained on large unlabeled text datasets.
The pre-training process allows these models to learn rich representations of language that can then be leveraged for a wide range of downstream NLP tasks.

## Why Fine-tuning
Training large language models or LLMs with billions of parameters is computationally expensive, requiring powerful hardware like graphics processing units or GPU's, and substantial training data.
The process is time consuming, taking weeks or months, and involves complex optimization over multiple epochs.
Setting up and maintaining the necessary infrastructure adds additional costs.
Overall, training LLMs demand significant computational resources, time, and investment.
![[Pasted image 20250323001739.png|400]]

## What is Fine-tunning
Fine-tuning LLMs adapt pre-trained models to specific tasks or domains using domain-specific data.
This process adjusts the model's parameters to improve task performance, leveraging pre-existing language understanding.
Fine-tuning enhances efficiency and saves time and computational resources compared to training models from scratch.
![[Pasted image 20250323001838.png|300]]

## Benefits of fine-tuning
Fine-tuning LLMs is valuable in transfer learning, especially with limited labeled data availability, providing time and resource efficiency by bypassing initial training stages and achieving faster conversions.
Fine-tuning allows you to tailor the model's responses to align with your specific requirements, ensuring that it produces accurate and contextually relevant outputs.
This task-specific adaptation is crucial for applications like sentiment analysis or text generation within diverse domains.
![[Pasted image 20250323002757.png|400]]

## Pitfalls in fine-tuning
To avoid pitfalls in fine-tuning LLMs, you should consider the following: 
- Overfitting: to prevent the model from performing well only on training data, you should avoid using a small dataset or extending training epochs excessively.
- Underfitting: you must ensure sufficient training and an appropriate learning rate to enable adequate learning.
- Catastrophic forgetting: prevent the model from losing its initial broad knowledge, which can hinder performance on various NLP tasks.
- Data leakage: keep training and validation datasets separate to avoid misleading metrics.
Addressing these issues can optimize fine-tuning for better performance and generalization on specific tasks or domains.
![[Pasted image 20250323002943.png|400]]

## Fine-tune: Question answering
Fine-tuning causal decoder models can appear straightforward by creating a dataset specific to the task.
For example, a question-answering bot designed to answer questions about cars can be retrained with a dataset about cars.
However, in practice, these methods require novel cost functions such as reinforcement learning, direct preference optimization, and training encoder models to evaluate the model.
![[Pasted image 20250323003109.png|400]]

## Response evaluation
Scoring large language models, that is, response evaluation can be challenging.
Humans excel at comparing two responses, but often struggle with assigning absolute scores.
For instance, given two responses to the question, `which country owns Antarctica?` One might say, `Antarctica is governed by the Antarctic Treaty System`, which is accurate and informative, while another might quip, `Our penguin overlords run the show down there`, which is humorous but incorrect.
While it's easy for humans to identify the first response as good and the second as bad, quantifying these judgments numerically is more complex.
![[Pasted image 20250323003241.png|400]]

This can be addressed by fine-tuning an LLM such as BERT that understands language to produce a single output analogous more to regression than classification using reward modeling.
As you can see here, the first response scores better than the second.
![[Pasted image 20250323003402.png|300]]![[Pasted image 20250323003426.png|300]]

## Self-supervised fine-tuning
There are three main approaches to fine-tuning language models.
Number one is self supervised fine-tuning.
Here, the model learns to predict missing words in a large unlabeled dataset such as next words or masked words.
![[Pasted image 20250323003520.png|400]]

## Supervised fine-tuning
Number two is supervised fine-tuning, where the model is fine-tuned using labeled data from the target task, improving its performance on specific tasks like sentiment classification.
![[Pasted image 20250323003603.png|400]]

## Reinforcement learning from human feedback (RLHF)
Number three is reinforcement learning from human feedback or RLHF.
In this technique, the model is adjusted based on explicit feedback from human annotators aligning its outputs with human preferences.
![[Pasted image 20250323003718.png|400]]

## Approaches of fine-tuning
These methods enable language models to adapt to specific tasks or domains, leveraging self-supervised learning, supervised learning, or human feedback.
### Hybrid fine-tuning
Hybrid fine-tuning combining multiple techniques can further enhance model performance.
Chat GPT, developed by OpenAI, is an example that utilizes such hybrid methods.
![[Pasted image 20250323003808.png|400]]

### Direct preference optimization (DPO)
Direct preference optimization (DPO) is an emerging popular approach that focuses on optimizing language models directly based on human preferences.

Features:
- Simplicity: DPO can be more straightforward to implement than RL.
- Human-centric optimization: DPO explicitly focuses on aligning model outputs with human preferences and judgments.
- DPO requires no reward training, that is, there's no need to train an additional reward model.
- Finally, DPO can achieve faster convergence due to its reliance on direct feedback.
![[Pasted image 20250323004029.png|400]]

## Ways to do supervised fine-tuning
Let's look at how supervised fine-tuning can be done.
There are two different ways: 
- Full fine-tuning, where all the parameters of the model are tuned for the specific task.
- The second is a more efficient way of fine-tuning foundation models called parameter-efficient fine-tuning (PEFT).
	In this methodology, large pre-trained models can be fine-tuned without modifying most of their original parameters.
![[Pasted image 20250323004237.png|400]]
## Recap
- Training large language models or LLMs with billions of parameters is computationally expensive, requiring powerful hardware like graphics processing units or GPOs and substantial training data.
- Fine-tuning LLMs:
	- adapts pre-trained models to specific tasks or domains using domain-specific data.
	- Fine-tuning enhances efficiency
	- Saves time and computational resources compared to training models from scratch.
- Benefits of fine-tuning:
	- Transfer learning
	- Time and resource efficiency
	- Tailored responses
	- Task-specific adaptation.
- Addressing issues can optimize fine-tuning for better performance and generalization on specific domains.
	- overfitting
	- underfitting
	- catastrophic forgetting
	- data leakage
- Three main approaches to fine-tuning language models
	- self-supervised fine-tuning
	- supervised fine-tuning
	- reinforcement learning from human feedback (RLHF).
- Direct preference optimization (DPO) is an emerging popular approach that focuses on optimizing language models directly based on human preferences.
- Supervised fine-tuning can be done in two different ways:
	- full fine-tuning
	- parameter-efficient fine-tuning (PEFT)