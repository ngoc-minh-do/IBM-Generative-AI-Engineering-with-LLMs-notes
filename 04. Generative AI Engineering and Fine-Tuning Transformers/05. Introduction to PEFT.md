# Introduction to PEFT (parameter-efficient fine-tuning)

## Goal
- Explain the concept of parameter-efficient fine-tuning, or PEFT, and its importance.
- Describe the types and uses of PEFT methods
- Recognize the concept of soft prompts and ranks.

## Supervised fine-tuning
- SFT, or supervised fine-tuning, is a method commonly used in machine learning, especially when working with pretrained models in transfer learning.
- This method involves acquiring the knowledge and understanding a model has gained from previous training and adapting it to a new task.

## Full fine-tuning
- One of the commonly used methods in supervised fine-tuning, full fine-tuning involves updating the learning parameters, layers and neurons of large language models or LLMs.
- It demands 
	- significant computational resources and memory 
	- Substantial task-specific labeled data.
- It also has a higher risk of overfitting, is time-consuming, and involves complex implementation 
- in addition, one inherent issue with full-fine tuning is catastrophic forgetting. Where the model forgets previously learned information upon being trained with new data, leading to a loss of valuable pretrained knowledge.
![[Pasted image 20250325094648.png|400]]

## Parameter-efficient fine-tuning (PEFT)
For this reason, parameter-efficient fine-tuning, or PEFT methods have been developed.
- PEFT methods reduce the number of trainable parameters that need to be updated to effectively adapt a large pretrained model to specific downstream applications.
- In doing so, PEFT significantly decreases the computational resources and memory storage needed to yield an effectively fine-tuned model.
- PEFT methods 
	- selective fine-tuning
	- additive fine-tuning
	- reparameterization fine-tuning.
![[Pasted image 20250325094827.png|400]]

These methods have often been demonstrated to be more stable than full fine-tuning methods particularly for NLP use cases.

## Full fine-tuning
Full fine-tuning, which involves updating the neural parameters, layers and neurons as shown in red and blue, demands significant computational resources and memory.
![[Pasted image 20250325101603.png|300]]

## Selective fine-tuning
On the contrary, selective fine-tuning which updates only a subset of layers or parameters works for other networks.
It is less effective for transformer architectures due to their higher number of parameters and the need for more extensive updates.
![[Pasted image 20250325101628.png|300]]

This limitation has led to the development of alternative methods, one of those methods is additive fine-tuning.

## Additive fine-tuning
Instead of modifying the existing pretrained parameters, this method involves adding new task-specific layers or components to the pretrained model.
You can then train these additional layers on task specific data while keeping the pretrained parameters fixed.
Additive fine-tuning allows for task-specific customization while preserving the pretrained knowledge.
You can inject additional layers anywhere in the model.
![[Pasted image 20250325101700.png|300]]

In transformers, adapters are used for additive fine-tuning.
They involve adding layers to a pretrained transformer model specifically between the attention blocks shown in green while keeping most of the models weights frozen.
![[Pasted image 20250325102011.png|200]]

If you examine the adapter layers, they start with a down projection layer shown in blue and yellow, which reduces the input dimension.
This is followed by a non linear transformation and then an up projection layer, also shown in blue and yellow.
Which restores the dimension back to that of the transformer, this design allows you to use an off the shelf transformer with only the adapters needing to be stored.
The transformer maintains a general understanding of language, while the adapters store information specific to a particular problem.
![[Pasted image 20250325102115.png|200]]

## Soft prompts
To train large pretrained language models, you require a significant amount of time and computational resources.
As these models increase in size, you can leverage soft prompts to:
- improve the training process.
- Soft prompts are learnable tensors concatenated with the input embeddings that can be optimized to a dataset.
- Soft prompt methods include prompt tuning, prefix tuning, p-tuning, and multitask prompt tuning.

## Prefix tuning and fine-tuning a medical chatbot
Let's take a closer look at prefix tuning, consider you are training the following decoder model from a general chatbot to a medical chatbot.
Instead of training the entire model on the dataset, you can simply append parameter embeddings to the existing embeddings.
As shown here, the original embeddings are in purple and the new embeddings are in red.
You'll then train the new model by freezing all the parameters except for the embeddings.
![[Pasted image 20250325155020.png|400]]

## Rank
Now, before learning about the most popular PEFT method, the reparameterization-based method lets review the concept of rank.

> Rank tells you the minimum number of vectors needed to span a space.

It is essentially what you commonly think of as a dimension, consider 2 vectors in a 2D space.
These 2 vectors can reach any point in that space, so the rank is 2.
![[Pasted image 20250325155601.png|300]]

Now lets say you extend these vectors into a 3D space.
Even though these 2 vectors now live in a 3D space, they can still only span a plane within the space.
Hence their rank remains two as they can only reach points in the 2D plane, not the entire 3D space.
![[Pasted image 20250325155800.png|300]]

- In neural networks, the input and output layers have fixed dimensions, however, you can use low-rank operations to reduce the number of parameters.
	As shown, you only need 2D to span the space even in a higher dimensional context.
- This reduction in dimensionality can help in making the model more efficient.

## Reparameterization-based methods
Reparameterization-based methods such as Low-rank Adaptation (LoRA) leverage the concept of re parametrizing network weights using low-rank transformations.
This reduces the number of trainable parameters while still working with high dimensional matrices like the pretrained parameters of the network.
![[Pasted image 20250325191147.png|400]]

In a typical network shown here, the forward method uses the full network.
![[Pasted image 20250325191244.png|300]]

However, in loRA, additional low-rank layers are added to the original layer as shown, reducing the number of parameters needed to represent the weight matrices.
This reparameterization effectively captures the most important directions in the data.
Maintaining the models performance while significantly reducing computational costs.
![[Pasted image 20250325191309.png|300]]

Other popular methods similar to LoRA include quantized low-rank adaptation (QLoRA) and weight decomposed low-rank adaptation (DoRA).
- QLoRA combines low-rank adaptations with quantization, reducing the memory footprint and computational requirements of the model.
- While DoRA adjusts the rank in the low-rank space based on the magnitude of the components optimizing the models performance and efficiency.
![[Pasted image 20250325191414.png|400]]
## Recap
- Parameter efficient fine-tuning (PEFT) methods:
	- Reduce the number of trainable parameters that need to be updated to effectively adapt a large pretrained model to specific downstream applications.
	- Include selective fine-tuning, additive fine-tuning, and reparameterization fine-tuning.
- Selective fine-tuning:
	- Updates only a subset of layers or parameters that work for other networks.
	- Less effective for transformer architectures due to their higher number of parameters and the need for more extensive updates.
- Additive fine-tuning:
	- involves adding task-specific layers or components to the pretrained model instead of modifying the existing pretrained parameters.
- Adapters are used for additive fine-tuning and involve adding layers to a pre trained transformer model, specifically between the attention blocks while keeping most of the models weights frozen.
- Soft prompts are learnable tensors concatenated with the input embeddings that can be optimized to a dataset.
- Rank tells you the minimum number of vectors needed to span a space.
- Reparameterization-based methods:
	- leverage the concept of reparametrizing network weights using low-rank transformations.
	- This reduces the number of trainable parameters while still working with high dimensional matrices like the pretrained parameters of the network.