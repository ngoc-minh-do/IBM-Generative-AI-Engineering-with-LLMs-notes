# Low-Rank Adaptation (LoRA)
## Goal
- Define LoRA 
- Explain working of LoRA

## LoRA
- LoRA simplifies the large and complex machine learning model suitable for specific uses.
- LoRA works by adding lightweight plug-ins to the original model for efficient functioning.

## Importance of LoRA
- LoRA enables reduced trainable parameters
- leveraging pre-trained models with high dimensional matrices, such as the network's pre-trained parameters.
- LoRA decreases the training time, resource usage, and memory footprint.

## Understanding LoRA
Let's understand this with an example.
Consider the third layer of the network displayed on the screen, with the input directions of 10 and 8 neurons, which equals 80 parameters.
Now, consider a layer with an input dimension of 10 and 3 neurons, which leads to 30 parameters.
Next, map the three dimensional output to 8 neurons with 3 dimensions, each with 3 parameters, leading to 24 parameters.
This approach reduces the number of trainable parameters to 54 to maintain the model performance.
It is interesting to note that LoRA, operates in the same way.
However, bias is not included for simplicity.
![[Pasted image 20250326134739.png|400]]

Let's understand LoRA using the matrix algebra.
The original neural network layer has a weight matrix W_0 with dimensions d by k, where d is the input size, and k is the output size.
The layers output gets computed as a function of h(x)=W_0x, x is the input from the last layer.
Hence, the result in parameter is d * k.
In matrix algebra, to remain consistent with the original LoRa layer, let's use x and not be confused with the feature.
![[Pasted image 20250326134938.png|400]]

Next, LoRA, while predicting the forward step, an additional matrix delta W is added to the original weight matrix W_0.
Therefore, the new weight matrix is W=W_0 plus delta W, and the layers output becomes a function h of x, which equals the quantity (W_0 + delta W)x.
Next, upon decomposing the LoRa update, the matrix delta W decomposes into two low rank matrices, B and A, meaning delta W=B * A.
A and B are the additional layers that map from a simpler to a larger dimension.
The dimensions of B are `d x r`, and that of A is `r x k`, where `r` is the rank.
![[Pasted image 20250326135537.png|400]]

## Value of rank `r`
The value of rank `r` should be smaller than `d` and `k`.
Let's understand this with an example.
Consider that r is a hyper parameter, and in LoRa, delta W is a product of B and A matrices.
![[Pasted image 20250326142100.png|400]]

Now upon setting r=1, the resultant matrices B and A have just one dimension, but the product size remains fixed.
![[Pasted image 20250326142115.png|400]]

The increase in r increases the number of parameters and keeps higher model capacity, keeping the product size constant.
![[Pasted image 20250326142130.png|400]]

Next, upon fine tuning with LoRA, the original weight matrix W_0 remains frozen and isn't updated during training.
This approach significantly reduces the number of learnable parameters.
In the original neural network, the number of learnable parameters is d * k.
And with LoRA, the number of learnable parameters reduces to d * r + r * k, which is much smaller than d * k.
This reduction occurs because the training updates only the low rank matrices, B and A, instead of the full weight matrix W.
Hence, this method saves computation and memory during the backward pass.
![[Pasted image 20250326142355.png|400]]

## Optimize LoRA
Let's understand how to optimize LoRA.
In the forward step of LoRa, apply scaling factor by multiplying Alpha divided by the rank r, both being hyperparameters.
![[Pasted image 20250326142900.png|400]]

The displayed loss function involves x as the input sequence and y as the target.
The loss function depends on matrices A and B.
![[Pasted image 20250326142943.png|400]]

Meaning only the delta W is updated using a gradient descent algorithm, typically Adam.
![[Pasted image 20250326143022.png|300]]

## LoRA for transformers
LoRA is a general framework for optimizing parameters, and it is useful for transformers having multiple parameters, specifically in the attention layers.
Moreover, it optimizes the key query and value parameters represented by delta theta.
![[Pasted image 20250326215938.png|400]]
![[Pasted image 20250326220000.png|400]]

It means that loss function, delta theta can apply to encoders and decoders.
It does not necessarily have to be cross entropy loss.
![[Pasted image 20250326220021.png|400]]

Following the notation from the original LoRA paper, The log likelihood acts as a function of the LoRa parameters.
Therefore, the summation over t is for each prediction in the auto regressive sequence, and the second summation is over each sample.
Here, omega represents the tokens, and x represents the corresponding embeddings.
![[Pasted image 20250326220041.png|400]]

## Recap
- LoRA helps complex machine learning for specific uses by adding lightweight plug-in components to the original model.
- It reduces the number of trainable parameters by leveraging pre-trained models and matrix algebra to decompose weight updates into low rank matrices.
- During the forward pass, LoRA adds an extra matrix delta W to the original weight matrix, enabling computation with fewer parameters by maintaining model performance.
- The original weight matrix remains frozen in the fine tuning process, and only low rank matrices are updated.
- Lastly, LoRA is applicable for transformers, optimizing key, query, and value parameters in attention layers, and applied to encoders and decoders to save computation and memory during the training and storage.