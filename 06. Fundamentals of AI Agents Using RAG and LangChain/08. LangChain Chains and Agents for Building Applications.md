## Learning objective
- Describe chains in LangChain for generating responses 
- Describe how LangChain stores memory.
- Define agents used in LangChain.

## Introduction
LangChain is a platform embedded with APIs to develop applications, empowering them to infuse language processing capabilities.
Therefore, developers find LangChain suitable for building applications.
LangChain uses certain tools, including documents, chains, and agents for building applications.
![[Pasted image 20250405212646.png]]

## Chain
In LangChain, chains are a sequence of calls.
A sequential chain consists of basic steps where each step takes one input to generate one output to create one seamless flow of information.
The output from Step 1 becomes the input for Step 2.
![[Pasted image 20250405212713.png]]

Let's look at the creation of a sequential chain of three individual chains.
The aim of this chain is to identify the recipe and the estimated cooking time for the famous dish available in the inserted location.
The users leverage 
- Chain 1 for selecting the geographic region to get the famous dish in that location
- Chain 2 for providing the recipe
- Chain 3 for estimating the cooking time.
![[Pasted image 20250405212745.png]]

Chain 1 in the sequence uses a user's prompt as input for a specific dish based on the user specified location.
For example, China.
Therefore, the output for Chain 1 should be a famous dish in China, Peking Duck.
![[Pasted image 20250405212856.png]]

Let's see how to use the code to create this chain.
First, define a template string for the prompt.
Ask for a specific dish from a specified location.
Create a prompt template object using the defined template, specifying the input variable as location.
Then create an LLM chain object named location chain, using LLM-based language model, such as Mixtral LLM.
It means that before running this code, assume that chat model object is available.
Therefore, the output will be stored under the key meal.
```python
from langchain.chains import LLMChain

template = """Your job is to come up with a classic dish from the area that the users suggests.
                {location}
                
                YOUR RESPONSE:
"""
prompt_template = PromptTemplate(template=template, input_variables=['location'])

# chain 1
location_chain = LLMChain(llm=mixtral_llm, prompt=prompt_template, 
output_key='meal')

location_chain.invoke(input={'location':'China'})
```

Let's look at Chain 2.
In the second chain of our sequential setup, use the output from the first chain, that is, the name of the dish is the input.
The output from this chain will be the recipe itself.
![[Pasted image 20250405213029.png]]

Let's look at the code.
First, define the template before asking for a simple recipe for a given meal.
Next, create a prompt template with meal as the input variable.
Finally, create an LLM chain named dish_chain, using Mixtral LLM and prompt template with the output key recipe.
```python
from langchain.chains import SequentialChain

template = """Given a meal {meal}, give a short and simple recipe on how to make that dish at home.

                YOUR RESPONSE:
"""
prompt_template = PromptTemplate(template=template, input_variables=['meal'])

# chain 2
dish_chain = LLMChain(llm=mixtral_llm, prompt=prompt_template, output_key='recipe')
```

Next is Chain 3.
Take the recipe obtained from the second chain as the input.
This chain is designed to estimate the cooking time for the meal based on the recipe.
![[Pasted image 20250405213148.png]]

Like Chain 2, define a template to estimate the cooking time for a given recipe.
Next, create a prompt template with recipe as the input variable.
Lastly, create an LLM chain named recipe_chain using Mixtral LLM and prompt template with the output_key, time.
```python
template = """Given the recipe {recipe}, estimate how much time I need to cook it.

                YOUR RESPONSE:
"""
prompt_template = PromptTemplate(template=template, input_variables=['recipe'])

# chain 3
recipe_chain = LLMChain(llm=mixtral_llm, prompt=prompt_template, output_key='time')
```

Now using three chains, the overall setup is a sequential chain that wraps all the individual chains together, creating a unified process.
By invoking the query through this combined chain, you can trace the flow of information from start to end.
You can set the verbose option to true to view the overall output.
This provides a clear and detailed view of how each input is transformed through chain into the final output.
```python
# overall chain
overall_chain = SequentialChain(
	chains=[location_chain, dish_chain, recipe_chain],
	input_variables=['location'],
	output_variables=['meal', 'recipe', 'time'],
	verbose= True
)

pprint(overall_chain.invoke(input={'location':'China'}))
```

Here's an example of the output.
![[Pasted image 20250405213418.png]]

## Memory
Do you know how memory is stored in the LLM applications? 
In LangChain, memory storage is important for reading and writing historical data.
Each chain relies on specific input, such as user and memory.
![[Pasted image 20250405213505.png]]

Chain reads from memory to enhance user inputs before executing its core logic and writes the current runs inputs and outputs back to the memory after execution.
This ensures continuity and context preservation across interactions.
![[Pasted image 20250405213541.png]]

The chat message history class in LangChain is designed to manage and store conversation histories effectively, including human messages and AI messages.
This allows adding messages from the AI and users to the history.
In this example, call a ChatMessageHistory class and add AI message hi into the memory.
The memory will append this AI message as input.
Now, add the user's message, what is the capital of France, and the memory will append this as human message input.
You would receive responses based on the stored memory.
```python
from langchain.memory import ChatMessageHistory

chat = mixtral_llm

history = ChatMessageHistory()

history.add_ai_message("hi!")

history.add_user_message("what is the capital of France?")
```
![[Pasted image 20250405213646.png]]

## Agents
Now, let's understand agents in LangChain.
Agents in LangChain are dynamic systems where a language model determines and sequences actions such as pre-defined chains.
The model generates text outputs to guide actions, but does not execute them directly.
However, agents integrate with tools such as search engines, databases, and websites to fulfill user requests.
![[Pasted image 20250405213710.png]]

For example, if a user asks for `the population of Italy`, the agent uses the language model to find options, query a database for details, and return a curated list.
This shows the agent's ability to autonomously leverage LLM reasoning with external tools.
![[Pasted image 20250405213738.png]]

In this example, let's create a Pandas DataFrame agent using LangChain.
This agent allows users to query and visualize data with natural language.
To set it up, instantiate the create_pandas_dataframe_agent class.
In this class, pass the LLM chat model in the data frame.
Next, set the verbose to true to see how the LLM thinks.
Finally, use the invoke code to execute the query, how many rows in the data frame.
The LLM transforms queries into Python code, executed in the background, enabling precise answers to the number of rows of the data frame.
For example, the response shows there are 139 rows in the DataFrame.
```python
df = pd.read_csv(
    "https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/ZNoKMJ9rssJn-QbJ49kOzA/student-mat.csv"
)

agent = create_pandas_dataframe_agent(
    mixtral_llm,
    df,
    verbose=True,
    return_intermediate_steps=True
)

response = agent.invoke("How many rows in the dataframe?",handle_parsing_errors=True)

print(response['output'])
# there are 139 rows in the DataFrame
```
## Recap
- LangChain is a platform that embeds APIs for developing applications.
- Chains
	- is the sequence of calls.
	- In chains, the output from one step becomes the input for the next step.
	- In LangChain, chains first define the template string for the prompt, then creates a prompt template using the defined template, and creates an LLM chain object name.
- In LangChain, memory storage is important for reading and writing historical data.
- Agents
	- are dynamic systems where a language model determines and sequences actions such as pre-defined chains.
	- Agents integrate with tools such as search engines, databases, and websites to fulfill user requests.