# RAG, Encoders, and FAISS

## Learning objective
- Describe context encoders and how they work.
- Explain Facebook AI similarity search or Faiss
- Explain question encoders
- Discuss how you can generate answers.

## Introduction
Imagine you're an AI engineer tasked with designing a chatbot to handle the ever changing landscape of company policies.
You might consider leveraging retrieval augmented generation also known as RAG.
This approach combines the power of a language model with real time information retrieval, enabling the chatbot to provide up to date responses without frequent retraining.
By integrating RAG, your chatbot can dynamically query an updated database of company policies and generate responses that reflect the most current information.
This enhances the chatbots accuracy and relevance and reduces maintenance overhead, making it an ideal solution for managing fluid corporate environments.
![[Pasted image 20250405142626.png|400]]

## RAG process
Lets break down the RAG process.
The retriever encodes user provided prompts and relevant documents into vectors, stores them in a vector database.
And retrieves relevant context vectors based on the distance between the encoded prompt and documents.
The generator then combines the retrieved context with the original prompt to produce a response.
![[Pasted image 20250405142713.png]]

## Context encoder
Lets learn more about the context encoder.
The dense passage retrieval or DPR context encoder and its tokenizer focus on encoding potential answer passages or documents.
This encoder creates embeddings from extensive texts, allowing the system to compare these with question embeddings to find the best match.
![[Pasted image 20250405142754.png|500]]

## Context tokenizer
Lets begin by understanding how you can use the context tokenizer.
First, import the DPR context encoder tokenizer from the transformers library.
Then load the pre trained tokenizer associated with the specified model, enabling the tokenization of context documents.
```python
from transformers import DPRContextEncoderTokenizer

context_tokenizer = DPRContextEncoderTokenizer.from_pretrained('facebook/dpr-ctx_encoder-single-nq-base')
context_tokenizer
```

Next, you must provide a list of tuples containing a pair of sentences.
The context tokenizer will process the input text by tokenizing, padding and truncating it to a maximum length of 256 tokens and then converting the text into a dictionary of pytorch tensors.
The output will be referred to as token info.
Token info holds the essential details for RAG, including input IDs which represent the token IDs or token indexes for the input text.
And token type IDs which represent the segment IDs are accompanied by attention mask values.
Following this, the RAG embeddings are generated.
```python
text = [("How are you?", "I am fine."), ("What's up?", "Not much.")]
print(text)

tokens_info=context_tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=256)
tokens_info
```
![[Pasted image 20250405143941.png|500]]

As the next step, import the DPR context encoder class from the transformers library for the context encoder.
Here, the encoder model specifies which pre trained DPR context encoder model to use.
Then initialize the encoder which loads the pre trained DPR context encoder model to convert text passages into RAG context vector embeddings.
You can generate the question using RAG context embeddings.
First, tokens are encoded, it passes the tokenized input known as token info through the context encoder to obtain the embeddings.
Next, the pooler output shape is retrieved from the encoded outputs, it indicates the shape of the pooler output.
Two, is the number of input text pairs processed.
768 is the dimensionality of the embedding vector for each text pair as defined by the DPR model.
```python
from transformers import DPRContextEncoder

context_encoder = DPRContextEncoder.from_pretrained('facebook/dpr-ctx_encoder-single-nq-base')

outputs=context_encoder(**tokens_info)
outputs.pooler_output.shape
# torch.Size([2, 768])
```

## Loanding the dataset
Consider an example of loading a text file containing company policies and preprocessing it to extract individual paragraphs.
Sample paragraphs from the text file are displayed here.
```python
def read_and_split_text(filename):
    with open(filename, 'r', encoding='utf-8') as file:
        text = file.read()
    # Split the text into paragraphs (simple split by newline characters)
    paragraphs = text.split('\n')
    # Filter out any empty paragraphs or undesired entries
    paragraphs = [para.strip() for para in paragraphs if len(para.strip()) > 0]
    return paragraphs

# Read the text file and split it into paragraphs
paragraphs = read_and_split_text('companyPolicies.txt')
paragraphs[0:10]
```
![[Pasted image 20250405144411.png|500]]

## Putting it together
Consolidate all the concepts discussed so far into a single function named encode context.
The output is a variable called paragraphs, which contains text across 76 paragraphs.
Consequently, the context embeddings have a shape of 76 by 768, where 76 represents the number of paragraphs, and 768 denotes the dimension of each embedding.
```python
def encode_contexts(text_list):
    # Encode a list of texts into embeddings
    embeddings = []
    for text in text_list:
        inputs = context_tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=256)
        outputs = context_encoder(**inputs)
        embeddings.append(outputs.pooler_output)
    return torch.cat(embeddings).detach().numpy()

# you would now encode these paragraphs to create embeddings.
context_embeddings = encode_contexts(paragraphs)
context_embeddings.shapre
# (76, 768)
```

## Facebook AI similarity search (Faiss)
Lets explore Facebook AI similarity search also known as Faiss, it is a library developed by Facebook AI research that offers efficient algorithms for searching through large collections of high dimensional vectors.
![[Pasted image 20250405144853.png|400]]

Faiss is essentially a tool to calculate the distance between the question embedding and the vector database of context vector embeddings.
![[Pasted image 20250405144915.png|400]]

You have to import Faiss, which will be used to calculate the distance.
First, pre process embeddings, this part of the code converts the context embeddings into a NumPy array of type float 32.
The next part initializes a Faiss index object for L2 or Euclidean distance named index, and adds the context embeddings to this Faiss index, making them searchable.
```python
import faiss

# Convert list of numpy arrays into a single numpy array
embedding_dim = 768  # This should match the dimension of your embeddings
context_embeddings_np = np.array(context_embeddings).astype('float32')

# Create a FAISS index for the embeddings
index = faiss.IndexFlatL2(embedding_dim)
index.add(context_embeddings_np)  # Add the context embeddings to the index
```

## Question encoder
Next lets learn about the question encoder.
In contrast, the DPR question encoder and its tokenizer focus on encoding the input questions into fixed dimensional vector representations, grasping their meaning and context to facilitate answering them.
![[Pasted image 20250405145155.png|500]]

You can import the DPR question encoder, and DPR question encoder tokenizer classes from the transformer library, which are used for encoding questions.
First, load the pre trained tokenizer for the question encoder model, enabling the tokenization of question texts.
Then load the pre trained question encoder model which converts tokenized question texts into dense vector embeddings.
The question encoder transforms questions into dense embeddings which are used to search through a corpus of context embeddings for the most relevant documents.
```python
from transformers import DPRQuestionEncoder, DPRQuestionEncoderTokenizer

# Load DPR question encoder and tokenizer
question_encoder = DPRQuestionEncoder.from_pretrained('facebook/dpr-question_encoder-single-nq-base')
question_tokenizer = DPRQuestionEncoderTokenizer.from_pretrained('facebook/dpr-question_encoder-single-nq-base')
```

## Retrieval process
Next, lets learn about the retrieval process.
![[Pasted image 20250405145410.png|500]]

## Example query and context retrieval
Here is an example where you can test it and retrieve the context embeddings nearest to a question.
The first part converts the question into embeddings.
Next, it uses the Faiss index to identify the top three closest context embeddings to the question embedding.
This search will provide these embeddings distance D and indices I.
This part outputs the distances and indices of the top three closest embeddings.
The Euclidean distances between the question embedding and the top three closest context embeddings in the Faiss index are shown.
Smaller values indicate closer matches, suggesting more relevant contexts.
The indices of the top three closest context embeddings in the Faiss index are provided.
These indices can be utilized to retrieve the corresponding context documents.
```python
# Example question
question = 'Drug and Alcohol Policy'
question_inputs = question_tokenizer(question, return_tensors='pt')
question_embedding = question_encoder(**question_inputs).pooler_output.detach().numpy()

# Search the index
D, I = index.search(question_embedding, k=3)  # Retrieve top 3 relevant contexts
print("D:",D)
print("I:",I)
```
![[Pasted image 20250405150625.png|300]]

In this example, corresponding paragraphs are printed according to the index found in the previous step.
These are documents that are most relevant to the question, they can be used as inputs to the model.
```python
print("Top 3 relevant contexts:")
for i, idx in enumerate(I[0]):
    print(f"{i+1}: {paragraphs[idx]}")
    print(f"distance {D[0][i]}\n")
```
![[Pasted image 20250405150809.png|500]]

Finally, lets input the prompt and context to generate a response.
![[Pasted image 20250405150840.png|500]]

## BART
You will need a decoder model such as a chatbot to effectively respond to the query question.
First, import the BartForConditionalGeneration and BartTokenizer classes from the transformers library.
Load the pre trained BART model and its corresponding tokenizer, this setup is crucial for initializing the text processing framework.
```python
from transformers import BartForConditionalGeneration, BartTokenizer

model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')
tokenizer = BartTokenizer.from_pretrained(facebook/bart-large-cnn' )
```

Tokenize the input question to prepare it for BART.
Ensure the tokens are returned as PyTorch tensors by using return_tensors='pt'.
Set the maximum length to 1024 with truncation to true to keep the input length within 1024 tokens.
Then, use BART to generate an answer with the following parameters.
- The maximum length of the generated sequence is 150.
- The minimum length of the generated sequence is 40.
- The length penalty is 2.0, which adjusts the length of the generated text where higher values discourage longer sequences.
- The number of beams for beam search is four, enhancing the quality of the generated sequences.
- Early stopping is set to true, it halts the generation process early if all beams reach the end token.
The generated token ids are then converted to a human readable string omitting special tokens.

```python
inputs = tokenizer(question, return_tensors='pt', max length=1024, truncation=True)
summary_ids = model.generate(inputs['input_ids'], max_length=150, min_length=40, length_penalty=2.0, num_beams=4, early_stopping=True)

answer = tokenizer.decode(summary_ids[0], skip_special_tokens=True)
```

Now display the generated response from BART based on the input question, providing immediate feedback on the output quality.
![[Pasted image 20250405151353.png]]

## Generating answers without context
Here, define the function to call a BART model using the code discussed in the previous step.
The questions are tokenized and converted to indexes which facilitates response generation.
Define a question and call the function which returns the answer generated by BART.
Next, the answer as generated by BART is displayed.
```python
def generate_answer_without_context(question):
    # Tokenize the input question
    inputs = tokenizer(question, return_tensors='pt', max_length=1024, truncation=True)
    
    # Generate output directly from the question without additional context
    summary_ids = model.generate(inputs['input_ids'], max_length=150, min_length=40, length_penalty=2.0, num_beams=4, early_stopping=True, pad_token_id=tokenizer.eos_token_id)
    
    # Decode and return the generated text
    answer = tokenizer.decode(summary_ids[0], skip_special_tokens=True)
    return answer

# Example usage
question = "what is mobile policy?"
answer = generate_answer_without_context(question)

print("Answer:", answer)
```

The chatbot model has not been trained on the company's policies, limiting its ability to generate relevant responses directly.
Therefore, you will utilize the context most closely related to the questions identified by RAG.
This context will then be input into the chatbot to produce appropriate outputs.
Here you can enhance the process by providing BART with the context of the question.
First, define a function that incorporates the text into the process.
Employ Faiss to identify the context indexes closest to our question demoted by I.
The content resembles the question stored in top contexts.
The model generates an answer taking the identified context into account, that's all I there's no need to train the chatbot.
```python
def generate_answer(question, contexts):
    # Concatenate the retrieved contexts to form the input to GPT2
    input_text = question + ' ' + ' '.join(contexts)
    inputs = tokenizer(input_text, return_tensors='pt', max_length=1024, truncation=True)

    # Generate output using GPT2
    summary_ids = model.generate(inputs['input_ids'], max_new_tokens=50, min_length=40, length_penalty=2.0, num_beams=4, early_stopping=True, pad_token_id=tokenizer.eos_token_id)
    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)

question = "what is mobile policy?"
_,I = search_relevant_contexts(question, question_tokenizer, question_encoder, index, k=5)
top_contexts = [paragraphs[idx] for idx in I[0]] 
answer = generate_answer(question, top_contexts)
```
![[Pasted image 20250405151740.png]]

## Recap
- RAG process involves encoding prompts into vectors, storing them, and retrieving relevant ones to produce a response.
- The DPR context encoder and its tokenizer encode the potential answer passages or documents.
- Faiss is a library developed by Facebook AI Research for searching through large collections of high dimensional vectors.
- The question encoder and its tokenizer
	- encode the input questions into fixed dimensional vector representations
	- Grasp meaning and context of questions to answering them.
- Finally, you learnt how to generate answers without context and by using RAG.
