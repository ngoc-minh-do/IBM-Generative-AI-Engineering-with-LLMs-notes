# LangChain: Core Concepts

## Learning objective
- Define a LangChain 
- Describe the components of LangChain.

## Introduction: LangChain
- LangChain is an open-source interface that simplifies the application development process using LLMs.
- It facilitates a structured way to integrate language models into various use cases, including natural language processing or NLP, and data retrieval.
![[Pasted image 20250405201340.png]]
## Components of LangChain
It consists of several components: documents, chains, agents, language model, chat model, chat message, prompt templates, and output parsers.
![[Pasted image 20250405201412.png]]

## Language model
The language models and LangChain are the foundation of LLMs.
It uses text input to generate text output and helps complete tasks and summarized documents.
LangChain uses IBM, OpenAI, Google, and Meta as a primary language model.
![[Pasted image 20250405201956.png]]

For example, to generate a response for a new sales approach using a language model, let's use IBM's watsonx.ai to create an LLM based on the mixtural 8 by 7 billion instruct model.
Ensure that the necessary dependencies, such as GenParams and ModelInference are imported from the IBM Watson Machine Learning package.
Now, customize the model by adjusting settings such as tokens and temperature.
A created model object will appear.
Next, the model shows a generated response text for the inserted prompt.
```python
model_id = 'mistralai/mixtral-8x7b-instruct-v01'

parameters = {
	GenParams.MAX_NEW_TOKENS: 256,  # this controls the maximum number of tokens in the generated output
	GenParams.TEMPERATURE: 0.5, # this randomness or creativity of the model's responses
}

credentials = {
	"url": "https://us-south.ml.cloud.ibm.com"
}

project_id = "skills-network"

model = Model(
	model_id=model_id,
	params=parameters,
	credentials=credentials,
	project_id=project_id
)

msg = model.generate("In today's sales meeting, we ")

print(msg['results'][0]['generated_text'])
```

You can view the generated sample response.
![[Pasted image 20250405201859.png]]

## Chat model
The next component of the LangChain is the chat model, a language model.
A chat model is designed for efficient conversations.
It means that it understands the questions or prompts and responds to them like a human.
![[Pasted image 20250405201935.png]]

Next, to generate a response, first, create a language model using watsonx.ai and transform the model into a chat model using WatsonxLLM function.
This converts the chat model into a conversational LLM to engage in dialogues.
For example, to see the response, insert a question into the model, such as, `who is man's best friend?`
```python
model = ModelInference(...)

mixtral_llm = WatsonxLLM(model=model)
response  = mixtral_llm.invoke("Who is man's best friend?")
```

You can view the generated sample response for the question.
![[Pasted image 20250405202221.png]]

## Chat message
Chat models handle various chat messages to make the model effective in the dynamic chat environment.
For example: 
- the HumanMessage helps user inputs.
- The AIMessage is generated by the model.
- The SystemMessage helps instruct the model.
- The FunctionMessage helps the function to call outcomes with a name parameter, and the ToolMessage helps in tool interaction to achieve results.
![[Pasted image 20250405202409.png]]

In the chat message, each chat message consists of two key properties.
The `role` means who is speaking and the `content` means what is being said.
![[Pasted image 20250405202450.png]]

Look at the example of a system generated message in which the model has given instructions to be an AI bot to respond to the question, what to eat in one short sentence.
To respond to this question, the chat model creates a list of messages.
First, configure the model as a fitness activity bot using a system message.
Then simulate the past conversation, using human message and AI message.
Next, using these settings, the model generates responses based on the previous dialogue.
```python
msg = mixtral_llm.invoke(
    [
        SystemMessage(content="You are a supportive AI bot that suggests fitness activities to a user in one short sentence"),
        HumanMessage(content="I like high-intensity workouts, what should I do?"),
        AIMessage(content="You should try a CrossFit class"),
        HumanMessage(content="How often should I attend?")
    ]
)
print(msg)
```
![[Pasted image 20250405203914.png]]

You can operate the chat model using human message as input and allow the model to generate responses without system message or AI message cues.
It means that the chat bot responds directly to human inputs.
```python
msg = mixtral_llm.invoke(
    [
        HumanMessage(content="What month follows June?")
    ]
)

print(msg)
```
![[Pasted image 20250405204002.png]]

## Prompt templates
The next component of LangChain is prompt templates.
The prompt templates and LangChain translate the user's questions or messages into clear instructions.
The language model uses these instructions to generate appropriate and coherent responses.
![[Pasted image 20250405204034.png]]

The types of prompt templates are 
- StringPromptTemplate is useful for single string formatting.
- ChatPromptTemplate is useful for message lists and specific templates, such as MessagePromptTemplate, including 
	- AIMessagePromptTemplate
	- SystemMessagePromptTemplate
	- HumanMessagePromptTemplate
	- ChatMessagePromptTemplate allows flexible role assignment.
- The Messages Placeholder provides full control over message rendering
- FewShotPromptTemplate provides specific examples or shots for LLMs.
![[Pasted image 20250405204223.png]]

Let's use the ChatPromptTemplate to generate a response.
In this prompt template, specify a message's role in content.
Next, within the content, include the parameter placeholders for repeated use to generate dynamic and flexible messages based on the input parameters and format your prompt.
```python
from langchain_core.prompts import ChatPromptTemplate

prompt = ChatPromptTemplate.from_messages([
    ("system", "You are a helpful assistant"),
    ("user", "Tell me a joke about {topic}")
])

input_ = {"topic": "cats"}

prompt.invoke(input_)
```
![[Pasted image 20250405204315.png]]

## Prompt templates: Example selector
Now, let's look at the example selectors in prompt templates.
It is important to select the most relevant examples from the example library to put them into the prompt.
An example selector in the prompt template makes this process efficient.
![[Pasted image 20250405204404.png]]

For example, the FewShotPromptTemplate 
- provides specific examples or shots to LLM.
- These examples or shots inform the model about the inserted context
- guide the LLM to generate the desired output.
![[Pasted image 20250405204430.png]]

Using example selectors from LangChain, you can optimize the FewShotPromptTemplates by:
- selecting semantic similarity
- max marginal relevance for diversity
- examples of efficient prompts
- 'N' gram overlap for textual similarity.
![[Pasted image 20250405204605.png]]

The screen displays the code to select examples using an N gram overlap example selector to form a FewShotPrompts.
```python
from langchain_core.example_selectors import LengthBasedExampleSelector,  NGramOverlapExampleSelector
from langchain_core.prompts import FewShotPromptTemplate, PromptTemplate

# Examples of a pretend task of creating antonyms.
examples = [
    {"input": "happy", "output": "sad"},
    {"input": "tall", "output": "short"},
    {"input": "energetic", "output": "lethargic"},
    {"input": "sunny", "output": "gloomy"},
    {"input": "windy", "output": "calm"},
]

example_prompt = PromptTemplate(
    input_variables=["input", "output"],
    template="Input: {input}\nOutput: {output}",
)

NGramOverlapExampleSelector(
	examples=examples,
	example_prompt=example_prompt,
	threshold =- 0.1
)

dynamic_prompt = FewShotPromptTemplate(
	example_selector=example_selector,
	example_prompt=example_prompt,
	prefix="Give the location an item is usually found in",
	suffix="Input: {item}\nOutput:",
	input_variables=["item"],
)
```
![[Pasted image 20250405210816.png]]

## Output parsers
The next component of LangChain is the output parsers.
The output parsers transform the output of an LLM into a more suitable format for generating structured data.
![[Pasted image 20250405210924.png]]

The LangChain provides a library of output parsers for various data formats, including JSON, XML, CSV, and Panda Data Frames.
Output parsers allow you to tailor the models output to meet specific data handling needs.
![[Pasted image 20250405210954.png|300]]

For example, let's use CommaSeparatedListOutputParser to convert LLMs response into CSV format.
This output parser effectively structures the output and simplifies it to handle and analyze in spreadsheet applications.
```python
from langchain.output_parsers import CommaSeparatedListOutputParser

output_parser = CommaSeparatedListOutputParser()

format_instructions = output_parser.get_format_instructions()
prompt = PromptTemplate(
    template="Answer the user query. {format_instructions}\nList five {subject}.",
    input_variables=["subject"],
    partial_variables={"format_instructions": format_instructions},
)

chain = prompt | mixtral_llm | output_parser

chain.invoke({"subject": "ice cream flavors"})
```
## Recap
- LangChain is an open-source interface that simplifies the application development process using LLMs.
- The core components of LangChain are: 
	- The language models in LangChain use text input to generate text output.
	- The chat model understands the question or prompts and responds like a human.
	- The chat model handles various chat messages such as 
		- HumanMessage
		- AIMessage
		- SystemMessage
		- FunctionMessage
		- ToolMessage.
- The prompt templates in LangChain translate the questions or messages into clear instructions.
- You've also learned about the example selector, which instructs the model for the inserted context and guides LLM to generate the desired output.
- Lastly, you learned about the output parsers that transform the output from an LLM into a suitable format.